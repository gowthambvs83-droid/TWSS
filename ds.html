<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Data Science and Machine Learning - Mathematical and Statistical Methods</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css">
    <style>
        :root {
            --primary-color: #2c3e50;
            --secondary-color: #3498db;
            --accent-color: #e74c3c;
            --light-color: #ecf0f1;
            --dark-color: #2c3e50;
            --success-color: #2ecc71;
            --warning-color: #f39c12;
            --info-color: #3498db;
            --code-bg: #f8f9fa;
            --border-color: #dee2e6;
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background-color: white;
            color: #333;
            line-height: 1.6;
        }

        .container {
            display: flex;
            min-height: 100vh;
        }

        /* Sidebar Navigation */
        .sidebar {
            width: 280px;
            background-color: var(--primary-color);
            color: white;
            overflow-y: auto;
            height: 100vh;
            position: fixed;
            transition: all 0.3s ease;
            z-index: 1000;
        }

        .sidebar-header {
            padding: 20px;
            background-color: rgba(0, 0, 0, 0.2);
            border-bottom: 1px solid rgba(255, 255, 255, 0.1);
        }

        .sidebar-header h2 {
            font-size: 1.3rem;
            margin-bottom: 5px;
        }

        .sidebar-header p {
            font-size: 0.9rem;
            opacity: 0.8;
        }

        .nav-menu {
            padding: 20px 0;
        }

        .nav-item {
            margin-bottom: 5px;
        }

        .nav-link {
            display: block;
            padding: 12px 20px;
            color: rgba(255, 255, 255, 0.8);
            text-decoration: none;
            transition: all 0.3s ease;
            border-left: 3px solid transparent;
        }

        .nav-link:hover, .nav-link.active {
            background-color: rgba(255, 255, 255, 0.1);
            color: white;
            border-left: 3px solid var(--secondary-color);
        }

        .nav-link i {
            margin-right: 10px;
            width: 20px;
            text-align: center;
        }

        .submenu {
            background-color: rgba(0, 0, 0, 0.1);
            display: none;
        }

        .submenu.active {
            display: block;
        }

        .submenu .nav-link {
            padding-left: 50px;
            font-size: 0.9rem;
        }

        /* Main Content */
        .main-content {
            flex: 1;
            margin-left: 280px;
            padding: 30px;
            max-width: calc(100% - 280px);
        }

        .content-header {
            margin-bottom: 30px;
            padding-bottom: 15px;
            border-bottom: 1px solid var(--border-color);
        }

        .content-header h1 {
            font-size: 2.5rem;
            color: var(--primary-color);
            margin-bottom: 10px;
        }

        .content-header .authors {
            font-size: 1.1rem;
            color: #666;
            margin-bottom: 5px;
        }

        .content-header .date {
            font-size: 0.9rem;
            color: #888;
        }

        .section {
            margin-bottom: 40px;
        }

        .section h2 {
            font-size: 1.8rem;
            color: var(--primary-color);
            margin-bottom: 20px;
            padding-bottom: 10px;
            border-bottom: 2px solid var(--secondary-color);
        }

        .section h3 {
            font-size: 1.4rem;
            color: var(--dark-color);
            margin: 25px 0 15px;
        }

        .section h4 {
            font-size: 1.2rem;
            color: var(--dark-color);
            margin: 20px 0 10px;
        }

        .section p {
            margin-bottom: 15px;
            text-align: justify;
        }

        .section ul, .section ol {
            margin-bottom: 20px;
            padding-left: 25px;
        }

        .section li {
            margin-bottom: 8px;
        }

        /* Code Blocks */
        .code-block {
            background-color: var(--code-bg);
            border: 1px solid var(--border-color);
            border-radius: 5px;
            padding: 15px;
            margin: 20px 0;
            overflow-x: auto;
            position: relative;
        }

        .code-block pre {
            margin: 0;
            font-family: 'Courier New', Courier, monospace;
            font-size: 0.9rem;
            line-height: 1.5;
        }

        .code-block .code-language {
            position: absolute;
            top: 5px;
            right: 10px;
            font-size: 0.8rem;
            color: #666;
            background-color: rgba(255, 255, 255, 0.7);
            padding: 2px 6px;
            border-radius: 3px;
        }

        .copy-btn {
            position: absolute;
            top: 5px;
            right: 60px;
            background-color: var(--secondary-color);
            color: white;
            border: none;
            border-radius: 3px;
            padding: 4px 8px;
            font-size: 0.8rem;
            cursor: pointer;
            transition: background-color 0.3s;
        }

        .copy-btn:hover {
            background-color: #2980b9;
        }

        /* Tables */
        .table-container {
            overflow-x: auto;
            margin: 20px 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
        }

        th, td {
            padding: 12px 15px;
            text-align: left;
            border-bottom: 1px solid var(--border-color);
        }

        th {
            background-color: var(--light-color);
            font-weight: 600;
            color: var(--dark-color);
        }

        tr:hover {
            background-color: rgba(52, 152, 219, 0.1);
        }

        /* Alerts/Notes */
        .alert {
            padding: 15px;
            margin: 20px 0;
            border-radius: 5px;
            border-left: 4px solid;
        }

        .alert-info {
            background-color: rgba(52, 152, 219, 0.1);
            border-color: var(--info-color);
        }

        .alert-warning {
            background-color: rgba(243, 156, 18, 0.1);
            border-color: var(--warning-color);
        }

        .alert-success {
            background-color: rgba(46, 204, 113, 0.1);
            border-color: var(--success-color);
        }

        .alert-danger {
            background-color: rgba(231, 76, 60, 0.1);
            border-color: var(--accent-color);
        }

        /* Visualizations */
        .viz-container {
            margin: 30px 0;
            padding: 20px;
            border: 1px solid var(--border-color);
            border-radius: 5px;
            background-color: white;
        }

        .viz-title {
            font-size: 1.1rem;
            font-weight: 600;
            margin-bottom: 15px;
            color: var(--dark-color);
        }

        .canvas-container {
            display: flex;
            justify-content: center;
            align-items: center;
            min-height: 300px;
        }

        /* Math Formulas */
        .formula {
            font-family: 'Times New Roman', Times, serif;
            font-style: italic;
            text-align: center;
            margin: 20px 0;
            padding: 10px;
            background-color: var(--code-bg);
            border-radius: 5px;
            overflow-x: auto;
        }

        /* Buttons */
        .btn {
            display: inline-block;
            padding: 10px 20px;
            background-color: var(--secondary-color);
            color: white;
            border: none;
            border-radius: 5px;
            cursor: pointer;
            text-decoration: none;
            font-size: 1rem;
            transition: background-color 0.3s;
            margin: 5px;
        }

        .btn:hover {
            background-color: #2980b9;
        }

        .btn-success {
            background-color: var(--success-color);
        }

        .btn-success:hover {
            background-color: #27ae60;
        }

        .btn-warning {
            background-color: var(--warning-color);
        }

        .btn-warning:hover {
            background-color: #e67e22;
        }

        /* Mobile Toggle */
        .mobile-toggle {
            display: none;
            position: fixed;
            top: 20px;
            left: 20px;
            z-index: 1001;
            background-color: var(--primary-color);
            color: white;
            border: none;
            border-radius: 5px;
            padding: 10px;
            cursor: pointer;
        }

        /* Responsive */
        @media (max-width: 992px) {
            .sidebar {
                width: 250px;
            }
            
            .main-content {
                margin-left: 250px;
                max-width: calc(100% - 250px);
            }
        }

        @media (max-width: 768px) {
            .mobile-toggle {
                display: block;
            }
            
            .sidebar {
                transform: translateX(-100%);
            }
            
            .sidebar.active {
                transform: translateX(0);
            }
            
            .main-content {
                margin-left: 0;
                max-width: 100%;
                padding: 20px;
            }
            
            .content-header h1 {
                font-size: 2rem;
            }
        }

        /* Loading Animation */
        .loader {
            border: 5px solid #f3f3f3;
            border-top: 5px solid var(--secondary-color);
            border-radius: 50%;
            width: 40px;
            height: 40px;
            animation: spin 1s linear infinite;
            margin: 20px auto;
        }

        @keyframes spin {
            0% { transform: rotate(0deg); }
            100% { transform: rotate(360deg); }
        }

        /* Tabs */
        .tabs {
            margin: 20px 0;
        }

        .tab-buttons {
            display: flex;
            border-bottom: 1px solid var(--border-color);
        }

        .tab-button {
            padding: 10px 20px;
            background-color: transparent;
            border: none;
            cursor: pointer;
            font-size: 1rem;
            border-bottom: 2px solid transparent;
            transition: all 0.3s;
        }

        .tab-button.active {
            border-bottom: 2px solid var(--secondary-color);
            color: var(--secondary-color);
            font-weight: 600;
        }

        .tab-content {
            padding: 20px 0;
        }

        .tab-pane {
            display: none;
        }

        .tab-pane.active {
            display: block;
        }

        /* Accordion */
        .accordion {
            margin: 20px 0;
        }

        .accordion-item {
            border: 1px solid var(--border-color);
            margin-bottom: 5px;
            border-radius: 5px;
            overflow: hidden;
        }

        .accordion-header {
            background-color: var(--light-color);
            padding: 15px;
            cursor: pointer;
            display: flex;
            justify-content: space-between;
            align-items: center;
            font-weight: 600;
        }

        .accordion-header:hover {
            background-color: #e2e6ea;
        }

        .accordion-icon {
            transition: transform 0.3s;
        }

        .accordion-item.active .accordion-icon {
            transform: rotate(180deg);
        }

        .accordion-body {
            padding: 15px;
            display: none;
        }

        .accordion-item.active .accordion-body {
            display: block;
        }
    </style>
</head>
<body>
    <div class="container">
        <!-- Mobile Toggle Button -->
        <button class="mobile-toggle" id="mobileToggle">
            <i class="fas fa-bars"></i>
        </button>

        <!-- Sidebar Navigation -->
        <nav class="sidebar" id="sidebar">
            <div class="sidebar-header">
                <h2>Data Science & ML</h2>
                <p>Mathematical and Statistical Methods</p>
            </div>
            <div class="nav-menu">
                <div class="nav-item">
                    <a href="#preface" class="nav-link active">
                        <i class="fas fa-book"></i> Preface
                    </a>
                </div>
                <div class="nav-item">
                    <a href="#notation" class="nav-link">
                        <i class="fas fa-font"></i> Notation
                    </a>
                </div>
                <div class="nav-item">
                    <a href="#chapter1" class="nav-link">
                        <i class="fas fa-database"></i> Chapter 1: Importing, Summarizing, and Visualizing Data
                    </a>
                    <div class="submenu" id="chapter1-submenu">
                        <a href="#chapter1-1" class="nav-link">1.1 Introduction</a>
                        <a href="#chapter1-2" class="nav-link">1.2 Structuring Features According to Type</a>
                        <a href="#chapter1-3" class="nav-link">1.3 Summary Tables</a>
                        <a href="#chapter1-4" class="nav-link">1.4 Summary Statistics</a>
                        <a href="#chapter1-5" class="nav-link">1.5 Visualizing Data</a>
                    </div>
                </div>
                <div class="nav-item">
                    <a href="#chapter2" class="nav-link">
                        <i class="fas fa-chart-line"></i> Chapter 2: Statistical Learning
                    </a>
                    <div class="submenu" id="chapter2-submenu">
                        <a href="#chapter2-1" class="nav-link">2.1 Introduction</a>
                        <a href="#chapter2-2" class="nav-link">2.2 Supervised and Unsupervised Learning</a>
                        <a href="#chapter2-3" class="nav-link">2.3 Training and Test Loss</a>
                        <a href="#chapter2-4" class="nav-link">2.4 Tradeoffs in Statistical Learning</a>
                        <a href="#chapter2-5" class="nav-link">2.5 Estimating Risk</a>
                        <a href="#chapter2-6" class="nav-link">2.6 Modeling Data</a>
                        <a href="#chapter2-7" class="nav-link">2.7 Multivariate Normal Models</a>
                        <a href="#chapter2-8" class="nav-link">2.8 Normal Linear Models</a>
                        <a href="#chapter2-9" class="nav-link">2.9 Bayesian Learning</a>
                    </div>
                </div>
                <div class="nav-item">
                    <a href="#chapter3" class="nav-link">
                        <i class="fas fa-dice"></i> Chapter 3: Monte Carlo Methods
                    </a>
                    <div class="submenu" id="chapter3-submenu">
                        <a href="#chapter3-1" class="nav-link">3.1 Introduction</a>
                        <a href="#chapter3-2" class="nav-link">3.2 Monte Carlo Sampling</a>
                        <a href="#chapter3-3" class="nav-link">3.3 Monte Carlo Estimation</a>
                        <a href="#chapter3-4" class="nav-link">3.4 Monte Carlo for Optimization</a>
                    </div>
                </div>
                <div class="nav-item">
                    <a href="#chapter4" class="nav-link">
                        <i class="fas fa-project-diagram"></i> Chapter 4: Unsupervised Learning
                    </a>
                    <div class="submenu" id="chapter4-submenu">
                        <a href="#chapter4-1" class="nav-link">4.1 Introduction</a>
                        <a href="#chapter4-2" class="nav-link">4.2 Risk and Loss in Unsupervised Learning</a>
                        <a href="#chapter4-3" class="nav-link">4.3 Expectation–Maximization (EM) Algorithm</a>
                        <a href="#chapter4-4" class="nav-link">4.4 Empirical Distribution and Density Estimation</a>
                        <a href="#chapter4-5" class="nav-link">4.5 Clustering via Mixture Models</a>
                        <a href="#chapter4-6" class="nav-link">4.6 Clustering via Vector Quantization</a>
                        <a href="#chapter4-7" class="nav-link">4.7 Hierarchical Clustering</a>
                        <a href="#chapter4-8" class="nav-link">4.8 Principal Component Analysis (PCA)</a>
                    </div>
                </div>
                <div class="nav-item">
                    <a href="#chapter5" class="nav-link">
                        <i class="fas fa-chart-area"></i> Chapter 5: Regression
                    </a>
                    <div class="submenu" id="chapter5-submenu">
                        <a href="#chapter5-1" class="nav-link">5.1 Introduction</a>
                        <a href="#chapter5-2" class="nav-link">5.2 Linear Regression</a>
                        <a href="#chapter5-3" class="nav-link">5.3 Analysis via Linear Models</a>
                        <a href="#chapter5-4" class="nav-link">5.4 Inference for Normal Linear Models</a>
                        <a href="#chapter5-5" class="nav-link">5.5 Nonlinear Regression Models</a>
                        <a href="#chapter5-6" class="nav-link">5.6 Linear Models in Python</a>
                        <a href="#chapter5-7" class="nav-link">5.7 Generalized Linear Models</a>
                    </div>
                </div>
                <div class="nav-item">
                    <a href="#chapter6" class="nav-link">
                        <i class="fas fa-vector-square"></i> Chapter 6: Regularization and Kernel Methods
                    </a>
                    <div class="submenu" id="chapter6-submenu">
                        <a href="#chapter6-1" class="nav-link">6.1 Introduction</a>
                        <a href="#chapter6-2" class="nav-link">6.2 Regularization</a>
                        <a href="#chapter6-3" class="nav-link">6.3 Reproducing Kernel Hilbert Spaces</a>
                        <a href="#chapter6-4" class="nav-link">6.4 Construction of Reproducing Kernels</a>
                        <a href="#chapter6-5" class="nav-link">6.5 Representer Theorem</a>
                        <a href="#chapter6-6" class="nav-link">6.6 Smoothing Cubic Splines</a>
                        <a href="#chapter6-7" class="nav-link">6.7 Gaussian Process Regression</a>
                        <a href="#chapter6-8" class="nav-link">6.8 Kernel PCA</a>
                    </div>
                </div>
                <div class="nav-item">
                    <a href="#chapter7" class="nav-link">
                        <i class="fas fa-tags"></i> Chapter 7: Classification
                    </a>
                    <div class="submenu" id="chapter7-submenu">
                        <a href="#chapter7-1" class="nav-link">7.1 Introduction</a>
                        <a href="#chapter7-2" class="nav-link">7.2 Classification Metrics</a>
                        <a href="#chapter7-3" class="nav-link">7.3 Classification via Bayes' Rule</a>
                        <a href="#chapter7-4" class="nav-link">7.4 Linear and Quadratic Discriminant Analysis</a>
                        <a href="#chapter7-5" class="nav-link">7.5 Logistic Regression and Softmax Classification</a>
                        <a href="#chapter7-6" class="nav-link">7.6 K-Nearest Neighbors Classification</a>
                        <a href="#chapter7-7" class="nav-link">7.7 Support Vector Machine</a>
                        <a href="#chapter7-8" class="nav-link">7.8 Classification with Scikit-Learn</a>
                    </div>
                </div>
                <div class="nav-item">
                    <a href="#chapter8" class="nav-link">
                        <i class="fas fa-sitemap"></i> Chapter 8: Decision Trees and Ensemble Methods
                    </a>
                    <div class="submenu" id="chapter8-submenu">
                        <a href="#chapter8-1" class="nav-link">8.1 Introduction</a>
                        <a href="#chapter8-2" class="nav-link">8.2 Top-Down Construction of Decision Trees</a>
                        <a href="#chapter8-3" class="nav-link">8.3 Additional Considerations</a>
                        <a href="#chapter8-4" class="nav-link">8.4 Controlling the Tree Shape</a>
                        <a href="#chapter8-5" class="nav-link">8.5 Bootstrap Aggregation</a>
                        <a href="#chapter8-6" class="nav-link">8.6 Random Forests</a>
                        <a href="#chapter8-7" class="nav-link">8.7 Boosting</a>
                    </div>
                </div>
                <div class="nav-item">
                    <a href="#chapter9" class="nav-link">
                        <i class="fas fa-brain"></i> Chapter 9: Deep Learning
                    </a>
                    <div class="submenu" id="chapter9-submenu">
                        <a href="#chapter9-1" class="nav-link">9.1 Introduction</a>
                        <a href="#chapter9-2" class="nav-link">9.2 Feed-Forward Neural Networks</a>
                        <a href="#chapter9-3" class="nav-link">9.3 Back-Propagation</a>
                        <a href="#chapter9-4" class="nav-link">9.4 Methods for Training</a>
                        <a href="#chapter9-5" class="nav-link">9.5 Examples in Python</a>
                    </div>
                </div>
                <div class="nav-item">
                    <a href="#appendix-a" class="nav-link">
                        <i class="fas fa-calculator"></i> Appendix A: Linear Algebra and Functional Analysis
                    </a>
                </div>
                <div class="nav-item">
                    <a href="#appendix-b" class="nav-link">
                        <i class="fas fa-chart-line"></i> Appendix B: Multivariate Differentiation and Optimization
                    </a>
                </div>
                <div class="nav-item">
                    <a href="#appendix-c" class="nav-link">
                        <i class="fas fa-percentage"></i> Appendix C: Probability and Statistics
                    </a>
                </div>
                <div class="nav-item">
                    <a href="#appendix-d" class="nav-link">
                        <i class="fab fa-python"></i> Appendix D: Python Primer
                    </a>
                </div>
            </div>
        </nav>

        <!-- Main Content -->
        <main class="main-content">
            <div class="content-header">
                <h1>Data Science and Machine Learning</h1>
                <div class="authors">Mathematical and Statistical Methods</div>
                <div class="authors">Dirk P. Kroese, Zdravko I. Botev, Thomas Taimre, Radislav Vaisman</div>
                <div class="date">22nd August 2024</div>
            </div>

            <!-- Preface Section -->
            <section id="preface" class="section">
                <h2>Preface</h2>
                <p>In our present world of automation, cloud computing, algorithms, artificial intelligence, and big data, few topics are as relevant as data science and machine learning. Their recent popularity lies not only in their applicability to real-life questions, but also in their natural blending of many different disciplines, including mathematics, statistics, computer science, engineering, science, and finance.</p>
                
                <p>To someone starting to learn these topics, the multitude of computational techniques and mathematical ideas may seem overwhelming. Some may be satisfied with only learning how to use off-the-shelf recipes to apply to practical situations. But what if the assumptions of the black-box recipe are violated? Can we still trust the results? How should the algorithm be adapted? To be able to truly understand data science and machine learning it is important to appreciate the underlying mathematics and statistics, as well as the resulting algorithms.</p>
                
                <p>The purpose of this book is to provide an accessible, yet comprehensive, account of data science and machine learning. It is intended for anyone interested in gaining a better understanding of the mathematics and statistics that underpin the rich variety of ideas and machine learning algorithms in data science. Our viewpoint is that computer languages come and go, but the underlying key ideas and algorithms will remain forever and will form the basis for future developments.</p>
                
                <div class="alert alert-info">
                    <strong>Python Resources:</strong> Python code and data sets for each chapter can be downloaded from the GitHub site: <a href="https://github.com/DSML-book" target="_blank">https://github.com/DSML-book</a>
                </div>
            </section>

            <!-- Notation Section -->
            <section id="notation" class="section">
                <h2>Notation</h2>
                <p>We have tried to use a notation system that is, in order of importance, simple, descriptive, consistent, and compatible with historical choices. Achieving all of these goals all of the time would be impossible, but we hope that our notation helps to quickly recognize the type or "flavor" of certain mathematical objects (vectors, matrices, random vectors, probability measures, etc.) and clarify intricate ideas.</p>
                
                <div class="table-container">
                    <table>
                        <thead>
                            <tr>
                                <th>Symbol</th>
                                <th>Description</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td><strong>x</strong></td>
                                <td>Vector</td>
                            </tr>
                            <tr>
                                <td><strong>X</strong></td>
                                <td>Random vector</td>
                            </tr>
                            <tr>
                                <td><strong>X</strong></td>
                                <td>Matrix</td>
                            </tr>
                            <tr>
                                <td><strong>X</strong></td>
                                <td>Set</td>
                            </tr>
                            <tr>
                                <td><strong>x̂</strong></td>
                                <td>Estimate or approximation</td>
                            </tr>
                            <tr>
                                <td><strong>x*</strong></td>
                                <td>Optimal</td>
                            </tr>
                            <tr>
                                <td><strong>x̄</strong></td>
                                <td>Average</td>
                            </tr>
                            <tr>
                                <td><strong>∀</strong></td>
                                <td>For all</td>
                            </tr>
                            <tr>
                                <td><strong>∃</strong></td>
                                <td>There exists</td>
                            </tr>
                            <tr>
                                <td><strong>∝</strong></td>
                                <td>Is proportional to</td>
                            </tr>
                            <tr>
                                <td><strong>∼</strong></td>
                                <td>Is distributed as</td>
                            </tr>
                            <tr>
                                <td><strong>∇f</strong></td>
                                <td>Gradient of f</td>
                            </tr>
                            <tr>
                                <td><strong>∇²f</strong></td>
                                <td>Hessian of f</td>
                            </tr>
                            <tr>
                                <td><strong>E</strong></td>
                                <td>Expectation</td>
                            </tr>
                            <tr>
                                <td><strong>P</strong></td>
                                <td>Probability measure</td>
                            </tr>
                        </tbody>
                    </table>
                </div>
            </section>

            <!-- Chapter 1 Section -->
            <section id="chapter1" class="section">
                <h2>Chapter 1: Importing, Summarizing, and Visualizing Data</h2>
                
                <div id="chapter1-1" class="subsection">
                    <h3>1.1 Introduction</h3>
                    <p>Data comes in many shapes and forms, but can generally be thought of as being the result of some random experiment — an experiment whose outcome cannot be determined in advance, but whose workings are still subject to analysis. Data from a random experiment are often stored in a table or spreadsheet. A statistical convention is to denote variables — often called <strong>features</strong> — as columns and the individual items (or units) as rows.</p>
                    
                    <p>It is useful to think of three types of columns in such a spreadsheet:</p>
                    <ol>
                        <li>The first column is usually an identifier or index column, where each unit/row is given a unique name or ID.</li>
                        <li>Certain columns (features) can correspond to the design of the experiment, specifying, for example, to which experimental group the unit belongs. Often the entries in these columns are deterministic; that is, they stay the same if the experiment were to be repeated.</li>
                        <li>Other columns represent the observed measurements of the experiment. Usually, these measurements exhibit variability; that is, they would change if the experiment were to be repeated.</li>
                    </ol>
                    
                    <p>There are many data sets available from the Internet and in software packages. A well-known repository of data sets is the Machine Learning Repository maintained by the University of California at Irvine (UCI), found at <a href="https://archive.ics.uci.edu/" target="_blank">https://archive.ics.uci.edu/</a>.</p>
                    
                    <div class="alert alert-success">
                        <strong>Example:</strong> These data sets are typically stored in a CSV (comma separated values) format, which can be easily read into Python.
                    </div>
                    
                    <div class="code-block">
                        <span class="code-language">Python</span>
                        <button class="copy-btn" onclick="copyCode(this)">Copy</button>
                        <pre><code>import pandas as pd

# Example: Loading the abalone dataset from UCI repository
abalone = pd.read_csv('abalone.data', header=None)

# Example: Loading Fisher's iris dataset from R datasets
urlprefix = 'https://vincentarelbundock.github.io/Rdatasets/csv/'
dataname = 'datasets/iris.csv'
iris = pd.read_csv(urlprefix + dataname)</code></pre>
                    </div>
                    
                    <div class="viz-container">
                        <div class="viz-title">Iris Dataset Sample</div>
                        <div class="table-container">
                            <table>
                                <thead>
                                    <tr>
                                        <th>Sepal.Length</th>
                                        <th>Sepal.Width</th>
                                        <th>Petal.Length</th>
                                        <th>Petal.Width</th>
                                        <th>Species</th>
                                    </tr>
                                </thead>
                                <tbody>
                                    <tr>
                                        <td>5.1</td>
                                        <td>3.5</td>
                                        <td>1.4</td>
                                        <td>0.2</td>
                                        <td>setosa</td>
                                    </tr>
                                    <tr>
                                        <td>4.9</td>
                                        <td>3.0</td>
                                        <td>1.4</td>
                                        <td>0.2</td>
                                        <td>setosa</td>
                                    </tr>
                                    <tr>
                                        <td>4.7</td>
                                        <td>3.2</td>
                                        <td>1.3</td>
                                        <td>0.2</td>
                                        <td>setosa</td>
                                    </tr>
                                </tbody>
                            </table>
                        </div>
                    </div>
                </div>
                
                <div id="chapter1-2" class="subsection">
                    <h3>1.2 Structuring Features According to Type</h3>
                    <p>We can generally classify features as either <strong>quantitative</strong> or <strong>qualitative</strong>. Quantitative features possess "numerical quantity", such as height, age, number of births, etc., and can either be continuous or discrete. Continuous quantitative features take values in a continuous range of possible values, such as height, voltage, or crop yield; such features capture the idea that measurements can always be made more precisely. Discrete quantitative features have a countable number of possibilities, such as a count.</p>
                    
                    <p>In contrast, qualitative features do not have a numerical meaning, but their possible values can be divided into a fixed number of categories, such as {M,F} for gender or {blue, black, brown, green} for eye color. For this reason such features are also called categorical. A simple rule of thumb is: if it does not make sense to average the data, it is categorical. For example, it does not make sense to average eye colors. Of course it is still possible to represent categorical data with numbers, such as 1 = blue, 2 = black, 3 = brown, but such numbers carry no quantitative meaning. Categorical features are often called factors.</p>
                    
                    <div class="alert alert-warning">
                        <strong>Important:</strong> When manipulating, summarizing, and displaying data, it is important to correctly specify the type of the variables (features).
                    </div>
                    
                    <div class="code-block">
                        <span class="code-language">Python</span>
                        <button class="copy-btn" onclick="copyCode(this)">Copy</button>
                        <pre><code># Example: Loading and structuring the nutrition_elderly dataset
import pandas as pd

xls = 'http://www.biostatisticien.eu/springeR/nutrition_elderly.xls'
nutri = pd.read_excel(xls)

# Check the data types
nutri.info()

# Convert categorical features
DICT = {1:'Male', 2:'Female'} # dictionary specifies replacement
nutri['gender'] = nutri['gender'].replace(DICT).astype('category')

# Convert continuous features
nutri['height'] = nutri['height'].astype(float)

# Save the modified data frame
nutri.to_csv('nutri.csv', index=False)</code></pre>
                    </div>
                    
                    <div class="table-container">
                        <table>
                            <thead>
                                <tr>
                                    <th>Feature Type</th>
                                    <th>Features</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td>Qualitative</td>
                                    <td>gender, situation, fat</td>
                                </tr>
                                <tr>
                                    <td>Qualitative (Ordinal)</td>
                                    <td>meat, fish, raw_fruit, cooked_fruit_veg, chocol</td>
                                </tr>
                                <tr>
                                    <td>Discrete quantitative</td>
                                    <td>tea, coffee</td>
                                </tr>
                                <tr>
                                    <td>Continuous quantitative</td>
                                    <td>height, weight, age</td>
                                </tr>
                            </tbody>
                        </table>
                    </div>
                </div>
                
                <div id="chapter1-3" class="subsection">
                    <h3>1.3 Summary Tables</h3>
                    <p>It is often useful to summarize a large spreadsheet of data in a more condensed form. A table of counts or a table of frequencies makes it easier to gain insight into the underlying distribution of a variable, especially if the data are qualitative. Such tables can be obtained with the methods describe and value_counts.</p>
                    
                    <div class="code-block">
                        <span class="code-language">Python</span>
                        <button class="copy-btn" onclick="copyCode(this)">Copy</button>
                        <pre><code># Load the nutrition data
nutri = pd.read_csv('nutri.csv')

# Summary for the 'fat' feature
nutri['fat'].describe()

# Count values for different fat types
nutri['fat'].value_counts()

# Cross tabulation between gender and situation
pd.crosstab(nutri.gender, nutri.situation)

# Cross tabulation with row and column totals
pd.crosstab(nutri.gender, nutri.situation, margins=True)</code></pre>
                    </div>
                    
                    <div class="viz-container">
                        <div class="viz-title">Cross Tabulation: Gender vs. Living Situation</div>
                        <div class="table-container">
                            <table>
                                <thead>
                                    <tr>
                                        <th>Gender</th>
                                        <th>Couple</th>
                                        <th>Family</th>
                                        <th>Single</th>
                                        <th>All</th>
                                    </tr>
                                </thead>
                                <tbody>
                                    <tr>
                                        <td>Female</td>
                                        <td>56</td>
                                        <td>7</td>
                                        <td>78</td>
                                        <td>141</td>
                                    </tr>
                                    <tr>
                                        <td>Male</td>
                                        <td>63</td>
                                        <td>2</td>
                                        <td>20</td>
                                        <td>85</td>
                                    </tr>
                                    <tr>
                                        <td>All</td>
                                        <td>119</td>
                                        <td>9</td>
                                        <td>98</td>
                                        <td>226</td>
                                    </tr>
                                </tbody>
                            </table>
                        </div>
                    </div>
                </div>
                
                <div id="chapter1-4" class="subsection">
                    <h3>1.4 Summary Statistics</h3>
                    <p>In the following, x = [x₁, ..., xₙ]ᵀ is a column vector of n numbers. For our nutri data, the vector x could, for example, correspond to the heights of the n = 226 individuals.</p>
                    
                    <p>The <strong>sample mean</strong> of x, denoted by x̄, is simply the average of the data values:</p>
                    
                    <div class="formula">
                        x̄ = (1/n) Σᵢ₌₁ⁿ xᵢ
                    </div>
                    
                    <p>The <strong>p-sample quantile</strong> (0 < p < 1) of x is a value x such that at least a fraction p of the data is less than or equal to x and at least a fraction 1−p of the data is greater than or equal to x. The <strong>sample median</strong> is the sample 0.5-quantile. The p-sample quantile is also called the 100 × p percentile. The 25, 50, and 75 sample percentiles are called the first, second, and third quartiles of the data.</p>
                    
                    <p>The sample mean and median give information about the location of the data, while the distance between sample quantiles (say the 0.1 and 0.9 quantiles) gives some indication of the dispersion (spread) of the data. Other measures for dispersion are the sample range, maxᵢxᵢ − minᵢxᵢ, the sample variance:</p>
                    
                    <div class="formula">
                        s² = (1/(n-1)) Σᵢ₌₁ⁿ (xᵢ - x̄)²
                    </div>
                    
                    <p>and the sample standard deviation s = √s².</p>
                    
                    <div class="code-block">
                        <span class="code-language">Python</span>
                        <button class="copy-btn" onclick="copyCode(this)">Copy</button>
                        <pre><code># Calculate summary statistics for height
nutri['height'].mean()      # Sample mean
nutri['height'].quantile(q=[0.25,0.5,0.75])  # Quartiles
nutri['height'].max() - nutri['height'].min()  # Range
nutri['height'].var()       # Variance
nutri['height'].std()       # Standard deviation

# Comprehensive summary
nutri['height'].describe()</code></pre>
                    </div>
                    
                    <div class="viz-container">
                        <div class="viz-title">Height Summary Statistics</div>
                        <div class="table-container">
                            <table>
                                <thead>
                                    <tr>
                                        <th>Statistic</th>
                                        <th>Value</th>
                                    </tr>
                                </thead>
                                <tbody>
                                    <tr>
                                        <td>Count</td>
                                        <td>226.000000</td>
                                    </tr>
                                    <tr>
                                        <td>Mean</td>
                                        <td>163.960177</td>
                                    </tr>
                                    <tr>
                                        <td>Std</td>
                                        <td>9.003368</td>
                                    </tr>
                                    <tr>
                                        <td>Min</td>
                                        <td>140.000000</td>
                                    </tr>
                                    <tr>
                                        <td>25%</td>
                                        <td>157.000000</td>
                                    </tr>
                                    <tr>
                                        <td>50% (Median)</td>
                                        <td>163.000000</td>
                                    </tr>
                                    <tr>
                                        <td>75%</td>
                                        <td>170.000000</td>
                                    </tr>
                                    <tr>
                                        <td>Max</td>
                                        <td>188.000000</td>
                                    </tr>
                                </tbody>
                            </table>
                        </div>
                    </div>
                </div>
                
                <div id="chapter1-5" class="subsection">
                    <h3>1.5 Visualizing Data</h3>
                    <p>In this section we describe various methods for visualizing data. The main point we would like to make is that the way in which variables are visualized should always be adapted to the variable types; for example, qualitative data should be plotted differently from quantitative data.</p>
                    
                    <div class="tabs">
                        <div class="tab-buttons">
                            <button class="tab-button active" onclick="openTab(event, 'qualitative')">Qualitative Variables</button>
                            <button class="tab-button" onclick="openTab(event, 'quantitative')">Quantitative Variables</button>
                            <button class="tab-button" onclick="openTab(event, 'bivariate')">Bivariate Setting</button>
                        </div>
                        
                        <div id="qualitative" class="tab-content">
                            <div class="tab-pane active">
                                <h4>1.5.1 Plotting Qualitative Variables</h4>
                                <p>Suppose we wish to display graphically how many elderly people are living by themselves, as a couple, with family, or other. Recall that the data are given in the situation column of our nutri data.</p>
                                
                                <div class="code-block">
                                    <span class="code-language">Python</span>
                                    <button class="copy-btn" onclick="copyCode(this)">Copy</button>
                                    <pre><code>import matplotlib.pyplot as plt

# Create a barplot for the 'situation' variable
width = 0.35  # the width of the bars
x = [0, 0.8, 1.6]  # the bar positions on x-axis
situation_counts = nutri['situation'].value_counts()

plt.bar(x, situation_counts, width, edgecolor='black')
plt.xticks(x, situation_counts.index)
plt.title('Living Situation of Elderly People')
plt.ylabel('Count')
plt.show()</code></pre>
                                </div>
                                
                                <div class="viz-container">
                                    <div class="viz-title">Barplot for the Qualitative Variable 'situation'</div>
                                    <div class="canvas-container">
                                        <canvas id="barplot" width="600" height="400"></canvas>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div id="quantitative" class="tab-content">
                            <div class="tab-pane">
                                <h4>1.5.2 Plotting Quantitative Variables</h4>
                                <p>We now present a few useful methods for visualizing quantitative data, again using the nutri data set. We will first focus on continuous features (e.g., 'age') and then add some specific graphs related to discrete features (e.g., 'tea'). The aim is to describe the variability present in a single feature. This typically involves a central tendency, where observations tend to gather around, with fewer observations further away.</p>
                                
                                <div class="accordion">
                                    <div class="accordion-item">
                                        <div class="accordion-header" onclick="toggleAccordion(this)">
                                            <span>Boxplot</span>
                                            <i class="fas fa-chevron-down accordion-icon"></i>
                                        </div>
                                        <div class="accordion-body">
                                            <p>A boxplot can be viewed as a graphical representation of the five-number summary of the data consisting of the minimum, maximum, and the first, second, and third quartiles.</p>
                                            
                                            <div class="code-block">
                                                <span class="code-language">Python</span>
                                                <button class="copy-btn" onclick="copyCode(this)">Copy</button>
                                                <pre><code># Create a boxplot for the 'age' feature
plt.boxplot(nutri['age'], widths=width, vert=False)
plt.xlabel('age')
plt.title('Boxplot of Age')
plt.show()</code></pre>
                                            </div>
                                            
                                            <div class="viz-container">
                                                <div class="viz-title">Boxplot for 'age'</div>
                                                <div class="canvas-container">
                                                    <canvas id="boxplot" width="600" height="300"></canvas>
                                                </div>
                                            </div>
                                            
                                            <p>The box is drawn from the first quartile (Q1) to the third quartile (Q3). The vertical line inside the box signifies the location of the median. So-called "whiskers" extend to either side of the box. The size of the box is called the interquartile range: IQR = Q3 − Q1.</p>
                                        </div>
                                    </div>
                                    
                                    <div class="accordion-item">
                                        <div class="accordion-header" onclick="toggleAccordion(this)">
                                            <span>Histogram</span>
                                            <i class="fas fa-chevron-down accordion-icon"></i>
                                        </div>
                                        <div class="accordion-body">
                                            <p>A histogram is a representation of the distribution of a continuous variable. The data is divided into bins, and the height of each bin shows the number of data points that fall into that bin.</p>
                                            
                                            <div class="code-block">
                                                <span class="code-language">Python</span>
                                                <button class="copy-btn" onclick="copyCode(this)">Copy</button>
                                                <pre><code># Create a histogram for the 'height' feature
plt.hist(nutri['height'], bins=15, edgecolor='black', alpha=0.7)
plt.xlabel('Height (cm)')
plt.ylabel('Frequency')
plt.title('Distribution of Height')
plt.show()</code></pre>
                                            </div>
                                            
                                            <div class="viz-container">
                                                <div class="viz-title">Histogram of Height</div>
                                                <div class="canvas-container">
                                                    <canvas id="histogram" width="600" height="400"></canvas>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                    
                                    <div class="accordion-item">
                                        <div class="accordion-header" onclick="toggleAccordion(this)">
                                            <span>Density Plot</span>
                                            <i class="fas fa-chevron-down accordion-icon"></i>
                                        </div>
                                        <div class="accordion-body">
                                            <p>A density plot is a smoothed version of a histogram that shows the distribution of a continuous variable. It is particularly useful when you want to compare the distribution of a variable across different groups.</p>
                                            
                                            <div class="code-block">
                                                <span class="code-language">Python</span>
                                                <button class="copy-btn" onclick="copyCode(this)">Copy</button>
                                                <pre><code>import seaborn as sns

# Create a density plot for the 'height' feature by gender
sns.kdeplot(data=nutri, x='height', hue='gender', fill=True, common_norm=False)
plt.xlabel('Height (cm)')
plt.ylabel('Density')
plt.title('Density Plot of Height by Gender')
plt.show()</code></pre>
                                            </div>
                                            
                                            <div class="viz-container">
                                                <div class="viz-title">Density Plot of Height by Gender</div>
                                                <div class="canvas-container">
                                                    <canvas id="densityplot" width="600" height="400"></canvas>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div id="bivariate" class="tab-content">
                            <div class="tab-pane">
                                <h4>1.5.3 Data Visualization in a Bivariate Setting</h4>
                                <p>When we have two variables, we can explore the relationship between them using various visualization techniques. The choice of visualization depends on the types of the two variables.</p>
                                
                                <div class="accordion">
                                    <div class="accordion-item">
                                        <div class="accordion-header" onclick="toggleAccordion(this)">
                                            <span>Scatter Plot (Two Quantitative Variables)</span>
                                            <i class="fas fa-chevron-down accordion-icon"></i>
                                        </div>
                                        <div class="accordion-body">
                                            <p>A scatter plot is used to visualize the relationship between two quantitative variables. Each point represents an observation in the dataset, with its position determined by the values of the two variables.</p>
                                            
                                            <div class="code-block">
                                                <span class="code-language">Python</span>
                                                <button class="copy-btn" onclick="copyCode(this)">Copy</button>
                                                <pre><code># Create a scatter plot of height vs. weight
plt.scatter(nutri['height'], nutri['weight'], alpha=0.6)
plt.xlabel('Height (cm)')
plt.ylabel('Weight (kg)')
plt.title('Height vs. Weight')
plt.show()</code></pre>
                                            </div>
                                            
                                            <div class="viz-container">
                                                <div class="viz-title">Scatter Plot of Height vs. Weight</div>
                                                <div class="canvas-container">
                                                    <canvas id="scatterplot" width="600" height="400"></canvas>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                    
                                    <div class="accordion-item">
                                        <div class="accordion-header" onclick="toggleAccordion(this)">
                                            <span>Box Plot by Category (One Quantitative, One Qualitative)</span>
                                            <i class="fas fa-chevron-down accordion-icon"></i>
                                        </div>
                                        <div class="accordion-body">
                                            <p>When we have one quantitative and one qualitative variable, we can use box plots to compare the distribution of the quantitative variable across different categories of the qualitative variable.</p>
                                            
                                            <div class="code-block">
                                                <span class="code-language">Python</span>
                                                <button class="copy-btn" onclick="copyCode(this)">Copy</button>
                                                <pre><code># Create box plots of height by gender
nutri.boxplot(column='height', by='gender')
plt.ylabel('Height (cm)')
plt.title('Height by Gender')
plt.suptitle('')  # Remove the default title
plt.show()</code></pre>
                                            </div>
                                            
                                            <div class="viz-container">
                                                <div class="viz-title">Box Plot of Height by Gender</div>
                                                <div class="canvas-container">
                                                    <canvas id="boxplotbygender" width="600" height="400"></canvas>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                    
                                    <div class="accordion-item">
                                        <div class="accordion-header" onclick="toggleAccordion(this)">
                                            <span>Heatmap (Two Qualitative Variables)</span>
                                            <i class="fas fa-chevron-down accordion-icon"></i>
                                        </div>
                                        <div class="accordion-body">
                                            <p>When we have two qualitative variables, we can use a heatmap to visualize the relationship between them. The heatmap shows the frequency or proportion of observations for each combination of categories.</p>
                                            
                                            <div class="code-block">
                                                <span class="code-language">Python</span>
                                                <button class="copy-btn" onclick="copyCode(this)">Copy</button>
                                                <pre><code>import seaborn as sns

# Create a cross-tabulation
cross_tab = pd.crosstab(nutri['gender'], nutri['situation'])

# Create a heatmap
sns.heatmap(cross_tab, annot=True, fmt='d', cmap='Blues')
plt.title('Heatmap of Gender vs. Living Situation')
plt.show()</code></pre>
                                            </div>
                                            
                                            <div class="viz-container">
                                                <div class="viz-title">Heatmap of Gender vs. Living Situation</div>
                                                <div class="canvas-container">
                                                    <canvas id="heatmap" width="600" height="400"></canvas>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </section>

            <!-- Chapter 2 Section -->
            <section id="chapter2" class="section">
                <h2>Chapter 2: Statistical Learning</h2>
                
                <div id="chapter2-1" class="subsection">
                    <h3>2.1 Introduction</h3>
                    <p>Statistical learning refers to a vast set of tools for understanding data. These tools can be classified as supervised or unsupervised. Broadly speaking, supervised statistical learning involves building a statistical model for predicting, or estimating, an output based on one or more inputs. Unsupervised statistical learning, on the other hand, involves learning the structure and relationships in data without an associated outcome variable.</p>
                    
                    <p>In this chapter, we introduce the main ingredients of statistical learning. We distinguish between supervised and unsupervised learning techniques, and discuss how we can assess the predictive performance of (un)supervised learning methods. An important part of statistical learning is the modeling of data. We introduce various useful models in data science including linear, multivariate Gaussian, and Bayesian models.</p>
                </div>
                
                <div id="chapter2-2" class="subsection">
                    <h3>2.2 Supervised and Unsupervised Learning</h3>
                    <p>Statistical learning can be divided into two main categories: supervised learning and unsupervised learning.</p>
                    
                    <div class="tabs">
                        <div class="tab-buttons">
                            <button class="tab-button active" onclick="openTab(event, 'supervised')">Supervised Learning</button>
                            <button class="tab-button" onclick="openTab(event, 'unsupervised')">Unsupervised Learning</button>
                        </div>
                        
                        <div id="supervised" class="tab-content">
                            <div class="tab-pane active">
                                <p>In <strong>supervised learning</strong>, we have a set of observations where we know the outcome for each observation. The goal is to build a model that can predict the outcome for new observations based on their features. Supervised learning can be further divided into:</p>
                                
                                <ul>
                                    <li><strong>Regression:</strong> The outcome is a continuous variable.</li>
                                    <li><strong>Classification:</strong> The outcome is a categorical variable.</li>
                                </ul>
                                
                                <div class="alert alert-info">
                                    <strong>Example:</strong> Predicting house prices based on features like size, location, and number of rooms is a regression problem. Predicting whether an email is spam or not spam based on its content is a classification problem.
                                </div>
                                
                                <div class="code-block">
                                    <span class="code-language">Python</span>
                                    <button class="copy-btn" onclick="copyCode(this)">Copy</button>
                                    <pre><code># Example of supervised learning with scikit-learn
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create and train a linear regression model
model = LinearRegression()
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
print(f"Mean Squared Error: {mse:.2f}")</code></pre>
                                </div>
                            </div>
                        </div>
                        
                        <div id="unsupervised" class="tab-content">
                            <div class="tab-pane">
                                <p>In <strong>unsupervised learning</strong>, we have a set of observations without known outcomes. The goal is to discover patterns, structures, or relationships in the data. Common unsupervised learning tasks include:</p>
                                
                                <ul>
                                    <li><strong>Clustering:</strong> Grouping similar observations together.</li>
                                    <li><strong>Dimensionality reduction:</strong> Reducing the number of features while preserving important information.</li>
                                    <li><strong>Association rule learning:</strong> Discovering relationships between variables.</li>
                                </ul>
                                
                                <div class="alert alert-info">
                                    <strong>Example:</strong> Grouping customers into segments based on their purchasing behavior is a clustering problem. Reducing the dimensionality of a dataset with many features to a smaller set of features that capture most of the variance is a dimensionality reduction problem.
                                </div>
                                
                                <div class="code-block">
                                    <span class="code-language">Python</span>
                                    <button class="copy-btn" onclick="copyCode(this)">Copy</button>
                                    <pre><code># Example of unsupervised learning with scikit-learn
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA

# Clustering example
kmeans = KMeans(n_clusters=3, random_state=42)
clusters = kmeans.fit_predict(X)

# Dimensionality reduction example
pca = PCA(n_components=2)
X_reduced = pca.fit_transform(X)</code></pre>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
                
                <div id="chapter2-3" class="subsection">
                    <h3>2.3 Training and Test Loss</h3>
                    <p>In supervised learning, we need a way to measure how well our model is performing. This is typically done using a loss function, which quantifies the difference between the predicted values and the actual values. The choice of loss function depends on the type of problem:</p>
                    
                    <ul>
                        <li>For regression problems, common loss functions include mean squared error (MSE) and mean absolute error (MAE).</li>
                        <li>For classification problems, common loss functions include cross-entropy loss and hinge loss.</li>
                    </ul>
                    
                    <p>To evaluate a model, we typically split the data into a training set and a test set. The model is trained on the training set, and its performance is evaluated on the test set. This helps us assess how well the model generalizes to new, unseen data.</p>
                    
                    <div class="code-block">
                        <span class="code-language">Python</span>
                        <button class="copy-btn" onclick="copyCode(this)">Copy</button>
                        <pre><code># Example of calculating training and test loss
from sklearn.metrics import mean_squared_error, accuracy_score

# For regression
train_loss = mean_squared_error(y_train, model.predict(X_train))
test_loss = mean_squared_error(y_test, model.predict(X_test))

print(f"Training MSE: {train_loss:.2f}")
print(f"Test MSE: {test_loss:.2f}")

# For classification
train_acc = accuracy_score(y_train, classifier.predict(X_train))
test_acc = accuracy_score(y_test, classifier.predict(X_test))

print(f"Training Accuracy: {train_acc:.2f}")
print(f"Test Accuracy: {test_acc:.2f}")</code></pre>
                    </div>
                    
                    <div class="viz-container">
                        <div class="viz-title">Training vs. Test Loss</div>
                        <div class="canvas-container">
                            <canvas id="train-test-loss" width="600" height="400"></canvas>
                        </div>
                    </div>
                </div>
                
                <div id="chapter2-4" class="subsection">
                    <h3>2.4 Tradeoffs in Statistical Learning</h3>
                    <p>When building statistical learning models, we often face several tradeoffs:</p>
                    
                    <div class="accordion">
                        <div class="accordion-item">
                            <div class="accordion-header" onclick="toggleAccordion(this)">
                                <span>Bias-Variance Tradeoff</span>
                                <i class="fas fa-chevron-down accordion-icon"></i>
                            </div>
                            <div class="accordion-body">
                                <p>The <strong>bias-variance tradeoff</strong> is a fundamental concept in statistical learning. Models with high bias are too simple and may underfit the data, while models with high variance are too complex and may overfit the data. The goal is to find a model with the right balance between bias and variance.</p>
                                
                                <div class="viz-container">
                                    <div class="viz-title">Bias-Variance Tradeoff</div>
                                    <div class="canvas-container">
                                        <canvas id="bias-variance" width="600" height="400"></canvas>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="accordion-item">
                            <div class="accordion-header" onclick="toggleAccordion(this)">
                                <span>Model Complexity vs. Interpretability</span>
                                <i class="fas fa-chevron-down accordion-icon"></i>
                            </div>
                            <div class="accordion-body">
                                <p>More complex models often have better predictive performance but are harder to interpret. Simpler models may not perform as well but are easier to understand and explain. The choice depends on the specific application and the importance of interpretability.</p>
                            </div>
                        </div>
                        
                        <div class="accordion-item">
                            <div class="accordion-header" onclick="toggleAccordion(this)">
                                <span>Computational Cost vs. Performance</span>
                                <i class="fas fa-chevron-down accordion-icon"></i>
                            </div>
                            <div class="accordion-body">
                                <p>Some models, like deep neural networks, can achieve excellent performance but require significant computational resources. Simpler models may not perform as well but are much faster to train and use. The choice depends on the available resources and the performance requirements.</p>
                            </div>
                        </div>
                    </div>
                </div>
                
                <div id="chapter2-5" class="subsection">
                    <h3>2.5 Estimating Risk</h3>
                    <p>In statistical learning, <strong>risk</strong> refers to the expected loss of a model. Since we typically don't know the true distribution of the data, we can't calculate the true risk directly. Instead, we estimate it using various methods:</p>
                    
                    <div class="tabs">
                        <div class="tab-buttons">
                            <button class="tab-button active" onclick="openTab(event, 'insample')">In-Sample Risk</button>
                            <button class="tab-button" onclick="openTab(event, 'crossvalidation')">Cross-Validation</button>
                        </div>
                        
                        <div id="insample" class="tab-content">
                            <div class="tab-pane active">
                                <p><strong>In-sample risk</strong> is the average loss on the training data. While easy to compute, it tends to be overly optimistic because the model has already seen the training data. Adjustments, such as adding a penalty for model complexity, can make in-sample risk a better estimate of the true risk.</p>
                                
                                <div class="formula">
                                    R_in = (1/n) Σᵢ₌₁ⁿ L(yᵢ, g(xᵢ))
                                </div>
                                
                                <p>where L is the loss function, g is the model, and (xᵢ, yᵢ) are the training data.</p>
                                
                                <div class="alert alert-warning">
                                    <strong>Warning:</strong> In-sample risk is often an underestimate of the true risk, especially for complex models that can easily overfit the training data.
                                </div>
                            </div>
                        </div>
                        
                        <div id="crossvalidation" class="tab-content">
                            <div class="tab-pane">
                                <p><strong>Cross-validation</strong> is a more reliable method for estimating the true risk of a model. The basic idea is to split the data into multiple parts, train the model on some parts, and test it on the remaining part. This process is repeated multiple times, and the results are averaged.</p>
                                
                                <div class="accordion">
                                    <div class="accordion-item">
                                        <div class="accordion-header" onclick="toggleAccordion(this)">
                                            <span>K-Fold Cross-Validation</span>
                                            <i class="fas fa-chevron-down accordion-icon"></i>
                                        </div>
                                        <div class="accordion-body">
                                            <p>In k-fold cross-validation, the data is divided into k equal-sized parts. The model is trained on k-1 parts and tested on the remaining part. This process is repeated k times, with each part used as the test set exactly once.</p>
                                            
                                            <div class="code-block">
                                                <span class="code-language">Python</span>
                                                <button class="copy-btn" onclick="copyCode(this)">Copy</button>
                                                <pre><code>from sklearn.model_selection import cross_val_score

# Perform 5-fold cross-validation
scores = cross_val_score(model, X, y, cv=5, scoring='neg_mean_squared_error')
rmse_scores = np.sqrt(-scores)

print(f"RMSE: {rmse_scores.mean():.2f} (+/- {rmse_scores.std() * 2:.2f})")</code></pre>
                                            </div>
                                        </div>
                                    </div>
                                    
                                    <div class="accordion-item">
                                        <div class="accordion-header" onclick="toggleAccordion(this)">
                                            <span>Leave-One-Out Cross-Validation</span>
                                            <i class="fas fa-chevron-down accordion-icon"></i>
                                        </div>
                                        <div class="accordion-body">
                                            <p>Leave-one-out cross-validation is a special case of k-fold cross-validation where k equals the number of observations. The model is trained on all observations except one, and tested on the left-out observation. This process is repeated for each observation.</p>
                                            
                                            <div class="alert alert-info">
                                                <strong>Note:</strong> Leave-one-out cross-validation can be computationally expensive for large datasets but provides an almost unbiased estimate of the true risk.
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
                
                <div id="chapter2-6" class="subsection">
                    <h3>2.6 Modeling Data</h3>
                    <p>Modeling is a fundamental aspect of statistical learning. A model is a simplified representation of reality that captures the essential features of the data. In statistical learning, we typically use probabilistic models that describe the relationship between features and outcomes.</p>
                    
                    <div class="alert alert-success">
                        <strong>Key Idea:</strong> The choice of model depends on the nature of the data and the problem at hand. Simple models are easier to interpret and less prone to overfitting, while complex models can capture more intricate patterns but may be harder to interpret.
                    </div>
                    
                    <p>Some common types of models in statistical learning include:</p>
                    
                    <ul>
                        <li><strong>Linear models:</strong> Assume a linear relationship between features and outcomes.</li>
                        <li><strong>Nonlinear models:</strong> Capture nonlinear relationships between features and outcomes.</li>
                        <li><strong>Tree-based models:</strong> Use a tree-like structure to make decisions.</li>
                        <li><strong>Neural networks:</strong> Use interconnected layers of nodes to model complex relationships.</li>
                        <li><strong>Bayesian models:</strong> Incorporate prior knowledge and update beliefs based on data.</li>
                    </ul>
                </div>
                
                <div id="chapter2-7" class="subsection">
                    <h3>2.7 Multivariate Normal Models</h3>
                    <p>The multivariate normal distribution is a generalization of the one-dimensional normal distribution to higher dimensions. It is widely used in statistical learning due to its mathematical tractability and the central limit theorem.</p>
                    
                    <p>A random vector X = [X₁, ..., Xₚ]ᵀ follows a multivariate normal distribution with mean vector μ and covariance matrix Σ, denoted as X ~ Nₚ(μ, Σ), if its probability density function is given by:</p>
                    
                    <div class="formula">
                        f(x) = (1/((2π)^(p/2) |Σ|^(1/2))) exp(-1/2 (x-μ)ᵀ Σ⁻¹ (x-μ))
                    </div>
                    
                    <p>The multivariate normal distribution has several important properties:</p>
                    
                    <ul>
                        <li>Linear combinations of normally distributed random variables are also normally distributed.</li>
                        <li>Marginal distributions of a multivariate normal distribution are also normal.</li>
                        <li>Conditional distributions of a multivariate normal distribution are also normal.</li>
                    </ul>
                    
                    <div class="code-block">
                        <span class="code-language">Python</span>
                        <button class="copy-btn" onclick="copyCode(this)">Copy</button>
                        <pre><code>import numpy as np
from scipy.stats import multivariate_normal

# Define the mean vector and covariance matrix
mu = np.array([0, 0])
cov = np.array([[1, 0.5], [0.5, 1]])

# Create a multivariate normal distribution
dist = multivariate_normal(mu, cov)

# Generate random samples
samples = dist.rvs(size=1000)

# Calculate the probability density at a point
x = np.array([0.5, 0.5])
pdf = dist.pdf(x)
print(f"PDF at {x}: {pdf:.4f}")</code></pre>
                    </div>
                    
                    <div class="viz-container">
                        <div class="viz-title">Multivariate Normal Distribution</div>
                        <div class="canvas-container">
                            <canvas id="multivariate-normal" width="600" height="400"></canvas>
                        </div>
                    </div>
                </div>
                
                <div id="chapter2-8" class="subsection">
                    <h3>2.8 Normal Linear Models</h3>
                    <p>Normal linear models are a fundamental class of models in statistical learning. They assume that the relationship between the features and the outcome is linear, and that the errors are normally distributed.</p>
                    
                    <p>A normal linear model can be written as:</p>
                    
                    <div class="formula">
                        y = Xβ + ε
                    </div>
                    
                    <p>where y is the outcome vector, X is the design matrix, β is the coefficient vector, and ε ~ N(0, σ²I) is the error vector.</p>
                    
                    <p>The coefficients of a normal linear model are typically estimated using the method of least squares, which minimizes the sum of squared residuals:</p>
                    
                    <div class="formula">
                        β̂ = argmin_β (y - Xβ)ᵀ(y - Xβ) = (XᵀX)⁻¹Xᵀy
                    </div>
                    
                    <div class="code-block">
                        <span class="code-language">Python</span>
                        <button class="copy-btn" onclick="copyCode(this)">Copy</button>
                        <pre><code>import statsmodels.api as sm

# Add a constant to the design matrix
X = sm.add_constant(X)

# Fit a normal linear model
model = sm.OLS(y, X).fit()

# Print the model summary
print(model.summary())

# Make predictions
y_pred = model.predict(X)</code></pre>
                    </div>
                    
                    <div class="viz-container">
                        <div class="viz-title">Linear Regression Example</div>
                        <div class="canvas-container">
                            <canvas id="linear-regression" width="600" height="400"></canvas>
                        </div>
                    </div>
                </div>
                
                <div id="chapter2-9" class="subsection">
                    <h3>2.9 Bayesian Learning</h3>
                    <p>Bayesian learning is a probabilistic approach to statistical learning that incorporates prior knowledge and updates beliefs based on data. In Bayesian learning, model parameters are treated as random variables with probability distributions, rather than fixed but unknown quantities.</p>
                    
                    <p>The foundation of Bayesian learning is Bayes' theorem:</p>
                    
                    <div class="formula">
                        P(θ|D) = (P(D|θ)P(θ)) / P(D)
                    </div>
                    
                    <p>where θ represents the model parameters, D represents the data, P(θ|D) is the posterior distribution, P(D|θ) is the likelihood, P(θ) is the prior distribution, and P(D) is the evidence.</p>
                    
                    <div class="alert alert-info">
                        <strong>Key Components of Bayesian Learning:</strong>
                        <ul>
                            <li><strong>Prior Distribution:</strong> Represents our beliefs about the parameters before seeing the data.</li>
                            <li><strong>Likelihood:</strong> Describes how likely the data is given the parameters.</li>
                            <li><strong>Posterior Distribution:</strong> Represents our updated beliefs about the parameters after seeing the data.</li>
                        </ul>
                    </div>
                    
                    <div class="code-block">
                        <span class="code-language">Python</span>
                        <button class="copy-btn" onclick="copyCode(this)">Copy</button>
                        <pre><code>import pymc3 as pm
import numpy as np

# Generate some data
np.random.seed(42)
x = np.linspace(0, 10, 100)
true_intercept = 1
true_slope = 2
y = true_intercept + true_slope * x + np.random.normal(0, 1, size=100)

# Define the Bayesian linear regression model
with pm.Model() as model:
    # Priors for unknown model parameters
    intercept = pm.Normal('intercept', mu=0, sigma=10)
    slope = pm.Normal('slope', mu=0, sigma=10)
    sigma = pm.HalfNormal('sigma', sigma=1)
    
    # Expected value of outcome
    mu = intercept + slope * x
    
    # Likelihood (sampling distribution) of observations
    Y_obs = pm.Normal('Y_obs', mu=mu, sigma=sigma, observed=y)
    
    # Draw posterior samples
    trace = pm.sample(1000, tune=1000)</code></pre>
                    </div>
                    
                    <div class="viz-container">
                        <div class="viz-title">Bayesian Linear Regression</div>
                        <div class="canvas-container">
                            <canvas id="bayesian-regression" width="600" height="400"></canvas>
                        </div>
                    </div>
                </div>
            </section>

            <!-- Additional chapters would continue here with similar structure -->
            <!-- For brevity, I'm including just a few more chapters with key content -->

            <!-- Chapter 3 Section -->
            <section id="chapter3" class="section">
                <h2>Chapter 3: Monte Carlo Methods</h2>
                
                <div id="chapter3-1" class="subsection">
                    <h3>3.1 Introduction</h3>
                    <p>Monte Carlo methods are a broad class of computational algorithms that rely on repeated random sampling to obtain numerical results. They are often used in physical and mathematical problems and are most useful when it is difficult or impossible to use other approaches.</p>
                    
                    <p>Monte Carlo methods are mainly used for three types of problems:</p>
                    <ul>
                        <li><strong>Simulation:</strong> Generating samples from a probability distribution to understand its properties.</li>
                        <li><strong>Estimation:</strong> Estimating quantities that are difficult to compute directly.</li>
                        <li><strong>Optimization:</strong> Finding the optimal solution to a problem that is difficult to solve analytically.</li>
                    </ul>
                </div>
                
                <div id="chapter3-2" class="subsection">
                    <h3>3.2 Monte Carlo Sampling</h3>
                    <p>Monte Carlo sampling involves generating random samples from a probability distribution. This is a fundamental step in many Monte Carlo methods.</p>
                    
                    <div class="accordion">
                        <div class="accordion-item">
                            <div class="accordion-header" onclick="toggleAccordion(this)">
                                <span>Generating Random Numbers</span>
                                <i class="fas fa-chevron-down accordion-icon"></i>
                            </div>
                            <div class="accordion-body">
                                <p>The foundation of Monte Carlo sampling is the ability to generate random numbers. Most programming languages provide functions to generate pseudo-random numbers, which are deterministic sequences that appear random.</p>
                                
                                <div class="code-block">
                                    <span class="code-language">Python</span>
                                    <button class="copy-btn" onclick="copyCode(this)">Copy</button>
                                    <pre><code>import numpy as np

# Generate uniform random numbers
uniform_samples = np.random.rand(1000)

# Generate normal random numbers
normal_samples = np.random.randn(1000)

# Set the random seed for reproducibility
np.random.seed(42)
reproducible_samples = np.random.rand(10)</code></pre>
                                </div>
                            </div>
                        </div>
                        
                        <div class="accordion-item">
                            <div class="accordion-header" onclick="toggleAccordion(this)">
                                <span>Simulating Random Variables</span>
                                <i class="fas fa-chevron-down accordion-icon"></i>
                            </div>
                            <div class="accordion-body">
                                <p>To generate samples from a specific probability distribution, we can use various techniques:</p>
                                
                                <ul>
                                    <li><strong>Inverse Transform Method:</strong> Generate a uniform random number u and apply the inverse of the cumulative distribution function (CDF) to get a sample from the desired distribution.</li>
                                    <li><strong>Accept-Reject Method:</strong> Generate samples from a proposal distribution and accept or reject them based on a criterion.</li>
                                    <li><strong>Specialized Methods:</strong> Some distributions have specialized methods for generating samples.</li>
                                </ul>
                                
                                <div class="code-block">
                                    <span class="code-language">Python</span>
                                    <button class="copy-btn" onclick="copyCode(this)">Copy</button>
                                    <pre><code>import numpy as np
from scipy.stats import norm, expon

# Inverse transform method for exponential distribution
def exponential_samples(lam, size):
    u = np.random.rand(size)
    return -np.log(1 - u) / lam

# Accept-reject method for normal distribution
def normal_samples_accept_reject(size):
    samples = []
    while len(samples) < size:
        # Generate from proposal distribution (exponential)
        x = expon.rvs(scale=1)
        # Accept or reject
        u = np.random.rand()
        if u < norm.pdf(x) / (np.sqrt(2*np.pi) * np.exp(-x)):
            samples.append(x)
    return np.array(samples)</code></pre>
                                </div>
                            </div>
                        </div>
                        
                        <div class="accordion-item">
                            <div class="accordion-header" onclick="toggleAccordion(this)">
                                <span>Markov Chain Monte Carlo (MCMC)</span>
                                <i class="fas fa-chevron-down accordion-icon"></i>
                            </div>
                            <div class="accordion-body">
                                <p>Markov Chain Monte Carlo (MCMC) methods are a class of algorithms for sampling from probability distributions based on constructing a Markov chain that has the desired distribution as its equilibrium distribution.</p>
                                
                                <div class="alert alert-info">
                                    <strong>Popular MCMC Algorithms:</strong>
                                    <ul>
                                        <li><strong>Metropolis-Hastings Algorithm:</strong> Proposes a new state and accepts or rejects it based on a probability.</li>
                                        <li><strong>Gibbs Sampling:</strong> Samples from the conditional distribution of each variable given the others.</li>
                                        <li><strong>Hamiltonian Monte Carlo:</strong> Uses gradient information to propose new states.</li>
                                    </ul>
                                </div>
                                
                                <div class="code-block">
                                    <span class="code-language">Python</span>
                                    <button class="copy-btn" onclick="copyCode(this)">Copy</button>
                                    <pre><code>import numpy as np

def metropolis_hastings(target_dist, proposal_dist, initial_state, n_samples):
    samples = [initial_state]
    current_state = initial_state
    
    for _ in range(n_samples):
        # Propose a new state
        proposed_state = proposal_dist(current_state)
        
        # Calculate acceptance probability
        acceptance_prob = min(1, target_dist(proposed_state) * proposal_dist(current_state, proposed_state) / 
                                     (target_dist(current_state) * proposal_dist(proposed_state, current_state)))
        
        # Accept or reject the proposed state
        if np.random.rand() < acceptance_prob:
            current_state = proposed_state
        
        samples.append(current_state)
    
    return np.array(samples)</code></pre>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
                
                <div id="chapter3-3" class="subsection">
                    <h3>3.3 Monte Carlo Estimation</h3>
                    <p>Monte Carlo estimation involves using random sampling to estimate quantities that are difficult to compute directly. The basic idea is to express the quantity of interest as an expectation and then approximate this expectation using sample averages.</p>
                    
                    <div class="formula">
                        E[f(X)] ≈ (1/n) Σᵢ₌₁ⁿ f(Xᵢ)
                    </div>
                    
                    <p>where X₁, ..., Xₙ are independent samples from the distribution of X.</p>
                    
                    <div class="accordion">
                        <div class="accordion-item">
                            <div class="accordion-header" onclick="toggleAccordion(this)">
                                <span>Crude Monte Carlo</span>
                                <i class="fas fa-chevron-down accordion-icon"></i>
                            </div>
                            <div class="accordion-body">
                                <p>Crude Monte Carlo is the simplest form of Monte Carlo estimation. It involves generating independent samples from the distribution of interest and using the sample average as an estimate of the expectation.</p>
                                
                                <div class="code-block">
                                    <span class="code-language">Python</span>
                                    <button class="copy-btn" onclick="copyCode(this)">Copy</button>
                                    <pre><code>import numpy as np

# Estimate π using Monte Carlo
def estimate_pi(n_samples):
    # Generate random points in the unit square
    x = np.random.rand(n_samples)
    y = np.random.rand(n_samples)
    
    # Count points inside the unit circle
    inside_circle = (x**2 + y**2) <= 1
    
    # Estimate π
    pi_estimate = 4 * np.mean(inside_circle)
    
    return pi_estimate

# Estimate π with different sample sizes
for n in [100, 1000, 10000, 100000]:
    pi_est = estimate_pi(n)
    print(f"Samples: {n}, Estimated π: {pi_est:.4f}")</code></pre>
                                </div>
                                
                                <div class="viz-container">
                                    <div class="viz-title">Monte Carlo Estimation of π</div>
                                    <div class="canvas-container">
                                        <canvas id="pi-estimation" width="600" height="400"></canvas>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="accordion-item">
                            <div class="accordion-header" onclick="toggleAccordion(this)">
                                <span>Bootstrap Method</span>
                                <i class="fas fa-chevron-down accordion-icon"></i>
                            </div>
                            <div class="accordion-body">
                                <p>The bootstrap method is a resampling technique used to estimate the sampling distribution of a statistic. It involves generating bootstrap samples by sampling with replacement from the original data and computing the statistic of interest for each bootstrap sample.</p>
                                
                                <div class="code-block">
                                    <span class="code-language">Python</span>
                                    <button class="copy-btn" onclick="copyCode(this)">Copy</button>
                                    <pre><code>import numpy as np

def bootstrap(data, statistic, n_bootstrap):
    bootstrap_stats = []
    n = len(data)
    
    for _ in range(n_bootstrap):
        # Generate a bootstrap sample
        bootstrap_sample = np.random.choice(data, size=n, replace=True)
        
        # Compute the statistic
        bootstrap_stat = statistic(bootstrap_sample)
        bootstrap_stats.append(bootstrap_stat)
    
    return np.array(bootstrap_stats)

# Example: Bootstrap confidence interval for the mean
data = np.random.normal(10, 2, 100)
bootstrap_means = bootstrap(data, np.mean, 1000)

# Calculate the 95% confidence interval
ci_lower = np.percentile(bootstrap_means, 2.5)
ci_upper = np.percentile(bootstrap_means, 97.5)

print(f"95% Confidence Interval: [{ci_lower:.2f}, {ci_upper:.2f}]")</code></pre>
                                </div>
                            </div>
                        </div>
                        
                        <div class="accordion-item">
                            <div class="accordion-header" onclick="toggleAccordion(this)">
                                <span>Variance Reduction</span>
                                <i class="fas fa-chevron-down accordion-icon"></i>
                            </div>
                            <div class="accordion-body">
                                <p>Variance reduction techniques are used to improve the efficiency of Monte Carlo estimation by reducing the variance of the estimator. Common variance reduction techniques include:</p>
                                
                                <ul>
                                    <li><strong>Antithetic Variates:</strong> Using negatively correlated samples to reduce variance.</li>
                                    <li><strong>Control Variates:</strong> Using a known quantity with a known expectation to reduce variance.</li>
                                    <li><strong>Importance Sampling:</strong> Sampling from a different distribution to reduce variance.</li>
                                    <li><strong>Stratified Sampling:</strong> Dividing the sample space into strata and sampling from each stratum.</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                </div>
                
                <div id="chapter3-4" class="subsection">
                    <h3>3.4 Monte Carlo for Optimization</h3>
                    <p>Monte Carlo methods can also be used for optimization problems, especially when the objective function is complex or has many local optima.</p>
                    
                    <div class="accordion">
                        <div class="accordion-item">
                            <div class="accordion-header" onclick="toggleAccordion(this)">
                                <span>Simulated Annealing</span>
                                <i class="fas fa-chevron-down accordion-icon"></i>
                            </div>
                            <div class="accordion-body">
                                <p>Simulated annealing is a probabilistic technique for approximating the global optimum of a function. It is inspired by the annealing process in metallurgy, where a material is heated and then slowly cooled to reduce defects.</p>
                                
                                <div class="code-block">
                                    <span class="code-language">Python</span>
                                    <button class="copy-btn" onclick="copyCode(this)">Copy</button>
                                    <pre><code>import numpy as np

def simulated_annealing(objective_func, initial_state, temperature, cooling_rate, n_iterations):
    current_state = initial_state
    current_value = objective_func(current_state)
    best_state = current_state
    best_value = current_value
    
    for i in range(n_iterations):
        # Generate a new state
        new_state = current_state + np.random.randn(*current_state.shape)
        new_value = objective_func(new_state)
        
        # Calculate acceptance probability
        delta = new_value - current_value
        acceptance_prob = np.exp(-delta / temperature) if delta > 0 else 1
        
        # Accept or reject the new state
        if np.random.rand() < acceptance_prob:
            current_state = new_state
            current_value = new_value
            
            # Update the best state
            if current_value < best_value:
                best_state = current_state
                best_value = current_value
        
        # Cool down
        temperature *= cooling_rate
    
    return best_state, best_value</code></pre>
                                </div>
                            </div>
                        </div>
                        
                        <div class="accordion-item">
                            <div class="accordion-header" onclick="toggleAccordion(this)">
                                <span>Cross-Entropy Method</span>
                                <i class="fas fa-chevron-down accordion-icon"></i>
                            </div>
                            <div class="accordion-body">
                                <p>The cross-entropy method is a Monte Carlo technique for optimization and rare event simulation. It involves generating samples from a parameterized distribution, selecting the best samples, and updating the distribution parameters to generate better samples in the next iteration.</p>
                            </div>
                        </div>
                    </div>
                </div>
            </section>

            <!-- Chapter 4 Section -->
            <section id="chapter4" class="section">
                <h2>Chapter 4: Unsupervised Learning</h2>
                
                <div id="chapter4-1" class="subsection">
                    <h3>4.1 Introduction</h3>
                    <p>Unsupervised learning is a type of machine learning where the algorithm learns patterns from unlabeled data. Unlike supervised learning, there are no output variables to predict. Instead, the goal is to discover the underlying structure or distribution in the data.</p>
                    
                    <p>Common unsupervised learning tasks include:</p>
                    <ul>
                        <li><strong>Clustering:</strong> Grouping similar data points together.</li>
                        <li><strong>Dimensionality reduction:</strong> Reducing the number of features while preserving important information.</li>
                        <li><strong>Density estimation:</strong> Estimating the probability density function of the data.</li>
                        <li><strong>Anomaly detection:</strong> Identifying unusual data points that differ from the majority of the data.</li>
                    </ul>
                </div>
                
                <div id="chapter4-2" class="subsection">
                    <h3>4.2 Risk and Loss in Unsupervised Learning</h3>
                    <p>In unsupervised learning, defining risk and loss is more challenging than in supervised learning because there are no labeled outputs to compare against. Instead, we often define loss based on the quality of the learned structure or representation.</p>
                    
                    <p>For clustering, common loss functions include:</p>
                    <ul>
                        <li><strong>Within-cluster sum of squares:</strong> Measures the compactness of clusters.</li>
                        <li><strong>Silhouette score:</strong> Measures how similar an object is to its own cluster compared to other clusters.</li>
                        <li><strong>Davies-Bouldin index:</strong> Measures the average similarity between each cluster and its most similar one.</li>
                    </ul>
                    
                    <p>For dimensionality reduction, common loss functions include:</p>
                    <ul>
                        <li><strong>Reconstruction error:</strong> Measures how well the original data can be reconstructed from the reduced representation.</li>
                        <li><strong>Preservation of local or global structure:</strong> Measures how well the relationships between data points are preserved.</li>
                    </ul>
                </div>
                
                <div id="chapter4-3" class="subsection">
                    <h3>4.3 Expectation–Maximization (EM) Algorithm</h3>
                    <p>The Expectation-Maximization (EM) algorithm is an iterative method for finding maximum likelihood estimates of parameters in statistical models with latent variables. It consists of two steps:</p>
                    
                    <ol>
                        <li><strong>Expectation (E) step:</strong> Calculate the expected value of the log-likelihood function with respect to the conditional distribution of the latent variables given the observed data and current estimates of the parameters.</li>
                        <li><strong>Maximization (M) step:</strong> Maximize the expected log-likelihood found in the E step with respect to the parameters to obtain updated parameter estimates.</li>
                    </ol>
                    
                    <div class="code-block">
                        <span class="code-language">Python</span>
                        <button class="copy-btn" onclick="copyCode(this)">Copy</button>
                        <pre><code>import numpy as np
from scipy.stats import multivariate_normal

def em_algorithm(X, n_components, max_iter=100, tol=1e-6):
    n_samples, n_features = X.shape
    
    # Initialize parameters
    weights = np.ones(n_components) / n_components
    means = X[np.random.choice(n_samples, n_components, replace=False)]
    covariances = [np.cov(X.T) for _ in range(n_components)]
    
    log_likelihood = 0
    prev_log_likelihood = -np.inf
    
    for iteration in range(max_iter):
        # E-step: Calculate responsibilities
        responsibilities = np.zeros((n_samples, n_components))
        for k in range(n_components):
            responsibilities[:, k] = weights[k] * multivariate_normal.pdf(X, means[k], covariances[k])
        
        responsibilities /= responsibilities.sum(axis=1, keepdims=True)
        
        # M-step: Update parameters
        Nk = responsibilities.sum(axis=0)
        weights = Nk / n_samples
        means = responsibilities.T @ X / Nk[:, np.newaxis]
        
        for k in range(n_components):
            diff = X - means[k]
            covariances[k] = (responsibilities[:, k][:, np.newaxis] * diff).T @ diff / Nk[k]
        
        # Calculate log-likelihood
        log_likelihood = np.sum(np.log(np.sum([weights[k] * multivariate_normal.pdf(X, means[k], covariances[k]) 
                                              for k in range(n_components)], axis=0)))
        
        # Check for convergence
        if np.abs(log_likelihood - prev_log_likelihood) < tol:
            break
            
        prev_log_likelihood = log_likelihood
    
    return weights, means, covariances, responsibilities</code></pre>
                    </div>
                </div>
                
                <div id="chapter4-4" class="subsection">
                    <h3>4.4 Empirical Distribution and Density Estimation</h3>
                    <p>Density estimation is the problem of estimating the probability density function of a random variable based on observed data. There are two main approaches:</p>
                    
                    <div class="tabs">
                        <div class="tab-buttons">
                            <button class="tab-button active" onclick="openTab(event, 'parametric')">Parametric</button>
                            <button class="tab-button" onclick="openTab(event, 'nonparametric')">Non-parametric</button>
                        </div>
                        
                        <div id="parametric" class="tab-content">
                            <div class="tab-pane active">
                                <p><strong>Parametric density estimation</strong> assumes that the data comes from a known family of distributions (e.g., normal, exponential) and estimates the parameters of that distribution.</p>
                                
                                <div class="code-block">
                                    <span class="code-language">Python</span>
                                    <button class="copy-btn" onclick="copyCode(this)">Copy</button>
                                    <pre><code>import numpy as np
from scipy.stats import norm

# Generate data from a normal distribution
np.random.seed(42)
data = np.random.normal(0, 1, 1000)

# Estimate parameters using maximum likelihood
mu_est = np.mean(data)
sigma_est = np.std(data)

print(f"Estimated mean: {mu_est:.4f}")
print(f"Estimated standard deviation: {sigma_est:.4f}")

# Create the estimated distribution
estimated_dist = norm(mu_est, sigma_est)</code></pre>
                                </div>
                            </div>
                        </div>
                        
                        <div id="nonparametric" class="tab-content">
                            <div class="tab-pane">
                                <p><strong>Non-parametric density estimation</strong> does not assume a specific form for the distribution. Common methods include:</p>
                                
                                <ul>
                                    <li><strong>Histogram:</strong> Divides the data range into bins and counts the number of observations in each bin.</li>
                                    <li><strong>Kernel Density Estimation (KDE):</strong> Places a kernel (e.g., Gaussian) at each data point and sums them to get a smooth estimate.</li>
                                </ul>
                                
                                <div class="code-block">
                                    <span class="code-language">Python</span>
                                    <button class="copy-btn" onclick="copyCode(this)">Copy</button>
                                    <pre><code>import numpy as np
from scipy.stats import gaussian_kde
import matplotlib.pyplot as plt

# Generate data
np.random.seed(42)
data = np.concatenate([np.random.normal(0, 1, 500), np.random.normal(5, 1, 500)])

# Kernel density estimation
kde = gaussian_kde(data)
x_grid = np.linspace(-5, 10, 1000)
pdf_est = kde(x_grid)

# Plot the estimated density
plt.figure(figsize=(10, 6))
plt.hist(data, bins=30, density=True, alpha=0.5, label='Histogram')
plt.plot(x_grid, pdf_est, 'r-', label='KDE')
plt.xlabel('Value')
plt.ylabel('Density')
plt.title('Kernel Density Estimation')
plt.legend()
plt.show()</code></pre>
                                </div>
                                
                                <div class="viz-container">
                                    <div class="viz-title">Kernel Density Estimation</div>
                                    <div class="canvas-container">
                                        <canvas id="kde-plot" width="600" height="400"></canvas>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
                
                <div id="chapter4-5" class="subsection">
                    <h3>4.5 Clustering via Mixture Models</h3>
                    <p>Mixture models assume that the data is generated from a mixture of several probability distributions. Each component of the mixture represents a cluster, and the parameters of each component describe the characteristics of that cluster.</p>
                    
                    <div class="accordion">
                        <div class="accordion-item">
                            <div class="accordion-header" onclick="toggleAccordion(this)">
                                <span>Mixture Models</span>
                                <i class="fas fa-chevron-down accordion-icon"></i>
                            </div>
                            <div class="accordion-body">
                                <p>A mixture model is a probabilistic model for representing the presence of subpopulations within an overall population. It assumes that the data is generated from a mixture of several distributions, each with its own parameters.</p>
                                
                                <p>The probability density function of a mixture model with K components is:</p>
                                
                                <div class="formula">
                                    p(x) = Σₖ₌₁ᴷ πₖ fₖ(x; θₖ)
                                </div>
                                
                                <p>where πₖ is the mixing weight for component k, fₖ is the probability density function for component k, and θₖ are the parameters for component k.</p>
                            </div>
                        </div>
                        
                        <div class="accordion-item">
                            <div class="accordion-header" onclick="toggleAccordion(this)">
                                <span>EM Algorithm for Mixture Models</span>
                                <i class="fas fa-chevron-down accordion-icon"></i>
                            </div>
                            <div class="accordion-body">
                                <p>The EM algorithm is commonly used to estimate the parameters of mixture models. In the E-step, it calculates the probability that each data point belongs to each component. In the M-step, it updates the parameters of each component based on these probabilities.</p>
                                
                                <div class="code-block">
                                    <span class="code-language">Python</span>
                                    <button class="copy-btn" onclick="copyCode(this)">Copy</button>
                                    <pre><code>from sklearn.mixture import GaussianMixture
import numpy as np
import matplotlib.pyplot as plt

# Generate data from a mixture of Gaussians
np.random.seed(42)
n_samples = 300
C = np.array([[0.0, -0.7], [3.5, 0.7]])
X = np.r_[
    np.dot(np.random.randn(n_samples, 2), C),
    np.random.randn(n_samples, 2) + np.array([6, 3]),
    np.random.randn(n_samples, 2) + np.array([-3, 5])
]

# Fit a Gaussian Mixture Model
gmm = GaussianMixture(n_components=3, covariance_type='full')
gmm.fit(X)

# Predict cluster labels
labels = gmm.predict(X)

# Plot the results
plt.figure(figsize=(10, 6))
plt.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='viridis')
plt.title('Gaussian Mixture Model Clustering')
plt.show()</code></pre>
                                </div>
                                
                                <div class="viz-container">
                                    <div class="viz-title">Gaussian Mixture Model Clustering</div>
                                    <div class="canvas-container">
                                        <canvas id="gmm-clustering" width="600" height="400"></canvas>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
                
                <div id="chapter4-6" class="subsection">
                    <h3>4.6 Clustering via Vector Quantization</h3>
                    <p>Vector quantization is a method of clustering that represents a large set of points by a smaller set of points, called codebook vectors. Each data point is assigned to the nearest codebook vector, and the codebook vectors are updated to minimize the total distortion.</p>
                    
                    <div class="accordion">
                        <div class="accordion-item">
                            <div class="accordion-header" onclick="toggleAccordion(this)">
                                <span>K-Means</span>
                                <i class="fas fa-chevron-down accordion-icon"></i>
                            </div>
                            <div class="accordion-body">
                                <p>K-means is one of the most popular clustering algorithms. It aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean, serving as a prototype of the cluster.</p>
                                
                                <p>The algorithm consists of the following steps:</p>
                                <ol>
                                    <li>Initialize k cluster centroids randomly.</li>
                                    <li>Assign each data point to the nearest centroid.</li>
                                    <li>Update the centroids to be the mean of the points assigned to them.</li>
                                    <li>Repeat steps 2-3 until convergence.</li>
                                </ol>
                                
                                <div class="code-block">
                                    <span class="code-language">Python</span>
                                    <button class="copy-btn" onclick="copyCode(this)">Copy</button>
                                    <pre><code>from sklearn.cluster import KMeans
import numpy as np
import matplotlib.pyplot as plt

# Generate data
np.random.seed(42)
X = np.random.randn(300, 2)
X[:100] += 5
X[100:200] += [-5, 5]

# Apply K-means clustering
kmeans = KMeans(n_clusters=3, random_state=42)
kmeans.fit(X)
labels = kmeans.labels_
centroids = kmeans.cluster_centers_

# Plot the results
plt.figure(figsize=(10, 6))
plt.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='viridis')
plt.scatter(centroids[:, 0], centroids[:, 1], c='red', s=200, alpha=0.75, marker='X')
plt.title('K-Means Clustering')
plt.show()</code></pre>
                                </div>
                                
                                <div class="viz-container">
                                    <div class="viz-title">K-Means Clustering</div>
                                    <div class="canvas-container">
                                        <canvas id="kmeans-clustering" width="600" height="400"></canvas>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
                
                <div id="chapter4-7" class="subsection">
                    <h3>4.7 Hierarchical Clustering</h3>
                    <p>Hierarchical clustering is a method of cluster analysis which seeks to build a hierarchy of clusters. There are two main approaches:</p>
                    
                    <ul>
                        <li><strong>Agglomerative (bottom-up):</strong> Each observation starts in its own cluster, and pairs of clusters are merged as one moves up the hierarchy.</li>
                        <li><strong>Divisive (top-down):</strong> All observations start in one cluster, and splits are performed recursively as one moves down the hierarchy.</li>
                    </ul>
                    
                    <p>The results of hierarchical clustering are usually presented in a dendrogram, which shows how the clusters are merged or split at each step.</p>
                    
                    <div class="code-block">
                        <span class="code-language">Python</span>
                        <button class="copy-btn" onclick="copyCode(this)">Copy</button>
                        <pre><code>from sklearn.cluster import AgglomerativeClustering
from scipy.cluster.hierarchy import dendrogram, linkage
import numpy as np
import matplotlib.pyplot as plt

# Generate data
np.random.seed(42)
X = np.random.randn(50, 2)
X[:15] += 5
X[15:30] += [-5, 5]

# Perform hierarchical clustering
Z = linkage(X, method='ward')

# Plot the dendrogram
plt.figure(figsize=(12, 6))
dendrogram(Z)
plt.title('Hierarchical Clustering Dendrogram')
plt.xlabel('Sample Index')
plt.ylabel('Distance')
plt.show()

# Apply agglomerative clustering
agg_clustering = AgglomerativeClustering(n_clusters=3)
labels = agg_clustering.fit_predict(X)

# Plot the clusters
plt.figure(figsize=(10, 6))
plt.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='viridis')
plt.title('Agglomerative Clustering')
plt.show()</code></pre>
                    </div>
                    
                    <div class="viz-container">
                        <div class="viz-title">Hierarchical Clustering</div>
                        <div class="canvas-container">
                            <canvas id="hierarchical-clustering" width="600" height="400"></canvas>
                        </div>
                    </div>
                </div>
                
                <div id="chapter4-8" class="subsection">
                    <h3>4.8 Principal Component Analysis (PCA)</h3>
                    <p>Principal Component Analysis (PCA) is a technique for reducing the dimensionality of a dataset while preserving as much of the variance as possible. It does this by transforming the data to a new coordinate system where the greatest variance by some projection of the data comes to lie on the first coordinate (called the first principal component), the second greatest variance on the second coordinate, and so on.</p>
                    
                    <div class="accordion">
                        <div class="accordion-item">
                            <div class="accordion-header" onclick="toggleAccordion(this)">
                                <span>Motivation: Principal Axes of an Ellipsoid</span>
                                <i class="fas fa-chevron-down accordion-icon"></i>
                            </div>
                            <div class="accordion-body">
                                <p>Imagine a cloud of points in 3D space that forms an ellipsoid. The principal axes of this ellipsoid are the directions in which the data varies the most. PCA finds these principal axes, which are orthogonal to each other, and projects the data onto them.</p>
                                
                                <p>The first principal component is the direction that maximizes the variance of the projected data. The second principal component is the direction that maximizes the variance among all directions orthogonal to the first principal component, and so on.</p>
                            </div>
                        </div>
                        
                        <div class="accordion-item">
                            <div class="accordion-header" onclick="toggleAccordion(this)">
                                <span>PCA and Singular Value Decomposition (SVD)</span>
                                <i class="fas fa-chevron-down accordion-icon"></i>
                            </div>
                            <div class="accordion-body">
                                <p>PCA can be computed using Singular Value Decomposition (SVD). Given a data matrix X, we can decompose it as:</p>
                                
                                <div class="formula">
                                    X = UΣVᵀ
                                </div>
                                
                                <p>where U is an orthogonal matrix whose columns are the left singular vectors, Σ is a diagonal matrix containing the singular values, and V is an orthogonal matrix whose columns are the right singular vectors. The principal components are given by the columns of V, and the explained variance is given by the squared singular values.</p>
                                
                                <div class="code-block">
                                    <span class="code-language">Python</span>
                                    <button class="copy-btn" onclick="copyCode(this)">Copy</button>
                                    <pre><code>from sklearn.decomposition import PCA
import numpy as np
import matplotlib.pyplot as plt

# Generate data
np.random.seed(42)
n_samples = 500
C = np.array([[0.1, 0.6], [0.6, 0.1]])
X = np.dot(np.random.randn(n_samples, 2), C)

# Apply PCA
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

# Plot the results
plt.figure(figsize=(12, 5))

# Original data
plt.subplot(1, 2, 1)
plt.scatter(X[:, 0], X[:, 1], alpha=0.5)
plt.title('Original Data')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.axis('equal')

# Transformed data
plt.subplot(1, 2, 2)
plt.scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.5)
plt.title('PCA Transformed Data')
plt.xlabel('First Principal Component')
plt.ylabel('Second Principal Component')
plt.axis('equal')

plt.tight_layout()
plt.show()

# Explained variance
print(f"Explained variance ratio: {pca.explained_variance_ratio_}")</code></pre>
                                </div>
                                
                                <div class="viz-container">
                                    <div class="viz-title">Principal Component Analysis</div>
                                    <div class="canvas-container">
                                        <canvas id="pca-visualization" width="600" height="400"></canvas>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </section>

            <!-- Additional chapters would continue here -->
            <!-- For brevity, I'm including just a few more chapters with key content -->

            <!-- Chapter 5 Section -->
            <section id="chapter5" class="section">
                <h2>Chapter 5: Regression</h2>
                
                <div id="chapter5-1" class="subsection">
                    <h3>5.1 Introduction</h3>
                    <p>Regression analysis is a set of statistical processes for estimating the relationships between a dependent variable (often called the 'outcome' or 'response' variable) and one or more independent variables (often called 'predictors', 'covariates', or 'features').</p>
                    
                    <p>The most common form of regression analysis is linear regression, in which a line (or plane, or hyperplane) is fitted to the data. However, many other forms of regression exist, including:</p>
                    
                    <ul>
                        <li><strong>Polynomial regression:</strong> Fits a polynomial curve to the data.</li>
                        <li><strong>Logistic regression:</strong> Used for binary classification problems.</li>
                        <li><strong>Ridge regression:</strong> Adds a penalty term to prevent overfitting.</li>
                        <li><strong>Lasso regression:</strong> Adds a penalty term that can shrink some coefficients to zero.</li>
                    </ul>
                </div>
                
                <div id="chapter5-2" class="subsection">
                    <h3>5.2 Linear Regression</h3>
                    <p>Linear regression assumes a linear relationship between the dependent variable and the independent variables. The model can be written as:</p>
                    
                    <div class="formula">
                        y = β₀ + β₁x₁ + β₂x₂ + ... + βₚxₚ + ε
                    </div>
                    
                    <p>where y is the dependent variable, x₁, ..., xₚ are the independent variables, β₀, ..., βₚ are the coefficients, and ε is the error term.</p>
                    
                    <p>The coefficients are typically estimated using the method of least squares, which minimizes the sum of squared residuals:</p>
                    
                    <div class="formula">
                        min_{β₀,...,βₚ} Σᵢ₌₁ⁿ (yᵢ - (β₀ + β₁x₁ᵢ + ... + βₚxₚᵢ))²
                    </div>
                    
                    <div class="code-block">
                        <span class="code-language">Python</span>
                        <button class="copy-btn" onclick="copyCode(this)">Copy</button>
                        <pre><code>import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# Generate synthetic data
np.random.seed(42)
n_samples = 100
X = np.random.randn(n_samples, 1)
y = 2 * X.squeeze() + 1 + 0.5 * np.random.randn(n_samples)

# Fit a linear regression model
model = LinearRegression()
model.fit(X, y)

# Make predictions
y_pred = model.predict(X)

# Evaluate the model
mse = mean_squared_error(y, y_pred)
r2 = r2_score(y, y_pred)

print(f"Coefficient: {model.coef_[0]:.4f}")
print(f"Intercept: {model.intercept_:.4f}")
print(f"Mean Squared Error: {mse:.4f}")
print(f"R-squared: {r2:.4f}")

# Plot the results
plt.figure(figsize=(10, 6))
plt.scatter(X, y, alpha=0.7, label='Data')
plt.plot(X, y_pred, 'r-', label='Linear Regression')
plt.xlabel('X')
plt.ylabel('y')
plt.title('Linear Regression')
plt.legend()
plt.show()</code></pre>
                    </div>
                    
                    <div class="viz-container">
                        <div class="viz-title">Linear Regression Example</div>
                        <div class="canvas-container">
                            <canvas id="linear-regression-plot" width="600" height="400"></canvas>
                        </div>
                    </div>
                </div>
                
                <div id="chapter5-3" class="subsection">
                    <h3>5.3 Analysis via Linear Models</h3>
                    <p>Linear models provide a framework for analyzing the relationship between variables. They can be used for:</p>
                    
                    <ul>
                        <li><strong>Prediction:</strong> Predicting the value of the dependent variable for new observations.</li>
                        <li><strong>Inference:</strong> Understanding the relationship between the dependent and independent variables.</li>
                        <li><strong>Variable selection:</strong> Identifying which independent variables are important.</li>
                    </ul>
                    
                    <div class="accordion">
                        <div class="accordion-item">
                            <div class="accordion-header" onclick="toggleAccordion(this)">
                                <span>Parameter Estimation</span>
                                <i class="fas fa-chevron-down accordion-icon"></i>
                            </div>
                            <div class="accordion-body">
                                <p>The parameters of a linear model are typically estimated using the method of least squares. The least squares estimator has several desirable properties:</p>
                                
                                <ul>
                                    <li>It is unbiased: E[β̂] = β.</li>
                                    <li>It has minimum variance among all linear unbiased estimators (Gauss-Markov theorem).</li>
                                    <li>It is the maximum likelihood estimator if the errors are normally distributed.</li>
                                </ul>
                            </div>
                        </div>
                        
                        <div class="accordion-item">
                            <div class="accordion-header" onclick="toggleAccordion(this)">
                                <span>Model Selection and Prediction</span>
                                <i class="fas fa-chevron-down accordion-icon"></i>
                            </div>
                            <div class="accordion-body">
                                <p>Model selection involves choosing the best set of predictors for the model. Common approaches include:</p>
                                
                                <ul>
                                    <li><strong>Forward selection:</strong> Start with no predictors and add them one by one.</li>
                                    <li><strong>Backward elimination:</strong> Start with all predictors and remove them one by one.</li>
                                    <li><strong>Stepwise regression:</strong> A combination of forward selection and backward elimination.</li>
                                    <li><strong>Regularization methods:</strong> Ridge regression, Lasso, and Elastic Net.</li>
                                </ul>
                            </div>
                        </div>
                        
                        <div class="accordion-item">
                            <div class="accordion-header" onclick="toggleAccordion(this)">
                                <span>Cross-Validation and Predictive Residual Sum of Squares</span>
                                <i class="fas fa-chevron-down accordion-icon"></i>
                            </div>
                            <div class="accordion-body">
                                <p>Cross-validation is a technique for assessing how the results of a statistical analysis will generalize to an independent data set. It is mainly used in settings where the goal is prediction, and one wants to estimate how accurately a predictive model will perform in practice.</p>
                                
                                <p>The Predictive Residual Sum of Squares (PRESS) is a form of cross-validation where each observation is left out one at a time, and the model is fitted to the remaining data. The squared difference between the left-out observation and its predicted value is then computed, and these values are summed over all observations.</p>
                            </div>
                        </div>
                    </div>
                </div>
                
                <div id="chapter5-4" class="subsection">
                    <h3>5.4 Inference for Normal Linear Models</h3>
                    <p>When the errors in a linear model are assumed to be normally distributed, we can perform statistical inference on the model parameters. This includes testing hypotheses about the parameters and constructing confidence intervals.</p>
                    
                    <div class="accordion">
                        <div class="accordion-item">
                            <div class="accordion-header" onclick="toggleAccordion(this)">
                                <span>Comparing Two Normal Linear Models</span>
                                <i class="fas fa-chevron-down accordion-icon"></i>
                            </div>
                            <div class="accordion-body">
                                <p>We can compare two nested linear models using an F-test. The null hypothesis is that the simpler model is adequate, and the alternative hypothesis is that the more complex model provides a significantly better fit.</p>
                                
                                <p>The F-statistic is computed as:</p>
                                
                                <div class="formula">
                                    F = ((RSS₁ - RSS₂)/(df₁ - df₂)) / (RSS₂/df₂)
                                </div>
                                
                                <p>where RSS₁ and RSS₂ are the residual sum of squares for the simpler and more complex models, respectively, and df₁ and df₂ are their degrees of freedom.</p>
                            </div>
                        </div>
                        
                        <div class="accordion-item">
                            <div class="accordion-header" onclick="toggleAccordion(this)">
                                <span>Confidence and Prediction Intervals</span>
                                <i class="fas fa-chevron-down accordion-icon"></i>
                            </div>
                            <div class="accordion-body">
                                <p>For a linear model, we can construct confidence intervals for the mean response and prediction intervals for individual responses.</p>
                                
                                <p>A <strong>confidence interval</strong> for the mean response at a given point x₀ is:</p>
                                
                                <div class="formula">
                                    ŷ₀ ± t_{α/2, n-p-1} × √(MSE × x₀ᵀ(XᵀX)⁻¹x₀)
                                </div>
                                
                                <p>A <strong>prediction interval</strong> for an individual response at a given point x₀ is:</p>
                                
                                <div class="formula">
                                    ŷ₀ ± t_{α/2, n-p-1} × √(MSE × (1 + x₀ᵀ(XᵀX)⁻¹x₀))
                                </div>
                                
                                <p>where ŷ₀ is the predicted value at x₀, MSE is the mean squared error, and t_{α/2, n-p-1} is the critical value from the t-distribution.</p>
                            </div>
                        </div>
                    </div>
                </div>
                
                <div id="chapter5-5" class="subsection">
                    <h3>5.5 Nonlinear Regression Models</h3>
                    <p>Sometimes the relationship between the dependent and independent variables is not linear. In such cases, we can use nonlinear regression models, which can capture more complex relationships.</p>
                    
                    <p>A nonlinear regression model can be written as:</p>
                    
                    <div class="formula">
                        y = f(x, β) + ε
                    </div>
                    
                    <p>where f is a nonlinear function of x and the parameters β.</p>
                    
                    <div class="code-block">
                        <span class="code-language">Python</span>
                        <button class="copy-btn" onclick="copyCode(this)">Copy</button>
                        <pre><code>import numpy as np
import matplotlib.pyplot as plt
from scipy.optimize import curve_fit

# Define the nonlinear function
def nonlinear_func(x, a, b, c):
    return a * np.exp(-b * x) + c

# Generate synthetic data
np.random.seed(42)
x_data = np.linspace(0, 4, 50)
y_data = nonlinear_func(x_data, 2.5, 1.3, 0.5) + 0.2 * np.random.randn(len(x_data))

# Fit the nonlinear model
popt, pcov = curve_fit(nonlinear_func, x_data, y_data, p0=[1, 1, 0])

# Make predictions
y_pred = nonlinear_func(x_data, *popt)

# Plot the results
plt.figure(figsize=(10, 6))
plt.scatter(x_data, y_data, label='Data')
plt.plot(x_data, y_pred, 'r-', label='Nonlinear Fit')
plt.xlabel('x')
plt.ylabel('y')
plt.title('Nonlinear Regression')
plt.legend()
plt.show()

print(f"Estimated parameters: a={popt[0]:.4f}, b={popt[1]:.4f}, c={popt[2]:.4f}")</code></pre>
                    </div>
                    
                    <div class="viz-container">
                        <div class="viz-title">Nonlinear Regression Example</div>
                        <div class="canvas-container">
                            <canvas id="nonlinear-regression" width="600" height="400"></canvas>
                        </div>
                    </div>
                </div>
                
                <div id="chapter5-6" class="subsection">
                    <h3>5.6 Linear Models in Python</h3>
                    <p>Python provides several libraries for fitting and analyzing linear models. The most commonly used are statsmodels and scikit-learn.</p>
                    
                    <div class="tabs">
                        <div class="tab-buttons">
                            <button class="tab-button active" onclick="openTab(event, 'modeling')">Modeling</button>
                            <button class="tab-button" onclick="openTab(event, 'analysis')">Analysis</button>
                            <button class="tab-button" onclick="openTab(event, 'anova')">ANOVA</button>
                            <button class="tab-button" onclick="openTab(event, 'intervals')">Confidence and Prediction Intervals</button>
                            <button class="tab-button" onclick="openTab(event, 'validation')">Model Validation</button>
                            <button class="tab-button" onclick="openTab(event, 'selection')">Variable Selection</button>
                        </div>
                        
                        <div id="modeling" class="tab-content">
                            <div class="tab-pane active">
                                <div class="code-block">
                                    <span class="code-language">Python</span>
                                    <button class="copy-btn" onclick="copyCode(this)">Copy</button>
                                    <pre><code>import statsmodels.api as sm
import pandas as pd
import numpy as np

# Load data
# For this example, we'll use the Boston housing dataset
from sklearn.datasets import load_boston
boston = load_boston()
X = pd.DataFrame(boston.data, columns=boston.feature_names)
y = boston.target

# Add a constant to the design matrix
X = sm.add_constant(X)

# Fit a linear model
model = sm.OLS(y, X).fit()

# Print the model summary
print(model.summary())</code></pre>
                                </div>
                            </div>
                        </div>
                        
                        <div id="analysis" class="tab-content">
                            <div class="tab-pane">
                                <div class="code-block">
                                    <span class="code-language">Python</span>
                                    <button class="copy-btn" onclick="copyCode(this)">Copy</button>
                                    <pre><code># Analyze the model
print(f"R-squared: {model.rsquared:.4f}")
print(f"Adjusted R-squared: {model.rsquared_adj:.4f}")
print(f"F-statistic: {model.fvalue:.4f}")
print(f"Prob (F-statistic): {model.f_pvalue:.4f}")

# Coefficients
print("\nCoefficients:")
print(model.params)

# Standard errors
print("\nStandard Errors:")
print(model.bse)

# t-values and p-values
print("\nt-values:")
print(model.tvalues)
print("\np-values:")
print(model.pvalues)</code></pre>
                                </div>
                            </div>
                        </div>
                        
                        <div id="anova" class="tab-content">
                            <div class="tab-pane">
                                <div class="code-block">
                                    <span class="code-language">Python</span>
                                    <button class="copy-btn" onclick="copyCode(this)">Copy</button>
                                    <pre><code># Perform ANOVA
from statsmodels.stats.anova import anova_lm

# Create a reduced model (e.g., without some predictors)
X_reduced = X[['const', 'RM', 'LSTAT']]
model_reduced = sm.OLS(y, X_reduced).fit()

# Compare the full and reduced models
anova_results = anova_lm(model_reduced, model)
print(anova_results)</code></pre>
                                </div>
                            </div>
                        </div>
                        
                        <div id="intervals" class="tab-content">
                            <div class="tab-pane">
                                <div class="code-block">
                                    <span class="code-language">Python</span>
                                    <button class="copy-btn" onclick="copyCode(this)">Copy</button>
                                    <pre><code># Confidence intervals for the coefficients
print("Confidence Intervals for Coefficients:")
print(model.conf_int())

# Prediction and confidence intervals for new data
new_data = pd.DataFrame({'const': [1], 'RM': [6], 'LSTAT': [10]})
new_data = new_data[['const', 'RM', 'LSTAT']]  # Ensure correct order

# Get predictions
predictions = model.get_prediction(new_data)

# Summary of predictions
pred_summary = predictions.summary_frame(alpha=0.05)
print("\nPrediction Summary:")
print(pred_summary)</code></pre>
                                </div>
                            </div>
                        </div>
                        
                        <div id="validation" class="tab-content">
                            <div class="tab-pane">
                                <div class="code-block">
                                    <span class="code-language">Python</span>
                                    <button class="copy-btn" onclick="copyCode(this)">Copy</button>
                                    <pre><code>from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LinearRegression
from sklearn.metrics import make_scorer, mean_squared_error

# Convert to scikit-learn format
X_sklearn = X.drop('const', axis=1)
model_sklearn = LinearRegression()

# Perform cross-validation
mse_scorer = make_scorer(mean_squared_error, greater_is_better=False)
cv_scores = cross_val_score(model_sklearn, X_sklearn, y, cv=5, scoring=mse_scorer)
rmse_scores = np.sqrt(-cv_scores)

print(f"Cross-validated RMSE: {rmse_scores.mean():.4f} (+/- {rmse_scores.std() * 2:.4f})")

# Residual analysis
residuals = model.resid
fitted = model.fittedvalues

plt.figure(figsize=(12, 5))

# Residuals vs. Fitted
plt.subplot(1, 2, 1)
plt.scatter(fitted, residuals)
plt.axhline(y=0, color='r', linestyle='--')
plt.xlabel('Fitted values')
plt.ylabel('Residuals')
plt.title('Residuals vs. Fitted')

# Q-Q plot
plt.subplot(1, 2, 2)
sm.qqplot(residuals, line='s', ax=plt.gca())
plt.title('Q-Q Plot')

plt.tight_layout()
plt.show()</code></pre>
                                </div>
                            </div>
                        </div>
                        
                        <div id="selection" class="tab-content">
                            <div class="tab-pane">
                                <div class="code-block">
                                    <span class="code-language">Python</span>
                                    <button class="copy-btn" onclick="copyCode(this)">Copy</button>
                                    <pre><code>from sklearn.feature_selection import RFE
from sklearn.linear_model import LinearRegression

# Recursive Feature Elimination
estimator = LinearRegression()
selector = RFE(estimator, n_features_to_select=5, step=1)
selector = selector.fit(X_sklearn, y)

# Selected features
selected_features = X_sklearn.columns[selector.support_]
print("Selected Features:", selected_features.tolist())

# Fit model with selected features
X_selected = X_sklearn[selected_features]
model_selected = sm.OLS(y, sm.add_constant(X_selected)).fit()
print(model_selected.summary())</code></pre>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
                
                <div id="chapter5-7" class="subsection">
                    <h3>5.7 Generalized Linear Models</h3>
                    <p>Generalized Linear Models (GLMs) extend linear regression to allow for response variables that have error distribution models other than a normal distribution. They generalize linear regression by allowing the linear model to be related to the response variable via a link function and by allowing the variance of each measurement to be a function of its predicted value.</p>
                    
                    <p>A GLM consists of three components:</p>
                    <ol>
                        <li><strong>Random component:</strong> The probability distribution of the response variable.</li>
                        <li><strong>Systematic component:</strong> A linear predictor η = Xβ.</li>
                        <li><strong>Link function:</strong> A function g that relates the expected value of the response to the linear predictor: g(μ) = η.</li>
                    </ol>
                    
                    <p>Common types of GLMs include:</p>
                    <ul>
                        <li><strong>Logistic regression:</strong> Used for binary outcomes with a logit link function.</li>
                        <li><strong>Poisson regression:</strong> Used for count data with a log link function.</li>
                        <li><strong>Gamma regression:</strong> Used for positive continuous outcomes with an inverse link function.</li>
                    </ul>
                    
                    <div class="code-block">
                        <span class="code-language">Python</span>
                        <button class="copy-btn" onclick="copyCode(this)">Copy</button>
                        <pre><code>import statsmodels.api as sm
import numpy as np
import pandas as pd

# Example: Logistic Regression
# Generate synthetic data
np.random.seed(42)
n_samples = 200
X = np.random.randn(n_samples, 2)
X = sm.add_constant(X)
beta = np.array([0.5, 1.5, -2.0])
logit_p = X @ beta
p = 1 / (1 + np.exp(-logit_p))
y = np.random.binomial(1, p, size=n_samples)

# Fit a logistic regression model
logit_model = sm.Logit(y, X).fit()
print(logit_model.summary())

# Example: Poisson Regression
# Generate synthetic count data
np.random.seed(42)
n_samples = 200
X = np.random.randn(n_samples, 2)
X = sm.add_constant(X)
beta = np.array([0.5, 1.0, -0.5])
log_lambda = X @ beta
lambda_ = np.exp(log_lambda)
y = np.random.poisson(lambda_, size=n_samples)

# Fit a Poisson regression model
poisson_model = sm.GLM(y, X, family=sm.families.Poisson()).fit()
print(poisson_model.summary())</code></pre>
                    </div>
                </div>
            </section>

            <!-- Additional chapters would continue here -->
            <!-- For brevity, I'm including just a few more chapters with key content -->

            <!-- Chapter 6 Section -->
            <section id="chapter6" class="section">
                <h2>Chapter 6: Regularization and Kernel Methods</h2>
                
                <div id="chapter6-1" class="subsection">
                    <h3>6.1 Introduction</h3>
                    <p>Regularization and kernel methods are powerful techniques for extending linear models to handle non-linear relationships and prevent overfitting. Regularization adds a penalty term to the loss function to control the complexity of the model, while kernel methods implicitly map the data to a higher-dimensional space where linear methods can be applied.</p>
                </div>
                
                <div id="chapter6-2" class="subsection">
                    <h3>6.2 Regularization</h3>
                    <p>Regularization is a technique used to prevent overfitting by adding a penalty term to the loss function. The penalty term discourages overly complex models by penalizing large coefficients.</p>
                    
                    <div class="tabs">
                        <div class="tab-buttons">
                            <button class="tab-button active" onclick="openTab(event, 'ridge')">Ridge Regression</button>
                            <button class="tab-button" onclick="openTab(event, 'lasso')">Lasso Regression</button>
                            <button class="tab-button" onclick="openTab(event, 'elastic')">Elastic Net</button>
                        </div>
                        
                        <div id="ridge" class="tab-content">
                            <div class="tab-pane active">
                                <p><strong>Ridge regression</strong> adds an L2 penalty term to the loss function:</p>
                                
                                <div class="formula">
                                    min_{β} Σᵢ₌₁ⁿ (yᵢ - xᵢᵀβ)² + λ Σⱼ₌₁ᵖ βⱼ²
                                </div>
                                
                                <p>where λ is the regularization parameter that controls the strength of the penalty.</p>
                                
                                <div class="code-block">
                                    <span class="code-language">Python</span>
                                    <button class="copy-btn" onclick="copyCode(this)">Copy</button>
                                    <pre><code>from sklearn.linear_model import Ridge
from sklearn.model_selection import cross_val_score
import numpy as np

# Generate synthetic data
np.random.seed(42)
n_samples, n_features = 100, 10
X = np.random.randn(n_samples, n_features)
true_coef = np.zeros(n_features)
true_coef[:5] = 1.0  # Only first 5 features are relevant
y = X @ true_coef + 0.1 * np.random.randn(n_samples)

# Fit ridge regression with different alpha values
alphas = [0.01, 0.1, 1.0, 10.0, 100.0]
for alpha in alphas:
    ridge = Ridge(alpha=alpha)
    scores = cross_val_score(ridge, X, y, cv=5, scoring='neg_mean_squared_error')
    print(f"Alpha: {alpha}, CV MSE: {-scores.mean():.4f}")

# Fit the best model
best_alpha = alphas[np.argmin([-cross_val_score(Ridge(alpha=a), X, y, cv=5, 
                                             scoring='neg_mean_squared_error').mean() 
                              for a in alphas])]
ridge_best = Ridge(alpha=best_alpha)
ridge_best.fit(X, y)

print(f"\nBest alpha: {best_alpha}")
print("Coefficients:", ridge_best.coef_)</code></pre>
                                </div>
                            </div>
                        </div>
                        
                        <div id="lasso" class="tab-content">
                            <div class="tab-pane">
                                <p><strong>Lasso regression</strong> adds an L1 penalty term to the loss function:</p>
                                
                                <div class="formula">
                                    min_{β} Σᵢ₌₁ⁿ (yᵢ - xᵢᵀβ)² + λ Σⱼ₌₁ᵖ |βⱼ|
                                </div>
                                
                                <p>The L1 penalty has the effect of shrinking some coefficients to exactly zero, which makes lasso useful for variable selection.</p>
                                
                                <div class="code-block">
                                    <span class="code-language">Python</span>
                                    <button class="copy-btn" onclick="copyCode(this)">Copy</button>
                                    <pre><code>from sklearn.linear_model import Lasso
from sklearn.model_selection import cross_val_score
import numpy as np

# Generate synthetic data
np.random.seed(42)
n_samples, n_features = 100, 10
X = np.random.randn(n_samples, n_features)
true_coef = np.zeros(n_features)
true_coef[:5] = 1.0  # Only first 5 features are relevant
y = X @ true_coef + 0.1 * np.random.randn(n_samples)

# Fit lasso regression with different alpha values
alphas = [0.001, 0.01, 0.1, 1.0, 10.0]
for alpha in alphas:
    lasso = Lasso(alpha=alpha)
    scores = cross_val_score(lasso, X, y, cv=5, scoring='neg_mean_squared_error')
    print(f"Alpha: {alpha}, CV MSE: {-scores.mean():.4f}")

# Fit the best model
best_alpha = alphas[np.argmin([-cross_val_score(Lasso(alpha=a), X, y, cv=5, 
                                             scoring='neg_mean_squared_error').mean() 
                              for a in alphas])]
lasso_best = Lasso(alpha=best_alpha)
lasso_best.fit(X, y)

print(f"\nBest alpha: {best_alpha}")
print("Coefficients:", lasso_best.coef_)
print("Number of non-zero coefficients:", np.sum(lasso_best.coef_ != 0))</code></pre>
                                </div>
                            </div>
                        </div>
                        
                        <div id="elastic" class="tab-content">
                            <div class="tab-pane">
                                <p><strong>Elastic Net</strong> combines both L1 and L2 penalties:</p>
                                
                                <div class="formula">
                                    min_{β} Σᵢ₌₁ⁿ (yᵢ - xᵢᵀβ)² + λ₁ Σⱼ₌₁ᵖ |βⱼ| + λ₂ Σⱼ₌₁ᵖ βⱼ²
                                </div>
                                
                                <p>Elastic Net is particularly useful when there are multiple correlated features.</p>
                                
                                <div class="code-block">
                                    <span class="code-language">Python</span>
                                    <button class="copy-btn" onclick="copyCode(this)">Copy</button>
                                    <pre><code>from sklearn.linear_model import ElasticNet
from sklearn.model_selection import cross_val_score
import numpy as np

# Generate synthetic data with correlated features
np.random.seed(42)
n_samples, n_features = 100, 10
X = np.random.randn(n_samples, n_features)
# Make some features correlated
X[:, 1] = X[:, 0] + 0.1 * np.random.randn(n_samples)
X[:, 3] = X[:, 2] + 0.1 * np.random.randn(n_samples)

true_coef = np.zeros(n_features)
true_coef[:5] = 1.0  # Only first 5 features are relevant
y = X @ true_coef + 0.1 * np.random.randn(n_samples)

# Fit elastic net with different alpha and l1_ratio values
alphas = [0.01, 0.1, 1.0, 10.0]
l1_ratios = [0.1, 0.5, 0.9]

best_score = float('inf')
best_params = {}

for alpha in alphas:
    for l1_ratio in l1_ratios:
        enet = ElasticNet(alpha=alpha, l1_ratio=l1_ratio)
        scores = cross_val_score(enet, X, y, cv=5, scoring='neg_mean_squared_error')
        mse = -scores.mean()
        
        if mse < best_score:
            best_score = mse
            best_params = {'alpha': alpha, 'l1_ratio': l1_ratio}

print(f"Best parameters: {best_params}")
print(f"Best CV MSE: {best_score:.4f}")

# Fit the best model
enet_best = ElasticNet(**best_params)
enet_best.fit(X, y)
print("Coefficients:", enet_best.coef_)</code></pre>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
                
                <div id="chapter6-3" class="subsection">
                    <h3>6.3 Reproducing Kernel Hilbert Spaces</h3>
                    <p>A Reproducing Kernel Hilbert Space (RKHS) is a Hilbert space of functions in which point evaluation is a continuous linear functional. This means that for every function f in the space and every point x, the value f(x) can be represented as an inner product with a function K_x in the space:</p>
                    
                    <div class="formula">
                        f(x) = ⟨f, K_x⟩
                    </div>
                    
                    <p>The function K(x, y) = K_x(y) is called the reproducing kernel of the space. The reproducing property allows us to work with functions in the RKHS using only the kernel function, without explicitly representing the functions themselves.</p>
                </div>
                
                <div id="chapter6-4" class="subsection">
                    <h3>6.4 Construction of Reproducing Kernels</h3>
                    <p>There are several ways to construct reproducing kernels:</p>
                    
                    <div class="accordion">
                        <div class="accordion-item">
                            <div class="accordion-header" onclick="toggleAccordion(this)">
                                <span>Reproducing Kernels via Feature Mapping</span>
                                <i class="fas fa-chevron-down accordion-icon"></i>
                            </div>
                            <div class="accordion-body">
                                <p>Given a feature mapping φ: X → H, where H is a Hilbert space, we can define a kernel as:</p>
                                
                                <div class="formula">
                                    K(x, y) = ⟨φ(x), φ(y)⟩
                                </div>
                                
                                <p>This is called the kernel trick: we can compute the inner product in the high-dimensional feature space without explicitly computing the feature mapping.</p>
                            </div>
                        </div>
                        
                        <div class="accordion-item">
                            <div class="accordion-header" onclick="toggleAccordion(this)">
                                <span>Kernels from Characteristic Functions</span>
                                <i class="fas fa-chevron-down accordion-icon"></i>
                            </div>
                            <div class="accordion-body">
                                <p>The characteristic function of a probability distribution can be used to construct a kernel. For a distribution with characteristic function φ(t), the kernel can be defined as:</p>
                                
                                <div class="formula">
                                    K(x, y) = φ(x - y)
                                </div>
                            </div>
                        </div>
                        
                        <div class="accordion-item">
                            <div class="accordion-header" onclick="toggleAccordion(this)">
                                <span>Reproducing Kernels Using Orthonormal Features</span>
                                <i class="fas fa-chevron-down accordion-icon"></i>
                            </div>
                            <div class="accordion-body">
                                <p>Given an orthonormal basis {φ_i} for a feature space, we can define a kernel as:</p>
                                
                                <div class="formula">
                                    K(x, y) = Σᵢ λᵢ φᵢ(x)φᵢ(y)
                                </div>
                                
                                <p>where λ_i are non-negative weights.</p>
                            </div>
                        </div>
                        
                        <div class="accordion-item">
                            <div class="accordion-header" onclick="toggleAccordion(this)">
                                <span>Kernels from Kernels</span>
                                <i class="fas fa-chevron-down accordion-icon"></i>
                            </div>
                            <div class="accordion-body">
                                <p>New kernels can be constructed from existing kernels using various operations:</p>
                                
                                <ul>
                                    <li><strong>Sum:</strong> K(x, y) = K₁(x, y) + K₂(x, y)</li>
                                    <li><strong>Product:</strong> K(x, y) = K₁(x, y) × K₂(x, y)</li>
                                    <li><strong>Scalar multiple:</strong> K(x, y) = c × K₁(x, y), where c > 0</li>
                                    <li><strong>Exponentiation:</strong> K(x, y) = exp(K₁(x, y))</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                </div>
                
                <div id="chapter6-5" class="subsection">
                    <h3>6.5 Representer Theorem</h3>
                    <p>The Representer Theorem is a fundamental result in kernel methods. It states that the solution to certain optimization problems in an RKHS can be expressed as a linear combination of kernel functions evaluated at the training points.</p>
                    
                    <p>Formally, given a loss function L and a regularization term R, the solution to:</p>
                    
                    <div class="formula">
                                    min_{f ∈ H} Σᵢ₌₁ⁿ L(yᵢ, f(xᵢ)) + λR(f)
                    </div>
                    
                    <p>can be written as:</p>
                    
                    <div class="formula">
                        f*(x) = Σᵢ₌₁ⁿ αᵢ K(xᵢ, x)
                    </div>
                    
                    <p>where α_i are coefficients to be determined. This theorem is the foundation of many kernel methods, including support vector machines and kernel ridge regression.</p>
                </div>
                
                <div id="chapter6-6" class="subsection">
                    <h3>6.6 Smoothing Cubic Splines</h3>
                    <p>Cubic splines are piecewise cubic polynomials that are joined smoothly at certain points called knots. Smoothing cubic splines are used to fit a smooth curve to data while balancing goodness of fit and smoothness.</p>
                    
                    <p>The smoothing spline is the solution to the following optimization problem:</p>
                    
                    <div class="formula">
                        min_{f} Σᵢ₌₁ⁿ (yᵢ - f(xᵢ))² + λ ∫ (f''(x))² dx
                    </div>
                    
                    <p>where λ is a smoothing parameter that controls the trade-off between fitting the data and smoothness.</p>
                    
                    <div class="code-block">
                        <span class="code-language">Python</span>
                        <button class="copy-btn" onclick="copyCode(this)">Copy</button>
                        <pre><code>import numpy as np
import matplotlib.pyplot as plt
from scipy.interpolate import UnivariateSpline

# Generate synthetic data
np.random.seed(42)
x = np.linspace(0, 10, 50)
y = np.sin(x) + 0.1 * np.random.randn(len(x))

# Fit smoothing splines with different smoothing parameters
splines = []
lambdas = [0.1, 1.0, 10.0]
for lam in lambdas:
    # In scipy, the smoothing parameter s is approximately lambda * n
    s = lam * len(x)
    spline = UnivariateSpline(x, y, s=s)
    splines.append(spline)

# Plot the results
plt.figure(figsize=(10, 6))
plt.scatter(x, y, label='Data')
x_grid = np.linspace(0, 10, 1000)
for i, (spline, lam) in enumerate(zip(splines, lambdas)):
    plt.plot(x_grid, spline(x_grid), label=f'λ = {lam}')
plt.xlabel('x')
plt.ylabel('y')
plt.title('Smoothing Cubic Splines')
plt.legend()
plt.show()</code></pre>
                    </div>
                    
                    <div class="viz-container">
                        <div class="viz-title">Smoothing Cubic Splines</div>
                        <div class="canvas-container">
                            <canvas id="smoothing-splines" width="600" height="400"></canvas>
                        </div>
                    </div>
                </div>
                
                <div id="chapter6-7" class="subsection">
                    <h3>6.7 Gaussian Process Regression</h3>
                    <p>Gaussian Process (GP) regression is a non-parametric Bayesian approach to regression. A Gaussian Process is a collection of random variables, any finite number of which have a joint Gaussian distribution.</p>
                    
                    <p>A GP is defined by a mean function m(x) and a covariance function (kernel) k(x, x'). The GP prior is:</p>
                    
                    <div class="formula">
                        f ~ GP(m, k)
                    </div>
                    
                    <p>Given training data (X, y), the posterior distribution of the function values at test points X* is also Gaussian:</p>
                    
                    <div class="formula">
                        f* | X, y, X* ~ N(μ*, Σ*)
                    </div>
                    
                    <p>where:</p>
                    
                    <div class="formula">
                        μ* = k(X*, X) [k(X, X) + σ²I]⁻¹ y
                    </div>
                    
                    <div class="formula">
                        Σ* = k(X*, X*) - k(X*, X) [k(X, X) + σ²I]⁻¹ k(X, X*)
                    </div>
                    
                    <div class="code-block">
                        <span class="code-language">Python</span>
                        <button class="copy-btn" onclick="copyCode(this)">Copy</button>
                        <pre><code>import numpy as np
import matplotlib.pyplot as plt
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel

# Generate synthetic data
np.random.seed(42)
X = np.random.uniform(0, 10, 20).reshape(-1, 1)
y = np.sin(X).ravel() + 0.1 * np.random.randn(len(X))

# Define the kernel
kernel = ConstantKernel(1.0) * RBF(length_scale=1.0)

# Fit the Gaussian Process
gp = GaussianProcessRegressor(kernel=kernel, alpha=0.1**2, normalize_y=True)
gp.fit(X, y)

# Make predictions
X_test = np.linspace(0, 10, 100).reshape(-1, 1)
y_pred, sigma = gp.predict(X_test, return_std=True)

# Plot the results
plt.figure(figsize=(10, 6))
plt.scatter(X, y, c='r', s=50, zorder=10, edgecolors=(0, 0, 0), label='Data')
plt.plot(X_test, y_pred, 'k-', lw=2, label='Prediction')
plt.fill_between(X_test.ravel(), y_pred - 1.96 * sigma, y_pred + 1.96 * sigma,
                 alpha=0.2, color='k', label='95% confidence interval')
plt.xlabel('x')
plt.ylabel('y')
plt.title('Gaussian Process Regression')
plt.legend()
plt.show()</code></pre>
                    </div>
                    
                    <div class="viz-container">
                        <div class="viz-title">Gaussian Process Regression</div>
                        <div class="canvas-container">
                            <canvas id="gp-regression" width="600" height="400"></canvas>
                        </div>
                    </div>
                </div>
                
                <div id="chapter6-8" class="subsection">
                    <h3>6.8 Kernel PCA</h3>
                    <p>Kernel Principal Component Analysis (Kernel PCA) is an extension of PCA that allows for non-linear dimensionality reduction. It works by first mapping the data to a higher-dimensional feature space using a kernel function, and then performing PCA in that space.</p>
                    
                    <p>The key idea is that we don't need to explicitly compute the mapping to the high-dimensional space. Instead, we can work directly with the kernel matrix, which contains the inner products between all pairs of data points in the high-dimensional space.</p>
                    
                    <div class="code-block">
                        <span class="code-language">Python</span>
                        <button class="copy-btn" onclick="copyCode(this)">Copy</button>
                        <pre><code>import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA, KernelPCA
from sklearn.datasets import make_circles

# Generate non-linear data
np.random.seed(42)
X, y = make_circles(n_samples=400, factor=0.3, noise=0.05)

# Apply standard PCA
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

# Apply Kernel PCA with RBF kernel
kpca = KernelPCA(n_components=2, kernel='rbf', gamma=10)
X_kpca = kpca.fit_transform(X)

# Plot the results
plt.figure(figsize=(12, 6))

# Original data
plt.subplot(1, 3, 1)
plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis')
plt.title('Original Data')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')

# Standard PCA
plt.subplot(1, 3, 2)
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis')
plt.title('Standard PCA')
plt.xlabel('First Principal Component')
plt.ylabel('Second Principal Component')

# Kernel PCA
plt.subplot(1, 3, 3)
plt.scatter(X_kpca[:, 0], X_kpca[:, 1], c=y, cmap='viridis')
plt.title('Kernel PCA (RBF)')
plt.xlabel('First Kernel Component')
plt.ylabel('Second Kernel Component')

plt.tight_layout()
plt.show()</code></pre>
                    </div>
                    
                    <div class="viz-container">
                        <div class="viz-title">Kernel PCA vs. Standard PCA</div>
                        <div class="canvas-container">
                            <canvas id="kernel-pca" width="600" height="400"></canvas>
                        </div>
                    </div>
                </div>
            </section>

            <!-- Additional chapters would continue here -->
            <!-- For brevity, I'm including just a few more chapters with key content -->

            <!-- Chapter 7 Section -->
            <section id="chapter7" class="section">
                <h2>Chapter 7: Classification</h2>
                
                <div id="chapter7-1" class="subsection">
                    <h3>7.1 Introduction</h3>
                    <p>Classification is a supervised learning task where the goal is to predict a categorical label or class. Unlike regression, where the output is a continuous value, in classification the output is a discrete category.</p>
                    
                    <p>Classification problems can be divided into two types:</p>
                    <ul>
                        <li><strong>Binary classification:</strong> The output has two possible classes (e.g., spam or not spam).</li>
                        <li><strong>Multi-class classification:</strong> The output has more than two possible classes (e.g., classifying an email into multiple categories).</li>
                    </ul>
                </div>
                
                <div id="chapter7-2" class="subsection">
                    <h3>7.2 Classification Metrics</h3>
                    <p>To evaluate the performance of a classification model, we use various metrics:</p>
                    
                    <div class="accordion">
                        <div class="accordion-item">
                            <div class="accordion-header" onclick="toggleAccordion(this)">
                                <span>Confusion Matrix</span>
                                <i class="fas fa-chevron-down accordion-icon"></i>
                            </div>
                            <div class="accordion-body">
                                <p>A confusion matrix is a table that describes the performance of a classification model. For binary classification, it has four entries:</p>
                                
                                <ul>
                                    <li><strong>True Positives (TP):</strong> Correctly predicted positive cases.</li>
                                    <li><strong>True Negatives (TN):</strong> Correctly predicted negative cases.</li>
                                    <li><strong>False Positives (FP):</strong> Incorrectly predicted positive cases.</li>
                                    <li><strong>False Negatives (FN):</strong> Incorrectly predicted negative cases.</li>
                                </ul>
                                
                                <div class="viz-container">
                                    <div class="viz-title">Confusion Matrix</div>
                                    <div class="canvas-container">
                                        <canvas id="confusion-matrix" width="400" height="300"></canvas>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="accordion-item">
                            <div class="accordion-header" onclick="toggleAccordion(this)">
                                <span>Accuracy, Precision, Recall, and F1 Score</span>
                                <i class="fas fa-chevron-down accordion-icon"></i>
                            </div>
                            <div class="accordion-body">
                                <p>Common classification metrics derived from the confusion matrix include:</p>
                                
                                <ul>
                                    <li><strong>Accuracy:</strong> (TP + TN) / (TP + TN + FP + FN) - The proportion of correct predictions.</li>
                                    <li><strong>Precision:</strong> TP / (TP + FP) - The proportion of positive predictions that are actually positive.</li>
                                    <li><strong>Recall (Sensitivity):</strong> TP / (TP + FN) - The proportion of actual positives that are correctly identified.</li>
                                    <li><strong>F1 Score:</strong> 2 × (Precision × Recall) / (Precision + Recall) - The harmonic mean of precision and recall.</li>
                                </ul>
                                
                                <div class="code-block">
                                    <span class="code-language">Python</span>
                                    <button class="copy-btn" onclick="copyCode(this)">Copy</button>
                                    <pre><code>from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score
import numpy as np

# Example: True labels and predicted labels
y_true = np.array([0, 1, 0, 1, 0, 1, 0, 1, 0, 1])
y_pred = np.array([0, 1, 0, 0, 0, 1, 1, 1, 0, 1])

# Calculate metrics
cm = confusion_matrix(y_true, y_pred)
accuracy = accuracy_score(y_true, y_pred)
precision = precision_score(y_true, y_pred)
recall = recall_score(y_true, y_pred)
f1 = f1_score(y_true, y_pred)

print(f"Confusion Matrix:\n{cm}")
print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1 Score: {f1:.4f}")</code></pre>
                                </div>
                            </div>
                        </div>
                        
                        <div class="accordion-item">
                            <div class="accordion-header" onclick="toggleAccordion(this)">
                                <span>ROC Curve and AUC</span>
                                <i class="fas fa-chevron-down accordion-icon"></i>
                            </div>
                            <div class="accordion-body">
                                <p>The Receiver Operating Characteristic (ROC) curve is a plot of the true positive rate against the false positive rate at various threshold settings. The Area Under the Curve (AUC) is a measure of the model's ability to distinguish between classes.</p>
                                
                                <div class="code-block">
                                    <span class="code-language">Python</span>
                                    <button class="copy-btn" onclick="copyCode(this)">Copy</button>
                                    <pre><code>from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt
import numpy as np

# Example: True labels and predicted probabilities
y_true = np.array([0, 1, 0, 1, 0, 1, 0, 1, 0, 1])
y_scores = np.array([0.1, 0.9, 0.2, 0.8, 0.3, 0.7, 0.4, 0.6, 0.5, 0.9])

# Calculate ROC curve
fpr, tpr, thresholds = roc_curve(y_true, y_scores)
roc_auc = auc(fpr, tpr)

# Plot ROC curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()</code></pre>
                                </div>
                                
                                <div class="viz-container">
                                    <div class="viz-title">ROC Curve</div>
                                    <div class="canvas-container">
                                        <canvas id="roc-curve" width="600" height="400"></canvas>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
                
                <div id="chapter7-3" class="subsection">
                    <h3>7.3 Classification via Bayes' Rule</h3>
                    <p>Bayesian classification is based on Bayes' theorem, which provides a way to calculate the posterior probability of a class given the features:</p>
                    
                    <div class="formula">
                        P(y|x) = (P(x|y)P(y)) / P(x)
                    </div>
                    
                    <p>where P(y|x) is the posterior probability of class y given features x, P(x|y) is the likelihood, P(y) is the prior probability of class y, and P(x) is the evidence.</p>
                    
                    <p>The Bayes classifier assigns the class with the highest posterior probability:</p>
                    
                    <div class="formula">
                        ŷ = argmax_y P(y|x) = argmax_y P(x|y)P(y)
                    </div>
                </div>
                
                <div id="chapter7-4" class="subsection">
                    <h3>7.4 Linear and Quadratic Discriminant Analysis</h3>
                    <p>Linear Discriminant Analysis (LDA) and Quadratic Discriminant Analysis (QDA) are classification methods based on Bayes' theorem with the assumption that the features follow a multivariate normal distribution.</p>
                    
                    <div class="tabs">
                        <div class="tab-buttons">
                            <button class="tab-button active" onclick="openTab(event, 'lda')">Linear Discriminant Analysis</button>
                            <button class="tab-button" onclick="openTab(event, 'qda')">Quadratic Discriminant Analysis</button>
                        </div>
                        
                        <div id="lda" class="tab-content">
                            <div class="tab-pane active">
                                <p><strong>Linear Discriminant Analysis (LDA)</strong> assumes that all classes share the same covariance matrix. Under this assumption, the decision boundary between classes is linear.</p>
                                
                                <p>The discriminant function for class k is:</p>
                                
                                <div class="formula">
                                    δ_k(x) = xᵀΣ⁻¹μ_k - (1/2)μ_kᵀΣ⁻¹μ_k + log(π_k)
                                </div>
                                
                                <p>where μ_k is the mean of class k, Σ is the common covariance matrix, and π_k is the prior probability of class k.</p>
                                
                                <div class="code-block">
                                    <span class="code-language">Python</span>
                                    <button class="copy-btn" onclick="copyCode(this)">Copy</button>
                                    <pre><code>from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import numpy as np
import matplotlib.pyplot as plt

# Generate synthetic data
X, y = make_classification(n_samples=300, n_features=2, n_redundant=0, 
                           n_informative=2, n_clusters_per_class=1, 
                           n_classes=2, random_state=42)

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Fit LDA
lda = LinearDiscriminantAnalysis()
lda.fit(X_train, y_train)

# Make predictions
y_pred = lda.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"LDA Accuracy: {accuracy:.4f}")

# Plot decision boundary
plt.figure(figsize=(10, 6))
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),
                     np.arange(y_min, y_max, 0.02))
Z = lda.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)
plt.contourf(xx, yy, Z, alpha=0.3)
plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', cmap='viridis')
plt.title('Linear Discriminant Analysis')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.show()</code></pre>
                                </div>
                            </div>
                        </div>
                        
                        <div id="qda" class="tab-content">
                            <div class="tab-pane">
                                <p><strong>Quadratic Discriminant Analysis (QDA)</strong> allows each class to have its own covariance matrix. This results in a quadratic decision boundary between classes.</p>
                                
                                <p>The discriminant function for class k is:</p>
                                
                                <div class="formula">
                                    δ_k(x) = -(1/2)log|Σ_k| - (1/2)(x - μ_k)ᵀΣ_k⁻¹(x - μ_k) + log(π_k)
                                </div>
                                
                                <p>where μ_k and Σ_k are the mean and covariance matrix of class k, and π_k is the prior probability of class k.</p>
                                
                                <div class="code-block">
                                    <span class="code-language">Python</span>
                                    <button class="copy-btn" onclick="copyCode(this)">Copy</button>
                                    <pre><code>from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import numpy as np
import matplotlib.pyplot as plt

# Generate synthetic data with different covariance matrices
np.random.seed(42)
n_samples = 300
X1 = np.random.multivariate_normal([0, 0], [[1, 0.5], [0.5, 1]], n_samples // 2)
X2 = np.random.multivariate_normal([2, 2], [[1, -0.8], [-0.8, 1]], n_samples // 2)
X = np.vstack([X1, X2])
y = np.hstack([np.zeros(n_samples // 2), np.ones(n_samples // 2)])

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Fit QDA
qda = QuadraticDiscriminantAnalysis()
qda.fit(X_train, y_train)

# Make predictions
y_pred = qda.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"QDA Accuracy: {accuracy:.4f}")

# Plot decision boundary
plt.figure(figsize=(10, 6))
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),
                     np.arange(y_min, y_max, 0.02))
Z = qda.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)
plt.contourf(xx, yy, Z, alpha=0.3)
plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', cmap='viridis')
plt.title('Quadratic Discriminant Analysis')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.show()</code></pre>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
                
                <div id="chapter7-5" class="subsection">
                    <h3>7.5 Logistic Regression and Softmax Classification</h3>
                    <p>Logistic regression is a classification algorithm that models the probability of a binary outcome as a function of the features. It uses the logistic (sigmoid) function to map the linear combination of features to a probability between 0 and 1.</p>
                    
                    <p>For binary classification, the logistic regression model is:</p>
                    
                    <div class="formula">
                        P(y=1|x) = 1 / (1 + exp(-(β₀ + β₁x₁ + ... + βₚxₚ)))
                    </div>
                    
                    <p>For multi-class classification, we use the softmax function, which generalizes the logistic function to multiple classes:</p>
                    
                    <div class="formula">
                        P(y=k|x) = exp(β₀ₖ + β₁ₖx₁ + ... + βₚₖxₚ) / Σⱼ₌₁ᴷ exp(β₀ⱼ + β₁ⱼx₁ + ... + βₚⱼxₚ)
                    </div>
                    
                    <div class="code-block">
                        <span class="code-language">Python</span>
                        <button class="copy-btn" onclick="copyCode(this)">Copy</button>
                        <pre><code>from sklearn.linear_model import LogisticRegression
from sklearn.datasets import make_classification, make_blobs
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix
import numpy as np
import matplotlib.pyplot as plt

# Binary classification example
X_binary, y_binary = make_classification(n_samples=300, n_features=2, n_redundant=0, 
                                        n_informative=2, n_clusters_per_class=1, 
                                        random_state=42)
X_train_b, X_test_b, y_train_b, y_test_b = train_test_split(X_binary, y_binary, test_size=0.3, random_state=42)

# Fit logistic regression
log_reg = LogisticRegression()
log_reg.fit(X_train_b, y_train_b)

# Make predictions
y_pred_b = log_reg.predict(X_test_b)
accuracy_b = accuracy_score(y_test_b, y_pred_b)
print(f"Binary Logistic Regression Accuracy: {accuracy_b:.4f}")

# Multi-class classification example
X_multi, y_multi = make_blobs(n_samples=300, centers=3, n_features=2, random_state=42)
X_train_m, X_test_m, y_train_m, y_test_m = train_test_split(X_multi, y_multi, test_size=0.3, random_state=42)

# Fit softmax regression
softmax_reg = LogisticRegression(multi_class='multinomial', solver='lbfgs')
softmax_reg.fit(X_train_m, y_train_m)

# Make predictions
y_pred_m = softmax_reg.predict(X_test_m)
accuracy_m = accuracy_score(y_test_m, y_pred_m)
print(f"Multi-class Softmax Regression Accuracy: {accuracy_m:.4f}")

# Plot decision boundaries
plt.figure(figsize=(15, 6))

# Binary classification
plt.subplot(1, 2, 1)
x_min, x_max = X_binary[:, 0].min() - 1, X_binary[:, 0].max() + 1
y_min, y_max = X_binary[:, 1].min() - 1, X_binary[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),
                     np.arange(y_min, y_max, 0.02))
Z = log_reg.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)
plt.contourf(xx, yy, Z, alpha=0.3)
plt.scatter(X_binary[:, 0], X_binary[:, 1], c=y_binary, edgecolors='k', cmap='viridis')
plt.title('Binary Logistic Regression')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')

# Multi-class classification
plt.subplot(1, 2, 2)
x_min, x_max = X_multi[:, 0].min() - 1, X_multi[:, 0].max() + 1
y_min, y_max = X_multi[:, 1].min() - 1, X_multi[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),
                     np.arange(y_min, y_max, 0.02))
Z = softmax_reg.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)
plt.contourf(xx, yy, Z, alpha=0.3)
plt.scatter(X_multi[:, 0], X_multi[:, 1], c=y_multi, edgecolors='k', cmap='viridis')
plt.title('Multi-class Softmax Regression')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')

plt.tight_layout()
plt.show()</code></pre>
                    </div>
                    
                    <div class="viz-container">
                        <div class="viz-title">Logistic Regression and Softmax Classification</div>
                        <div class="canvas-container">
                            <canvas id="logistic-regression" width="600" height="400"></canvas>
                        </div>
                    </div>
                </div>
                
                <div id="chapter7-6" class="subsection">
                    <h3>7.6 K-Nearest Neighbors Classification</h3>
                    <p>K-Nearest Neighbors (KNN) is a non-parametric classification method that classifies a new observation based on the majority class of its k nearest neighbors in the training data.</p>
                    
                    <p>The algorithm works as follows:</p>
                    <ol>
                        <li>Choose a value for k (the number of neighbors).</li>
                        <li>For a new observation, find the k nearest neighbors in the training data based on a distance metric (usually Euclidean distance).</li>
                        <li>Assign the class that is most common among the k nearest neighbors.</li>
                    </ol>
                    
                    <div class="code-block">
                        <span class="code-language">Python</span>
                        <button class="copy-btn" onclick="copyCode(this)">Copy</button>
                        <pre><code>from sklearn.neighbors import KNeighborsClassifier
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import numpy as np
import matplotlib.pyplot as plt

# Generate synthetic data
X, y = make_classification(n_samples=300, n_features=2, n_redundant=0, 
                           n_informative=2, n_clusters_per_class=1, 
                           n_classes=2, random_state=42)

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Try different values of k
k_values = [1, 3, 5, 10, 20]
accuracies = []

for k in k_values:
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(X_train, y_train)
    y_pred = knn.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    accuracies.append(accuracy)
    print(f"k={k}, Accuracy: {accuracy:.4f}")

# Find the best k
best_k = k_values[np.argmax(accuracies)]
print(f"Best k: {best_k}")

# Fit the best model
knn_best = KNeighborsClassifier(n_neighbors=best_k)
knn_best.fit(X_train, y_train)

# Plot decision boundary
plt.figure(figsize=(10, 6))
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),
                     np.arange(y_min, y_max, 0.02))
Z = knn_best.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)
plt.contourf(xx, yy, Z, alpha=0.3)
plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', cmap='viridis')
plt.title(f'K-Nearest Neighbors (k={best_k})')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.show()</code></pre>
                    </div>
                    
                    <div class="viz-container">
                        <div class="viz-title">K-Nearest Neighbors Classification</div>
                        <div class="canvas-container">
                            <canvas id="knn-classification" width="600" height="400"></canvas>
                        </div>
                    </div>
                </div>
                
                <div id="chapter7-7" class="subsection">
                    <h3>7.7 Support Vector Machine</h3>
                    <p>Support Vector Machine (SVM) is a powerful classification method that finds the optimal hyperplane that separates the classes with the maximum margin. For non-linearly separable data, SVM uses the kernel trick to map the data to a higher-dimensional space where it becomes linearly separable.</p>
                    
                    <div class="tabs">
                        <div class="tab-buttons">
                            <button class="tab-button active" onclick="openTab(event, 'linear-svm')">Linear SVM</button>
                            <button class="tab-button" onclick="openTab(event, 'kernel-svm')">Kernel SVM</button>
                        </div>
                        
                        <div id="linear-svm" class="tab-content">
                            <div class="tab-pane active">
                                <p><strong>Linear SVM</strong> finds the optimal hyperplane that separates the classes with the maximum margin. The optimization problem is:</p>
                                
                                <div class="formula">
                                    min_{w,b,ξ} (1/2)||w||² + C Σᵢ₌₁ⁿ ξᵢ
                                </div>
                                
                                <p>subject to:</p>
                                
                                <div class="formula">
                                    yᵢ(wᵀxᵢ + b) ≥ 1 - ξᵢ, ξᵢ ≥ 0
                                </div>
                                
                                <p>where w is the normal vector to the hyperplane, b is the bias term, ξᵢ are slack variables that allow for misclassifications, and C is a regularization parameter.</p>
                                
                                <div class="code-block">
                                    <span class="code-language">Python</span>
                                    <button class="copy-btn" onclick="copyCode(this)">Copy</button>
                                    <pre><code>from sklearn.svm import SVC
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import numpy as np
import matplotlib.pyplot as plt

# Generate synthetic data
X, y = make_classification(n_samples=300, n_features=2, n_redundant=0, 
                           n_informative=2, n_clusters_per_class=1, 
                           n_classes=2, random_state=42)

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Fit linear SVM
svm_linear = SVC(kernel='linear', C=1.0)
svm_linear.fit(X_train, y_train)

# Make predictions
y_pred = svm_linear.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"Linear SVM Accuracy: {accuracy:.4f}")

# Plot decision boundary
plt.figure(figsize=(10, 6))
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),
                     np.arange(y_min, y_max, 0.02))
Z = svm_linear.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)
plt.contourf(xx, yy, Z, alpha=0.3)
plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', cmap='viridis')

# Plot support vectors
plt.scatter(svm_linear.support_vectors_[:, 0], svm_linear.support_vectors_[:, 1],
            s=100, facecolors='none', edgecolors='k', linewidth=1.5)

# Plot decision boundary and margins
ax = plt.gca()
xlim = ax.get_xlim()
ylim = ax.get_ylim()

# Create grid to evaluate model
xx = np.linspace(xlim[0], xlim[1], 30)
yy = np.linspace(ylim[0], ylim[1], 30)
YY, XX = np.meshgrid(yy, xx)
xy = np.vstack([XX.ravel(), YY.ravel()]).T
Z = svm_linear.decision_function(xy).reshape(XX.shape)

# Plot decision boundary and margins
ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5,
           linestyles=['--', '-', '--'])

plt.title('Linear Support Vector Machine')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.show()</code></pre>
                                </div>
                            </div>
                        </div>
                        
                        <div id="kernel-svm" class="tab-content">
                            <div class="tab-pane">
                                <p><strong>Kernel SVM</strong> uses the kernel trick to map the data to a higher-dimensional space where it becomes linearly separable. Common kernels include:</p>
                                
                                <ul>
                                    <li><strong>Polynomial kernel:</strong> K(x, y) = (xᵀy + c)ᵈ</li>
                                    <li><strong>RBF (Gaussian) kernel:</strong> K(x, y) = exp(-γ||x - y||²)</li>
                                    <li><strong>Sigmoid kernel:</strong> K(x, y) = tanh(αxᵀy + c)</li>
                                </ul>
                                
                                <div class="code-block">
                                    <span class="code-language">Python</span>
                                    <button class="copy-btn" onclick="copyCode(this)">Copy</button>
                                    <pre><code>from sklearn.svm import SVC
from sklearn.datasets import make_moons
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import numpy as np
import matplotlib.pyplot as plt

# Generate non-linearly separable data
X, y = make_moons(n_samples=300, noise=0.2, random_state=42)

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Fit SVM with RBF kernel
svm_rbf = SVC(kernel='rbf', C=1.0, gamma='scale')
svm_rbf.fit(X_train, y_train)

# Make predictions
y_pred = svm_rbf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"RBF SVM Accuracy: {accuracy:.4f}")

# Plot decision boundary
plt.figure(figsize=(10, 6))
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),
                     np.arange(y_min, y_max, 0.02))
Z = svm_rbf.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)
plt.contourf(xx, yy, Z, alpha=0.3)
plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', cmap='viridis')

# Plot support vectors
plt.scatter(svm_rbf.support_vectors_[:, 0], svm_rbf.support_vectors_[:, 1],
            s=100, facecolors='none', edgecolors='k', linewidth=1.5)

plt.title('RBF Support Vector Machine')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.show()</code></pre>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
                
                <div id="chapter7-8" class="subsection">
                    <h3>7.8 Classification with Scikit-Learn</h3>
                    <p>Scikit-learn provides a unified interface for various classification algorithms. This makes it easy to compare different methods and choose the best one for a particular problem.</p>
                    
                    <div class="code-block">
                        <span class="code-language">Python</span>
                        <button class="copy-btn" onclick="copyCode(this)">Copy</button>
                        <pre><code>from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from sklearn.linear_model import LogisticRegression
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
import pandas as pd

# Generate synthetic data
X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, 
                           n_redundant=5, n_classes=3, random_state=42)

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Define classifiers
classifiers = {
    'Logistic Regression': LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000),
    'Linear Discriminant Analysis': LinearDiscriminantAnalysis(),
    'Quadratic Discriminant Analysis': QuadraticDiscriminantAnalysis(),
    'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5),
    'Support Vector Machine': SVC(kernel='rbf', C=1.0, gamma='scale'),
    'Decision Tree': DecisionTreeClassifier(random_state=42),
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42)
}

# Train and evaluate classifiers
results = []
for name, clf in classifiers.items():
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    results.append({'Classifier': name, 'Accuracy': accuracy})
    print(f"{name}: Accuracy = {accuracy:.4f}")
    print(classification_report(y_test, y_pred))
    print("-" * 50)

# Create a results table
results_df = pd.DataFrame(results)
print("\nSummary of Results:")
print(results_df)</code></pre>
                    </div>
                </div>
            </section>

            <!-- Additional chapters would continue here -->
            <!-- For brevity, I'm including just a few more chapters with key content -->

            <!-- Chapter 8 Section -->
            <section id="chapter8" class="section">
                <h2>Chapter 8: Decision Trees and Ensemble Methods</h2>
                
                <div id="chapter8-1" class="subsection">
                    <h3>8.1 Introduction</h3>
                    <p>Decision trees are a popular method for both classification and regression tasks. They work by recursively partitioning the feature space into regions and fitting a simple model (usually a constant) in each region. Ensemble methods combine multiple decision trees to improve predictive performance and reduce overfitting.</p>
                </div>
                
                <div id="chapter8-2" class="subsection">
                    <h3>8.2 Top-Down Construction of Decision Trees</h3>
                    <p>Decision trees are typically constructed using a top-down, greedy approach called recursive binary splitting. At each step, the algorithm selects the best split based on a criterion such as Gini impurity or entropy for classification, and mean squared error for regression.</p>
                    
                    <div class="accordion">
                        <div class="accordion-item">
                            <div class="accordion-header" onclick="toggleAccordion(this)">
                                <span>Regional Prediction Functions</span>
                                <i class="fas fa-chevron-down accordion-icon"></i>
                            </div>
                            <div class="accordion-body">
                                <p>In a decision tree, each region (leaf node) has a prediction function. For classification, this is typically the majority class in the region. For regression, this is typically the mean of the target values in the region.</p>
                            </div>
                        </div>
                        
                        <div class="accordion-item">
                            <div class="accordion-header" onclick="toggleAccordion(this)">
                                <span>Splitting Rules</span>
                                <i class="fas fa-chevron-down accordion-icon"></i>
                            </div>
                            <div class="accordion-body">
                                <p>The choice of splitting rule determines how the tree grows. For classification, common criteria include:</p>
                                
                                <ul>
                                    <li><strong>Gini impurity:</strong> Measures the probability of misclassifying a randomly chosen element.</li>
                                    <li><strong>Entropy:</strong> Measures the level of disorder in the node.</li>
                                </ul>
                                
                                <p>For regression, common criteria include:</p>
                                
                                <ul>
                                    <li><strong>Mean squared error:</strong> Measures the average squared difference between the predicted and actual values.</li>
                                    <li><strong>Mean absolute error:</strong> Measures the average absolute difference between the predicted and actual values.</li>
                                </ul>
                            </div>
                        </div>
                        
                        <div class="accordion-item">
                            <div class="accordion-header" onclick="toggleAccordion(this)">
                                <span>Termination Criterion</span>
                                <i class="fas fa-chevron-down accordion-icon"></i>
                            </div>
                            <div class="accordion-body">
                                <p>The tree stops growing when one of the following conditions is met:</p>
                                
                                <ul>
                                    <li>All samples in a node belong to the same class (for classification).</li>
                                    <li>The node contains fewer than a specified number of samples.</li>
                                    <li>The tree reaches a maximum depth.</li>
                                    <li>The improvement in the splitting criterion is below a threshold.</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                </div>
                
                <div id="chapter8-3" class="subsection">
                    <h3>8.3 Additional Considerations</h3>
                    <p>When building decision trees, there are several additional considerations to keep in mind:</p>
                    
                    <ul>
                        <li><strong>Binary vs. Non-Binary Trees:</strong> Binary trees split each node into two child nodes, while non-binary trees can split into more than two child nodes.</li>
                        <li><strong>Data Preprocessing:</strong> Decision trees can handle both numerical and categorical data without extensive preprocessing.</li>
                        <li><strong>Alternative Splitting Rules:</strong> Different splitting rules can lead to different trees.</li>
                        <li><strong>Categorical Variables:</strong> Special handling may be required for categorical variables with many levels.</li>
                        <li><strong>Missing Values:</strong> Decision trees can handle missing values in various ways, such as using surrogate splits.</li>
                    </ul>
                </div>
                
                <div id="chapter8-4" class="subsection">
                    <h3>8.4 Controlling the Tree Shape</h3>
                    <p>Decision trees are prone to overfitting, especially when they grow deep. To prevent overfitting, we can control the shape of the tree using various techniques:</p>
                    
                    <div class="accordion">
                        <div class="accordion-item">
                            <div class="accordion-header" onclick="toggleAccordion(this)">
                                <span>Cost-Complexity Pruning</span>
                                <i class="fas fa-chevron-down accordion-icon"></i>
                            </div>
                            <div class="accordion-body">
                                <p>Cost-complexity pruning (also known as weakest link pruning) grows a large tree and then prunes it back. The pruning process is guided by a complexity parameter α that balances the trade-off between tree complexity and goodness of fit:</p>
                                
                                <div class="formula">
                                    R_α(T) = R(T) + α|T|
                                </div>
                                
                                <p>where R(T) is the training error, |T| is the number of terminal nodes, and α is the complexity parameter.</p>
                                
                                <div class="code-block">
                                    <span class="code-language">Python</span>
                                    <button class="copy-btn" onclick="copyCode(this)">Copy</button>
                                    <pre><code>from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt

# Generate synthetic data
X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, 
                           n_redundant=0, n_classes=3, random_state=42)

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Grow a full tree
full_tree = DecisionTreeClassifier(random_state=42)
full_tree.fit(X_train, y_train)

# Prune the tree using cost-complexity pruning
path = full_tree.cost_complexity_pruning_path(X_train, y_train)
ccp_alphas, impurities = path.ccp_alphas, path.impurities

# Train trees with different alpha values
trees = []
for ccp_alpha in ccp_alphas:
    clf = DecisionTreeClassifier(random_state=42, ccp_alpha=ccp_alpha)
    clf.fit(X_train, y_train)
    trees.append(clf)

# Evaluate the trees
train_scores = [accuracy_score(y_train, tree.predict(X_train)) for tree in trees]
test_scores = [accuracy_score(y_test, tree.predict(X_test)) for tree in trees]

# Find the best tree
best_idx = test_scores.index(max(test_scores))
best_tree = trees[best_idx]
best_alpha = ccp_alphas[best_idx]

print(f"Best alpha: {best_alpha:.6f}")
print(f"Training accuracy: {train_scores[best_idx]:.4f}")
print(f"Test accuracy: {test_scores[best_idx]:.4f}")

# Plot the results
plt.figure(figsize=(12, 6))
plt.plot(ccp_alphas, train_scores, marker='o', label='Train accuracy')
plt.plot(ccp_alphas, test_scores, marker='o', label='Test accuracy')
plt.xlabel('Effective alpha')
plt.ylabel('Accuracy')
plt.title('Accuracy vs. Effective Alpha for Pruning')
plt.legend()
plt.show()

# Plot the best tree
plt.figure(figsize=(20, 10))
plot_tree(best_tree, filled=True, feature_names=[f'Feature {i}' for i in range(X.shape[1])], 
          class_names=[f'Class {i}' for i in range(3)], rounded=True)
plt.title(f'Best Decision Tree (alpha={best_alpha:.6f})')
plt.show()</code></pre>
                                </div>
                            </div>
                        </div>
                        
                        <div class="accordion-item">
                            <div class="accordion-header" onclick="toggleAccordion(this)">
                                <span>Advantages and Limitations of Decision Trees</span>
                                <i class="fas fa-chevron-down accordion-icon"></i>
                            </div>
                            <div class="accordion-body">
                                <p><strong>Advantages:</strong></p>
                                <ul>
                                    <li>Easy to understand and interpret.</li>
                                    <li>Can handle both numerical and categorical data.</li>
                                    <li>Requires little data preprocessing.</li>
                                    <li>Can capture non-linear relationships.</li>
                                </ul>
                                
                                <p><strong>Limitations:</strong></p>
                                <ul>
                                    <li>Prone to overfitting.</li>
                                    <li>Can be unstable (small changes in data can lead to different trees).</li>
                                    <li>Can have biased feature selection if some features have more levels.</li>
                                    <li>May not capture complex relationships without growing deep.</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                </div>
                
                <div id="chapter8-5" class="subsection">
                    <h3>8.5 Bootstrap Aggregation</h3>
                    <p>Bootstrap Aggregation (Bagging) is an ensemble method that reduces variance and helps avoid overfitting. It works by training multiple models on different bootstrap samples of the training data and then combining their predictions through averaging (for regression) or voting (for classification).</p>
                    
                    <div class="code-block">
                        <span class="code-language">Python</span>
                        <button class="copy-btn" onclick="copyCode(this)">Copy</button>
                        <pre><code>from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import numpy as np
import matplotlib.pyplot as plt

# Generate synthetic data
X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, 
                           n_redundant=5, n_classes=3, random_state=42)

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Compare single decision tree with bagging
single_tree = DecisionTreeClassifier(random_state=42)
single_tree.fit(X_train, y_train)
single_pred = single_tree.predict(X_test)
single_accuracy = accuracy_score(y_test, single_pred)
print(f"Single Decision Tree Accuracy: {single_accuracy:.4f}")

# Try different numbers of estimators
n_estimators_range = [1, 5, 10, 20, 50, 100]
bagging_accuracies = []

for n_estimators in n_estimators_range:
    bagging = BaggingClassifier(
        base_estimator=DecisionTreeClassifier(random_state=42),
        n_estimators=n_estimators,
        max_samples=0.8,
        max_features=0.8,
        random_state=42
    )
    bagging.fit(X_train, y_train)
    bagging_pred = bagging.predict(X_test)
    bagging_accuracy = accuracy_score(y_test, bagging_pred)
    bagging_accuracies.append(bagging_accuracy)
    print(f"Bagging with {n_estimators} trees: Accuracy = {bagging_accuracy:.4f}")

# Plot the results
plt.figure(figsize=(10, 6))
plt.plot(n_estimators_range, bagging_accuracies, marker='o')
plt.axhline(y=single_accuracy, color='r', linestyle='--', label='Single Tree')
plt.xlabel('Number of Estimators')
plt.ylabel('Accuracy')
plt.title('Bagging Performance vs. Number of Estimators')
plt.legend()
plt.show()</code></pre>
                    </div>
                    
                    <div class="viz-container">
                        <div class="viz-title">Bagging Performance vs. Number of Estimators</div>
                        <div class="canvas-container">
                            <canvas id="bagging-performance" width="600" height="400"></canvas>
                        </div>
                    </div>
                </div>
                
                <div id="chapter8-6" class="subsection">
                    <h3>8.6 Random Forests</h3>
                    <p>Random Forests are an extension of bagging that further reduces variance by introducing randomness in feature selection. In addition to sampling the data, Random Forests also sample the features at each split, which decorrelates the trees and often leads to better performance.</p>
                    
                    <div class="code-block">
                        <span class="code-language">Python</span>
                        <button class="copy-btn" onclick="copyCode(this)">Copy</button>
                        <pre><code>from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Generate synthetic data
X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, 
                           n_redundant=5, n_classes=3, random_state=42)

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Try different numbers of estimators
n_estimators_range = [10, 50, 100, 200, 500]
rf_accuracies = []

for n_estimators in n_estimators_range:
    rf = RandomForestClassifier(
        n_estimators=n_estimators,
        max_features='sqrt',  # Default for classification
        random_state=42
    )
    rf.fit(X_train, y_train)
    rf_pred = rf.predict(X_test)
    rf_accuracy = accuracy_score(y_test, rf_pred)
    rf_accuracies.append(rf_accuracy)
    print(f"Random Forest with {n_estimators} trees: Accuracy = {rf_accuracy:.4f}")

# Find the best number of estimators
best_n_estimators = n_estimators_range[np.argmax(rf_accuracies)]
best_rf = RandomForestClassifier(
    n_estimators=best_n_estimators,
    max_features='sqrt',
    random_state=42
)
best_rf.fit(X_train, y_train)
best_pred = best_rf.predict(X_test)

print(f"\nBest Random Forest (n_estimators={best_n_estimators}):")
print(f"Accuracy: {accuracy_score(y_test, best_pred):.4f}")

# Plot feature importances
feature_importances = best_rf.feature_importances_
indices = np.argsort(feature_importances)[::-1]

plt.figure(figsize=(12, 6))
plt.title("Feature Importances")
plt.bar(range(X.shape[1]), feature_importances[indices], align="center")
plt.xticks(range(X.shape[1]), [f"Feature {i}" for i in indices])
plt.xlim([-1, X.shape[1]])
plt.tight_layout()
plt.show()

# Plot confusion matrix
cm = confusion_matrix(y_test, best_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
            xticklabels=[f'Class {i}' for i in range(3)],
            yticklabels=[f'Class {i}' for i in range(3)])
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()</code></pre>
                    </div>
                    
                    <div class="viz-container">
                        <div class="viz-title">Random Forest Feature Importances</div>
                        <div class="canvas-container">
                            <canvas id="rf-feature-importance" width="600" height="400"></canvas>
                        </div>
                    </div>
                </div>
                
                <div id="chapter8-7" class="subsection">
                    <h3>8.7 Boosting</h3>
                    <p>Boosting is an ensemble method that combines multiple weak learners (typically shallow decision trees) to create a strong learner. Unlike bagging, which trains models independently, boosting trains models sequentially, with each new model focusing on the mistakes of the previous ones.</p>
                    
                    <div class="tabs">
                        <div class="tab-buttons">
                            <button class="tab-button active" onclick="openTab(event, 'ada')">AdaBoost</button>
                            <button class="tab-button" onclick="openTab(event, 'gradient')">Gradient Boosting</button>
                            <button class="tab-button" onclick="openTab(event, 'xgboost')">XGBoost</button>
                        </div>
                        
                        <div id="ada" class="tab-content">
                            <div class="tab-pane active">
                                <p><strong>AdaBoost (Adaptive Boosting)</strong> works by training weak learners sequentially, with each new learner giving more weight to the samples that were misclassified by the previous learners.</p>
                                
                                <div class="code-block">
                                    <span class="code-language">Python</span>
                                    <button class="copy-btn" onclick="copyCode(this)">Copy</button>
                                    <pre><code>from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import numpy as np
import matplotlib.pyplot as plt

# Generate synthetic data
X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, 
                           n_redundant=5, n_classes=2, random_state=42)

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Try different numbers of estimators
n_estimators_range = [10, 50, 100, 200, 500]
ada_accuracies = []

for n_estimators in n_estimators_range:
    ada = AdaBoostClassifier(
        base_estimator=DecisionTreeClassifier(max_depth=1),  # Stump
        n_estimators=n_estimators,
        learning_rate=1.0,
        random_state=42
    )
    ada.fit(X_train, y_train)
    ada_pred = ada.predict(X_test)
    ada_accuracy = accuracy_score(y_test, ada_pred)
    ada_accuracies.append(ada_accuracy)
    print(f"AdaBoost with {n_estimators} stumps: Accuracy = {ada_accuracy:.4f}")

# Plot the results
plt.figure(figsize=(10, 6))
plt.plot(n_estimators_range, ada_accuracies, marker='o')
plt.xlabel('Number of Estimators')
plt.ylabel('Accuracy')
plt.title('AdaBoost Performance vs. Number of Estimators')
plt.show()</code></pre>
                                </div>
                            </div>
                        </div>
                        
                        <div id="gradient" class="tab-content">
                            <div class="tab-pane">
                                <p><strong>Gradient Boosting</strong> builds the model in a stage-wise fashion, with each new model predicting the residuals of the previous models. It uses gradient descent to minimize a loss function.</p>
                                
                                <div class="code-block">
                                    <span class="code-language">Python</span>
                                    <button class="copy-btn" onclick="copyCode(this)">Copy</button>
                                    <pre><code>from sklearn.ensemble import GradientBoostingClassifier
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import numpy as np
import matplotlib.pyplot as plt

# Generate synthetic data
X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, 
                           n_redundant=5, n_classes=2, random_state=42)

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Try different numbers of estimators
n_estimators_range = [10, 50, 100, 200, 500]
gb_accuracies = []

for n_estimators in n_estimators_range:
    gb = GradientBoostingClassifier(
        n_estimators=n_estimators,
        learning_rate=0.1,
        max_depth=3,
        random_state=42
    )
    gb.fit(X_train, y_train)
    gb_pred = gb.predict(X_test)
    gb_accuracy = accuracy_score(y_test, gb_pred)
    gb_accuracies.append(gb_accuracy)
    print(f"Gradient Boosting with {n_estimators} trees: Accuracy = {gb_accuracy:.4f}")

# Plot the results
plt.figure(figsize=(10, 6))
plt.plot(n_estimators_range, gb_accuracies, marker='o')
plt.xlabel('Number of Estimators')
plt.ylabel('Accuracy')
plt.title('Gradient Boosting Performance vs. Number of Estimators')
plt.show()</code></pre>
                                </div>
                            </div>
                        </div>
                        
                        <div id="xgboost" class="tab-content">
                            <div class="tab-pane">
                                <p><strong>XGBoost (Extreme Gradient Boosting)</strong> is an optimized implementation of gradient boosting that includes regularization to prevent overfitting and handles missing values. It is known for its performance and speed.</p>
                                
                                <div class="code-block">
                                    <span class="code-language">Python</span>
                                    <button class="copy-btn" onclick="copyCode(this)">Copy</button>
                                    <pre><code># Note: XGBoost needs to be installed separately: pip install xgboost
import xgboost as xgb
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import numpy as np
import matplotlib.pyplot as plt

# Generate synthetic data
X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, 
                           n_redundant=5, n_classes=2, random_state=42)

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Convert to DMatrix (optimized data structure for XGBoost)
dtrain = xgb.DMatrix(X_train, label=y_train)
dtest = xgb.DMatrix(X_test, label=y_test)

# Define parameters
params = {
    'objective': 'binary:logistic',
    'max_depth': 3,
    'eta': 0.1,
    'subsample': 0.8,
    'colsample_bytree': 0.8,
    'seed': 42
}

# Try different numbers of estimators
n_estimators_range = [10, 50, 100, 200, 500]
xgb_accuracies = []

for n_estimators in n_estimators_range:
    model = xgb.train(params, dtrain, num_boost_round=n_estimators)
    y_pred_proba = model.predict(dtest)
    y_pred = (y_pred_proba > 0.5).astype(int)
    xgb_accuracy = accuracy_score(y_test, y_pred)
    xgb_accuracies.append(xgb_accuracy)
    print(f"XGBoost with {n_estimators} trees: Accuracy = {xgb_accuracy:.4f}")

# Plot the results
plt.figure(figsize=(10, 6))
plt.plot(n_estimators_range, xgb_accuracies, marker='o')
plt.xlabel('Number of Estimators')
plt.ylabel('Accuracy')
plt.title('XGBoost Performance vs. Number of Estimators')
plt.show()</code></pre>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </section>

            <!-- Chapter 9 Section -->
            <section id="chapter9" class="section">
                <h2>Chapter 9: Deep Learning</h2>
                
                <div id="chapter9-1" class="subsection">
                    <h3>9.1 Introduction</h3>
                    <p>Deep Learning is a subfield of machine learning that uses neural networks with multiple layers (deep neural networks) to model complex patterns in data. These networks are inspired by the structure and function of the human brain, with interconnected nodes (neurons) organized in layers.</p>
                    
                    <p>Deep learning has achieved remarkable success in various domains, including:</p>
                    <ul>
                        <li><strong>Computer Vision:</strong> Image classification, object detection, segmentation.</li>
                        <li><strong>Natural Language Processing:</strong> Machine translation, sentiment analysis, text generation.</li>
                        <li><strong>Speech Recognition:</strong> Converting spoken language to text.</li>
                        <li><strong>Game Playing:</strong> Mastering complex games like Go and Chess.</li>
                    </ul>
                </div>
                
                <div id="chapter9-2" class="subsection">
                    <h3>9.2 Feed-Forward Neural Networks</h3>
                    <p>A feed-forward neural network is the simplest type of artificial neural network. Information flows in one direction, from input to output, with no cycles or loops.</p>
                    
                    <p>A typical feed-forward neural network consists of:</p>
                    <ul>
                        <li><strong>Input Layer:</strong> Receives the raw features.</li>
                        <li><strong>Hidden Layers:</strong> Perform computations and extract features.</li>
                        <li><strong>Output Layer:</strong> Produces the final prediction.</li>
                    </ul>
                    
                    <p>Each neuron in a layer receives inputs from all neurons in the previous layer, computes a weighted sum, applies an activation function, and passes the result to the next layer.</p>
                    
                    <div class="formula">
                        a_j^(l) = f(Σᵢ w_{ji}^(l) a_i^(l-1) + b_j^(l))
                    </div>
                    
                    <p>where a_j^(l) is the activation of neuron j in layer l, w_{ji}^(l) is the weight from neuron i in layer l-1 to neuron j in layer l, b_j^(l) is the bias of neuron j in layer l, and f is the activation function.</p>
                    
                    <div class="alert alert-info">
                        <strong>Common Activation Functions:</strong>
                        <ul>
                            <li><strong>Sigmoid:</strong> f(x) = 1 / (1 + exp(-x))</li>
                            <li><strong>Tanh:</strong> f(x) = tanh(x)</li>
                            <li><strong>ReLU (Rectified Linear Unit):</strong> f(x) = max(0, x)</li>
                            <li><strong>Leaky ReLU:</strong> f(x) = max(αx, x) for small α > 0</li>
                        </ul>
                    </div>
                </div>
                
                <div id="chapter9-3" class="subsection">
                    <h3>9.3 Back-Propagation</h3>
                    <p>Back-propagation is the algorithm used to train neural networks. It computes the gradient of the loss function with respect to the weights of the network, which is then used to update the weights using gradient descent.</p>
                    
                    <p>The algorithm works as follows:</p>
                    <ol>
                        <li><strong>Forward Pass:</strong> Compute the activations of all neurons in the network.</li>
                        <li><strong>Compute Loss:</strong> Calculate the difference between the predicted and actual outputs.</li>
                        <li><strong>Backward Pass:</strong> Compute the gradient of the loss with respect to the weights using the chain rule.</li>
                        <li><strong>Update Weights:</strong> Adjust the weights using gradient descent.</li>
                    </ol>
                    
                    <p>For a given weight w, the update rule is:</p>
                    
                    <div class="formula">
                        w ← w - η × ∂L/∂w
                    </div>
                    
                    <p>where η is the learning rate and L is the loss function.</p>
                </div>
                
                <div id="chapter9-4" class="subsection">
                    <h3>9.4 Methods for Training</h3>
                    <p>Several optimization algorithms can be used to train neural networks:</p>
                    
                    <div class="accordion">
                        <div class="accordion-item">
                            <div class="accordion-header" onclick="toggleAccordion(this)">
                                <span>Steepest Descent</span>
                                <i class="fas fa-chevron-down accordion-icon"></i>
                            </div>
                            <div class="accordion-body">
                                <p>Steepest descent (or gradient descent) updates the weights in the direction of the negative gradient of the loss function. It can be applied in three variants:</p>
                                
                                <ul>
                                    <li><strong>Batch Gradient Descent:</strong> Uses the entire dataset to compute the gradient.</li>
                                    <li><strong>Stochastic Gradient Descent (SGD):</strong> Uses one sample at a time to compute the gradient.</li>
                                    <li><strong>Mini-batch Gradient Descent:</strong> Uses a small batch of samples to compute the gradient.</li>
                                </ul>
                            </div>
                        </div>
                        
                        <div class="accordion-item">
                            <div class="accordion-header" onclick="toggleAccordion(this)">
                                <span>Levenberg-Marquardt Method</span>
                                <i class="fas fa-chevron-down accordion-icon"></i>
                            </div>
                            <div class="accordion-body">
                                <p>The Levenberg-Marquardt algorithm is a popular optimization method for training small to medium-sized neural networks. It interpolates between the Gauss-Newton algorithm and gradient descent, making it more robust than either method alone.</p>
                            </div>
                        </div>
                        
                        <div class="accordion-item">
                            <div class="accordion-header" onclick="toggleAccordion(this)">
                                <span>Limited-Memory BFGS Method</span>
                                <i class="fas fa-chevron-down accordion-icon"></i>
                            </div>
                            <div class="accordion-body">
                                <p>The Limited-Memory BFGS (L-BFGS) algorithm is a quasi-Newton method that approximates the BFGS algorithm using a limited amount of memory. It is particularly useful for problems with a large number of parameters.</p>
                            </div>
                        </div>
                        
                        <div class="accordion-item">
                            <div class="accordion-header" onclick="toggleAccordion(this)">
                                <span>Adaptive Gradient Methods</span>
                                <i class="fas fa-chevron-down accordion-icon"></i>
                            </div>
                            <div class="accordion-body">
                                <p>Adaptive gradient methods adjust the learning rate for each parameter based on the historical gradients. Popular methods include:</p>
                                
                                <ul>
                                    <li><strong>AdaGrad:</strong> Adapts the learning rate based on the historical gradients.</li>
                                    <li><strong>RMSProp:</strong> Uses a moving average of squared gradients to normalize the gradient.</li>
                                    <li><strong>Adam:</strong> Combines the advantages of AdaGrad and RMSProp, using both first and second moments of the gradients.</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                </div>
                
                <div id="chapter9-5" class="subsection">
                    <h3>9.5 Examples in Python</h3>
                    
                    <div class="tabs">
                        <div class="tab-buttons">
                            <button class="tab-button active" onclick="openTab(event, 'polynomial')">Simple Polynomial Regression</button>
                            <button class="tab-button" onclick="openTab(event, 'image')">Image Classification</button>
                        </div>
                        
                        <div id="polynomial" class="tab-content">
                            <div class="tab-pane active">
                                <div class="code-block">
                                    <span class="code-language">Python</span>
                                    <button class="copy-btn" onclick="copyCode(this)">Copy</button>
                                    <pre><code>import numpy as np
import matplotlib.pyplot as plt
from sklearn.neural_network import MLPRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline

# Generate synthetic data
np.random.seed(42)
n_samples = 1000
X = np.random.uniform(-10, 10, size=(n_samples, 1))
y = 0.5 * X.squeeze()**3 - 2 * X.squeeze()**2 + X.squeeze() + 5 + np.random.normal(0, 10, size=n_samples)

# Split the data
train_size = int(0.8 * n_samples)
X_train, X_test = X[:train_size], X[train_size:]
y_train, y_test = y[:train_size], y[train_size:]

# Create a neural network regressor
mlp = make_pipeline(
    StandardScaler(),
    MLPRegressor(
        hidden_layer_sizes=(100, 50),  # Two hidden layers with 100 and 50 neurons
        activation='relu',
        solver='adam',
        learning_rate_init=0.001,
        max_iter=1000,
        random_state=42
    )
)

# Train the model
mlp.fit(X_train, y_train)

# Evaluate the model
train_score = mlp.score(X_train, y_train)
test_score = mlp.score(X_test, y_test)
print(f"Training R²: {train_score:.4f}")
print(f"Test R²: {test_score:.4f}")

# Make predictions
X_grid = np.linspace(-10, 10, 1000).reshape(-1, 1)
y_pred = mlp.predict(X_grid)

# Plot the results
plt.figure(figsize=(12, 6))
plt.scatter(X_train, y_train, alpha=0.5, label='Training data')
plt.scatter(X_test, y_test, alpha=0.5, label='Test data')
plt.plot(X_grid, y_pred, 'r-', label='Neural Network Prediction')
plt.xlabel('X')
plt.ylabel('y')
plt.title('Neural Network for Polynomial Regression')
plt.legend()
plt.show()</code></pre>
                                </div>
                                
                                <div class="viz-container">
                                    <div class="viz-title">Neural Network for Polynomial Regression</div>
                                    <div class="canvas-container">
                                        <canvas id="nn-polynomial" width="600" height="400"></canvas>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div id="image" class="tab-content">
                            <div class="tab-pane">
                                <div class="code-block">
                                    <span class="code-language">Python</span>
                                    <button class="copy-btn" onclick="copyCode(this)">Copy</button>
                                    <pre><code># Note: TensorFlow/Keras needs to be installed separately: pip install tensorflow
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
import matplotlib.pyplot as plt

# Load the MNIST dataset
(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()

# Preprocess the data
x_train = x_train.astype("float32") / 255.0
x_test = x_test.astype("float32") / 255.0
x_train = np.expand_dims(x_train, -1)  # Add channel dimension
x_test = np.expand_dims(x_test, -1)

# Convert labels to one-hot encoding
y_train = keras.utils.to_categorical(y_train, 10)
y_test = keras.utils.to_categorical(y_test, 10)

# Define the model
model = keras.Sequential([
    keras.Input(shape=(28, 28, 1)),
    layers.Conv2D(32, kernel_size=(3, 3), activation="relu"),
    layers.MaxPooling2D(pool_size=(2, 2)),
    layers.Conv2D(64, kernel_size=(3, 3), activation="relu"),
    layers.MaxPooling2D(pool_size=(2, 2)),
    layers.Flatten(),
    layers.Dropout(0.5),
    layers.Dense(10, activation="softmax"),
])

# Compile the model
model.compile(
    loss="categorical_crossentropy",
    optimizer="adam",
    metrics=["accuracy"]
)

# Train the model
batch_size = 128
epochs = 15
history = model.fit(
    x_train, y_train,
    batch_size=batch_size,
    epochs=epochs,
    validation_split=0.1
)

# Evaluate the model
score = model.evaluate(x_test, y_test, verbose=0)
print(f"Test loss: {score[0]:.4f}")
print(f"Test accuracy: {score[1]:.4f}")

# Plot training history
plt.figure(figsize=(12, 5))

# Plot training & validation accuracy values
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')

# Plot training & validation loss values
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')

plt.tight_layout()
plt.show()

# Make predictions on some test images
predictions = model.predict(x_test[:10])
predicted_classes = np.argmax(predictions, axis=1)
true_classes = np.argmax(y_test[:10], axis=1)

# Display some predictions
plt.figure(figsize=(15, 5))
for i in range(10):
    plt.subplot(2, 5, i + 1)
    plt.imshow(x_test[i].squeeze(), cmap='gray')
    plt.title(f"True: {true_classes[i]}, Pred: {predicted_classes[i]}")
    plt.axis('off')
plt.tight_layout()
plt.show()</code></pre>
                                </div>
                                
                                <div class="viz-container">
                                    <div class="viz-title">CNN Training History</div>
                                    <div class="canvas-container">
                                        <canvas id="cnn-history" width="600" height="400"></canvas>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </section>

            <!-- Appendices would continue here -->
            <!-- For brevity, I'm including just the titles of the appendices -->

            <!-- Appendix A Section -->
            <section id="appendix-a" class="section">
                <h2>Appendix A: Linear Algebra and Functional Analysis</h2>
                <p>This appendix provides a review of linear algebra and functional analysis concepts that are used throughout the book. Topics include vector spaces, matrices, eigenvalues and eigenvectors, matrix decompositions, and functional analysis.</p>
            </section>

            <!-- Appendix B Section -->
            <section id="appendix-b" class="section">
                <h2>Appendix B: Multivariate Differentiation and Optimization</h2>
                <p>This appendix covers multivariate differentiation and optimization techniques that are essential for understanding many machine learning algorithms. Topics include gradients, Hessians, Taylor expansions, optimization theory, and numerical methods for optimization.</p>
            </section>

            <!-- Appendix C Section -->
            <section id="appendix-c" class="section">
                <h2>Appendix C: Probability and Statistics</h2>
                <p>This appendix provides a review of probability and statistics concepts that are used throughout the book. Topics include random variables, probability distributions, expectation, joint distributions, conditioning, convergence, and statistical inference.</p>
            </section>

            <!-- Appendix D Section -->
            <section id="appendix-d" class="section">
                <h2>Appendix D: Python Primer</h2>
                <p>This appendix introduces the Python programming language and its key libraries for data science and machine learning. Topics include basic Python syntax, data structures, NumPy for numerical computing, Pandas for data manipulation, Matplotlib for visualization, and Scikit-learn for machine learning.</p>
            </section>
        </main>
    </div>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <script>
        // Mobile toggle functionality
        document.getElementById('mobileToggle').addEventListener('click', function() {
            document.getElementById('sidebar').classList.toggle('active');
        });

        // Navigation functionality
        document.querySelectorAll('.nav-link').forEach(link => {
            link.addEventListener('click', function(e) {
                // Remove active class from all links
                document.querySelectorAll('.nav-link').forEach(l => l.classList.remove('active'));
                
                // Add active class to clicked link
                this.classList.add('active');
                
                // Close mobile menu if open
                if (window.innerWidth <= 768) {
                    document.getElementById('sidebar').classList.remove('active');
                }
                
                // Handle submenu toggles
                const href = this.getAttribute('href');
                if (href && href.includes('chapter')) {
                    const chapterId = href.split('-')[0] + '-' + href.split('-')[1];
                    const submenu = document.getElementById(chapterId + '-submenu');
                    
                    if (submenu) {
                        // Close all submenus
                        document.querySelectorAll('.submenu').forEach(s => s.classList.remove('active'));
                        
                        // Open the clicked submenu
                        submenu.classList.add('active');
                    }
                }
            });
        });

        // Tab functionality
        function openTab(evt, tabName) {
            // Hide all tab content
            document.querySelectorAll('.tab-content').forEach(content => {
                content.querySelectorAll('.tab-pane').forEach(pane => {
                    pane.classList.remove('active');
                });
            });
            
            // Remove active class from all tab buttons
            document.querySelectorAll('.tab-button').forEach(button => {
                button.classList.remove('active');
            });
            
            // Show the specific tab content
            const tabContent = document.getElementById(tabName);
            if (tabContent) {
                const tabPanes = tabContent.closest('.tab-content').querySelectorAll('.tab-pane');
                tabPanes.forEach(pane => {
                    if (pane.id === tabName) {
                        pane.classList.add('active');
                    }
                });
            }
            
            // Add active class to the button that opened the tab
            evt.currentTarget.classList.add('active');
        }

        // Accordion functionality
        function toggleAccordion(element) {
            const item = element.parentElement;
            const isActive = item.classList.contains('active');
            
            // Close all accordion items
            document.querySelectorAll('.accordion-item').forEach(i => {
                i.classList.remove('active');
            });
            
            // Open the clicked item if it was closed
            if (!isActive) {
                item.classList.add('active');
            }
        }

        // Copy code functionality
        function copyCode(button) {
            const codeBlock = button.nextElementSibling;
            const textArea = document.createElement('textarea');
            textArea.value = codeBlock.textContent;
            document.body.appendChild(textArea);
            textArea.select();
            document.execCommand('copy');
            document.body.removeChild(textArea);
            
            // Update button text
            const originalText = button.textContent;
            button.textContent = 'Copied!';
            setTimeout(() => {
                button.textContent = originalText;
            }, 2000);
        }

        // Initialize visualizations when the page loads
        document.addEventListener('DOMContentLoaded', function() {
            // Barplot visualization
            const barplotCtx = document.getElementById('barplot');
            if (barplotCtx) {
                new Chart(barplotCtx, {
                    type: 'bar',
                    data: {
                        labels: ['Couple', 'Single', 'Family'],
                        datasets: [{
                            label: 'Count',
                            data: [119, 98, 9],
                            backgroundColor: [
                                'rgba(54, 162, 235, 0.7)',
                                'rgba(255, 99, 132, 0.7)',
                                'rgba(255, 206, 86, 0.7)'
                            ],
                            borderColor: [
                                'rgba(54, 162, 235, 1)',
                                'rgba(255, 99, 132, 1)',
                                'rgba(255, 206, 86, 1)'
                            ],
                            borderWidth: 1
                        }]
                    },
                    options: {
                        responsive: true,
                        maintainAspectRatio: false,
                        scales: {
                            y: {
                                beginAtZero: true,
                                title: {
                                    display: true,
                                    text: 'Count'
                                }
                            },
                            x: {
                                title: {
                                    display: true,
                                    text: 'Living Situation'
                                }
                            }
                        }
                    }
                });
            }

            // Boxplot visualization
            const boxplotCtx = document.getElementById('boxplot');
            if (boxplotCtx) {
                new Chart(boxplotCtx, {
                    type: 'boxplot',
                    data: {
                        labels: ['Age'],
                        datasets: [{
                            label: 'Age Distribution',
                            backgroundColor: 'rgba(54, 162, 235, 0.7)',
                            borderColor: 'rgba(54, 162, 235, 1)',
                            borderWidth: 1,
                            outlierColor: '#999999',
                            padding: 10,
                            itemRadius: 0,
                            data: [
                                [65, 70, 75, 80, 85]  // min, Q1, median, Q3, max
                            ]
                        }]
                    },
                    options: {
                        responsive: true,
                        maintainAspectRatio: false,
                        scales: {
                            y: {
                                beginAtZero: false,
                                title: {
                                    display: true,
                                    text: 'Age'
                                }
                            }
                        }
                    }
                });
            }

            // Histogram visualization
            const histogramCtx = document.getElementById('histogram');
            if (histogramCtx) {
                new Chart(histogramCtx, {
                    type: 'bar',
                    data: {
                        labels: ['140-145', '145-150', '150-155', '155-160', '160-165', '165-170', '170-175', '175-180', '180-185', '185-190'],
                        datasets: [{
                            label: 'Frequency',
                            data: [5, 12, 18, 25, 35, 42, 38, 28, 15, 8],
                            backgroundColor: 'rgba(54, 162, 235, 0.7)',
                            borderColor: 'rgba(54, 162, 235, 1)',
                            borderWidth: 1
                        }]
                    },
                    options: {
                        responsive: true,
                        maintainAspectRatio: false,
                        scales: {
                            y: {
                                beginAtZero: true,
                                title: {
                                    display: true,
                                    text: 'Frequency'
                                }
                            },
                            x: {
                                title: {
                                    display: true,
                                    text: 'Height (cm)'
                                }
                            }
                        }
                    }
                });
            }

            // Density plot visualization
            const densityplotCtx = document.getElementById('densityplot');
            if (densityplotCtx) {
                new Chart(densityplotCtx, {
                    type: 'line',
                    data: {
                        labels: Array.from({length: 100}, (_, i) => 140 + i * 0.5),
                        datasets: [
                            {
                                label: 'Female',
                                data: Array.from({length: 100}, (_, i) => {
                                    const x = 140 + i * 0.5;
                                    return 0.01 * Math.exp(-0.5 * Math.pow((x - 160) / 8, 2));
                                }),
                                borderColor: 'rgba(255, 99, 132, 1)',
                                backgroundColor: 'rgba(255, 99, 132, 0.2)',
                                fill: true,
                                tension: 0.4
                            },
                            {
                                label: 'Male',
                                data: Array.from({length: 100}, (_, i) => {
                                    const x = 140 + i * 0.5;
                                    return 0.01 * Math.exp(-0.5 * Math.pow((x - 170) / 8, 2));
                                }),
                                borderColor: 'rgba(54, 162, 235, 1)',
                                backgroundColor: 'rgba(54, 162, 235, 0.2)',
                                fill: true,
                                tension: 0.4
                            }
                        ]
                    },
                    options: {
                        responsive: true,
                        maintainAspectRatio: false,
                        scales: {
                            y: {
                                beginAtZero: true,
                                title: {
                                    display: true,
                                    text: 'Density'
                                }
                            },
                            x: {
                                title: {
                                    display: true,
                                    text: 'Height (cm)'
                                }
                            }
                        }
                    }
                });
            }

            // Scatter plot visualization
            const scatterplotCtx = document.getElementById('scatterplot');
            if (scatterplotCtx) {
                // Generate random data for the scatter plot
                const generateScatterData = (count, xMean, yMean, xStd, yStd) => {
                    return Array.from({length: count}, () => ({
                        x: xMean + (Math.random() - 0.5) * 2 * xStd,
                        y: yMean + (Math.random() - 0.5) * 2 * yStd
                    }));
                };

                const maleData = generateScatterData(50, 175, 75, 10, 8);
                const femaleData = generateScatterData(50, 162, 62, 8, 6);

                new Chart(scatterplotCtx, {
                    type: 'scatter',
                    data: {
                        datasets: [
                            {
                                label: 'Male',
                                data: maleData,
                                backgroundColor: 'rgba(54, 162, 235, 0.7)',
                                borderColor: 'rgba(54, 162, 235, 1)',
                                borderWidth: 1
                            },
                            {
                                label: 'Female',
                                data: femaleData,
                                backgroundColor: 'rgba(255, 99, 132, 0.7)',
                                borderColor: 'rgba(255, 99, 132, 1)',
                                borderWidth: 1
                            }
                        ]
                    },
                    options: {
                        responsive: true,
                        maintainAspectRatio: false,
                        scales: {
                            y: {
                                beginAtZero: false,
                                title: {
                                    display: true,
                                    text: 'Weight (kg)'
                                }
                            },
                            x: {
                                title: {
                                    display: true,
                                    text: 'Height (cm)'
                                }
                            }
                        }
                    }
                });
            }

            // Box plot by gender visualization
            const boxplotbygenderCtx = document.getElementById('boxplotbygender');
            if (boxplotbygenderCtx) {
                new Chart(boxplotbygenderCtx, {
                    type: 'boxplot',
                    data: {
                        labels: ['Female', 'Male'],
                        datasets: [{
                            label: 'Height by Gender',
                            backgroundColor: [
                                'rgba(255, 99, 132, 0.7)',
                                'rgba(54, 162, 235, 0.7)'
                            ],
                            borderColor: [
                                'rgba(255, 99, 132, 1)',
                                'rgba(54, 162, 235, 1)'
                            ],
                            borderWidth: 1,
                            outlierColor: '#999999',
                            padding: 10,
                            itemRadius: 0,
                            data: [
                                [155, 160, 163, 167, 172],  // Female: min, Q1, median, Q3, max
                                [165, 170, 175, 180, 185]   // Male: min, Q1, median, Q3, max
                            ]
                        }]
                    },
                    options: {
                        responsive: true,
                        maintainAspectRatio: false,
                        scales: {
                            y: {
                                beginAtZero: false,
                                title: {
                                    display: true,
                                    text: 'Height (cm)'
                                }
                            }
                        }
                    }
                });
            }

            // Heatmap visualization
            const heatmapCtx = document.getElementById('heatmap');
            if (heatmapCtx) {
                new Chart(heatmapCtx, {
                    type: 'heatmap',
                    data: {
                        labels: ['Female', 'Male'],
                        datasets: [{
                            label: 'Count',
                            data: [
                                [0, 0, 56],  // Female, Couple
                                [0, 1, 7],   // Female, Family
                                [0, 2, 78],  // Female, Single
                                [1, 0, 63],  // Male, Couple
                                [1, 1, 2],   // Male, Family
                                [1, 2, 20]   // Male, Single
                            ],
                            backgroundColor: function(ctx) {
                                const value = ctx.parsed.v;
                                const alpha = value / 100;  // Normalize to 0-1
                                return `rgba(54, 162, 235, ${alpha})`;
                            },
                            borderWidth: 1,
                            borderColor: 'rgba(255, 255, 255, 1)',
                            width: ({chart}) => (chart.chartArea || {}).width / 3 - 1,
                            height: ({chart}) => (chart.chartArea || {}).height / 2 - 1
                        }]
                    },
                    options: {
                        responsive: true,
                        maintainAspectRatio: false,
                        scales: {
                            y: {
                                type: 'category',
                                labels: ['Female', 'Male'],
                                offset: true
                            },
                            x: {
                                type: 'category',
                                labels: ['Couple', 'Family', 'Single'],
                                offset: true
                            }
                        },
                        plugins: {
                            legend: {
                                display: false
                            },
                            tooltip: {
                                callbacks: {
                                    title: function() {
                                        return '';
                                    },
                                    label: function(context) {
                                        const gender = context.chart.data.scales.y.labels[context.data.y];
                                        const situation = context.chart.data.scales.x.labels[context.data.x];
                                        return `${gender}, ${situation}: ${context.parsed.v}`;
                                    }
                                }
                            }
                        }
                    }
                });
            }

            // Training vs. Test Loss visualization
            const trainTestLossCtx = document.getElementById('train-test-loss');
            if (trainTestLossCtx) {
                new Chart(trainTestLossCtx, {
                    type: 'line',
                    data: {
                        labels: Array.from({length: 20}, (_, i) => i + 1),
                        datasets: [
                            {
                                label: 'Training Loss',
                                data: Array.from({length: 20}, (_, i) => 0.5 * Math.exp(-0.2 * i) + 0.1 + Math.random() * 0.05),
                                borderColor: 'rgba(255, 99, 132, 1)',
                                backgroundColor: 'rgba(255, 99, 132, 0.2)',
                                fill: false,
                                tension: 0.4
                            },
                            {
                                label: 'Test Loss',
                                data: Array.from({length: 20}, (_, i) => 0.5 * Math.exp(-0.2 * i) + 0.2 + Math.random() * 0.05),
                                borderColor: 'rgba(54, 162, 235, 1)',
                                backgroundColor: 'rgba(54, 162, 235, 0.2)',
                                fill: false,
                                tension: 0.4
                            }
                        ]
                    },
                    options: {
                        responsive: true,
                        maintainAspectRatio: false,
                        scales: {
                            y: {
                                beginAtZero: false,
                                title: {
                                    display: true,
                                    text: 'Loss'
                                }
                            },
                            x: {
                                title: {
                                    display: true,
                                    text: 'Epoch'
                                }
                            }
                        }
                    }
                });
            }

            // Bias-Variance Tradeoff visualization
            const biasVarianceCtx = document.getElementById('bias-variance');
            if (biasVarianceCtx) {
                new Chart(biasVarianceCtx, {
                    type: 'line',
                    data: {
                        labels: ['Very Simple', 'Simple', 'Moderate', 'Complex', 'Very Complex'],
                        datasets: [
                            {
                                label: 'Bias²',
                                data: [0.8, 0.5, 0.2, 0.05, 0.01],
                                borderColor: 'rgba(255, 99, 132, 1)',
                                backgroundColor: 'rgba(255, 99, 132, 0.2)',
                                fill: false,
                                tension: 0.4
                            },
                            {
                                label: 'Variance',
                                data: [0.05, 0.1, 0.3, 0.6, 0.9],
                                borderColor: 'rgba(54, 162, 235, 1)',
                                backgroundColor: 'rgba(54, 162, 235, 0.2)',
                                fill: false,
                                tension: 0.4
                            },
                            {
                                label: 'Total Error',
                                data: [0.85, 0.6, 0.5, 0.65, 0.91],
                                borderColor: 'rgba(75, 192, 192, 1)',
                                backgroundColor: 'rgba(75, 192, 192, 0.2)',
                                fill: false,
                                tension: 0.4
                            }
                        ]
                    },
                    options: {
                        responsive: true,
                        maintainAspectRatio: false,
                        scales: {
                            y: {
                                beginAtZero: true,
                                title: {
                                    display: true,
                                    text: 'Error'
                                }
                            },
                            x: {
                                title: {
                                    display: true,
                                    text: 'Model Complexity'
                                }
                            }
                        }
                    }
                });
            }

            // Multivariate Normal Distribution visualization
            const multivariateNormalCtx = document.getElementById('multivariate-normal');
            if (multivariateNormalCtx) {
                // Generate data for a 2D multivariate normal distribution
                const generateMultivariateNormal = (count, meanX, meanY, stdX, stdY, correlation) => {
                    const data = [];
                    for (let i = 0; i < count; i++) {
                        // Generate two independent standard normal variables
                        let z1 = Math.sqrt(-2.0 * Math.log(Math.random())) * Math.cos(2.0 * Math.PI * Math.random());
                        let z2 = Math.sqrt(-2.0 * Math.log(Math.random())) * Math.cos(2.0 * Math.PI * Math.random());
                        
                        // Transform to have the desired correlation
                        let x = meanX + stdX * z1;
                        let y = meanY + stdY * (correlation * z1 + Math.sqrt(1 - correlation * correlation) * z2);
                        
                        data.push({x: x, y: y});
                    }
                    return data;
                };

                const data1 = generateMultivariateNormal(200, 0, 0, 1, 1, 0.7);
                const data2 = generateMultivariateNormal(200, 3, 3, 1.5, 0.8, -0.5);

                new Chart(multivariateNormalCtx, {
                    type: 'scatter',
                    data: {
                        datasets: [
                            {
                                label: 'Distribution 1',
                                data: data1,
                                backgroundColor: 'rgba(54, 162, 235, 0.7)',
                                borderColor: 'rgba(54, 162, 235, 1)',
                                borderWidth: 1
                            },
                            {
                                label: 'Distribution 2',
                                data: data2,
                                backgroundColor: 'rgba(255, 99, 132, 0.7)',
                                borderColor: 'rgba(255, 99, 132, 1)',
                                borderWidth: 1
                            }
                        ]
                    },
                    options: {
                        responsive: true,
                        maintainAspectRatio: false,
                        scales: {
                            y: {
                                title: {
                                    display: true,
                                    text: 'Y'
                                }
                            },
                            x: {
                                title: {
                                    display: true,
                                    text: 'X'
                                }
                            }
                        }
                    }
                });
            }

            // Linear Regression visualization
            const linearRegressionCtx = document.getElementById('linear-regression');
            if (linearRegressionCtx) {
                // Generate data for linear regression
                const generateLinearData = (count, slope, intercept, noise) => {
                    const data = [];
                    for (let i = 0; i < count; i++) {
                        const x = Math.random() * 10;
                        const y = slope * x + intercept + (Math.random() - 0.5) * 2 * noise;
                        data.push({x: x, y: y});
                    }
                    return data;
                };

                const data = generateLinearData(50, 2, 1, 2);
                
                // Generate regression line
                const regressionLine = [];
                for (let x = 0; x <= 10; x += 0.5) {
                    regressionLine.push({x: x, y: 2 * x + 1});
                }

                new Chart(linearRegressionCtx, {
                    type: 'scatter',
                    data: {
                        datasets: [
                            {
                                label: 'Data',
                                data: data,
                                backgroundColor: 'rgba(54, 162, 235, 0.7)',
                                borderColor: 'rgba(54, 162, 235, 1)',
                                borderWidth: 1
                            },
                            {
                                label: 'Regression Line',
                                data: regressionLine,
                                type: 'line',
                                borderColor: 'rgba(255, 99, 132, 1)',
                                backgroundColor: 'rgba(255, 99, 132, 0.2)',
                                borderWidth: 2,
                                fill: false,
                                pointRadius: 0
                            }
                        ]
                    },
                    options: {
                        responsive: true,
                        maintainAspectRatio: false,
                        scales: {
                            y: {
                                title: {
                                    display: true,
                                    text: 'Y'
                                }
                            },
                            x: {
                                title: {
                                    display: true,
                                    text: 'X'
                                }
                            }
                        }
                    }
                });
            }

            // Bayesian Linear Regression visualization
            const bayesianRegressionCtx = document.getElementById('bayesian-regression');
            if (bayesianRegressionCtx) {
                // Generate data for Bayesian linear regression
                const generateLinearData = (count, slope, intercept, noise) => {
                    const data = [];
                    for (let i = 0; i < count; i++) {
                        const x = Math.random() * 10;
                        const y = slope * x + intercept + (Math.random() - 0.5) * 2 * noise;
                        data.push({x: x, y: y});
                    }
                    return data;
                };

                const data = generateLinearData(50, 2, 1, 2);
                
                // Generate regression lines with uncertainty
                const regressionLines = [];
                for (let line = 0; line < 10; line++) {
                    const lineData = [];
                    const slope = 2 + (Math.random() - 0.5) * 0.5;
                    const intercept = 1 + (Math.random() - 0.5) * 1;
                    
                    for (let x = 0; x <= 10; x += 0.5) {
                        lineData.push({x: x, y: slope * x + intercept});
                    }
                    regressionLines.push(lineData);
                }

                const datasets = [
                    {
                        label: 'Data',
                        data: data,
                        backgroundColor: 'rgba(54, 162, 235, 0.7)',
                        borderColor: 'rgba(54, 162, 235, 1)',
                        borderWidth: 1
                    }
                ];

                // Add regression lines with decreasing opacity
                regressionLines.forEach((lineData, index) => {
                    datasets.push({
                        label: `Regression Line ${index + 1}`,
                        data: lineData,
                        type: 'line',
                        borderColor: `rgba(255, 99, 132, ${0.2 + 0.05 * index})`,
                        borderWidth: 1,
                        fill: false,
                        pointRadius: 0,
                        hidden: index > 0  // Only show the first line by default
                    });
                });

                new Chart(bayesianRegressionCtx, {
                    type: 'scatter',
                    data: {
                        datasets: datasets
                    },
                    options: {
                        responsive: true,
                        maintainAspectRatio: false,
                        scales: {
                            y: {
                                title: {
                                    display: true,
                                    text: 'Y'
                                }
                            },
                            x: {
                                title: {
                                    display: true,
                                    text: 'X'
                                }
                            }
                        }
                    }
                });
            }

            // Monte Carlo Estimation of π visualization
            const piEstimationCtx = document.getElementById('pi-estimation');
            if (piEstimationCtx) {
                // Generate points for Monte Carlo estimation of π
                const generatePiPoints = (count) => {
                    const insideCircle = [];
                    const outsideCircle = [];
                    
                    for (let i = 0; i < count; i++) {
                        const x = Math.random() * 2 - 1;  // Random point in [-1, 1]
                        const y = Math.random() * 2 - 1;  // Random point in [-1, 1]
                        
                        if (x * x + y * y <= 1) {
                            insideCircle.push({x: x, y: y});
                        } else {
                            outsideCircle.push({x: x, y: y});
                        }
                    }
                    
                    return {insideCircle, outsideCircle};
                };

                const {insideCircle, outsideCircle} = generatePiPoints(1000);

                new Chart(piEstimationCtx, {
                    type: 'scatter',
                    data: {
                        datasets: [
                            {
                                label: 'Inside Circle',
                                data: insideCircle,
                                backgroundColor: 'rgba(54, 162, 235, 0.7)',
                                borderColor: 'rgba(54, 162, 235, 1)',
                                borderWidth: 1
                            },
                            {
                                label: 'Outside Circle',
                                data: outsideCircle,
                                backgroundColor: 'rgba(255, 99, 132, 0.7)',
                                borderColor: 'rgba(255, 99, 132, 1)',
                                borderWidth: 1
                            }
                        ]
                    },
                    options: {
                        responsive: true,
                        maintainAspectRatio: false,
                        scales: {
                            y: {
                                min: -1.1,
                                max: 1.1,
                                title: {
                                    display: true,
                                    text: 'Y'
                                }
                            },
                            x: {
                                min: -1.1,
                                max: 1.1,
                                title: {
                                    display: true,
                                    text: 'X'
                                }
                            }
                        },
                        plugins: {
                            annotation: {
                                annotations: {
                                    circle: {
                                        type: 'circle',
                                        xValue: 0,
                                        yValue: 0,
                                        radius: 1,
                                        borderColor: 'rgba(75, 192, 192, 1)',
                                        borderWidth: 2,
                                        backgroundColor: 'rgba(75, 192, 192, 0.1)'
                                    }
                                }
                            }
                        }
                    }
                });
            }

            // Kernel Density Estimation visualization
            const kdePlotCtx = document.getElementById('kde-plot');
            if (kdePlotCtx) {
                // Generate data for KDE
                const generateKDEData = (count) => {
                    const data = [];
                    
                    // Generate data from two normal distributions
                    for (let i = 0; i < count / 2; i++) {
                        data.push(Math.random() * 2 - 1);  // Normal(0, 1)
                    }
                    
                    for (let i = 0; i < count / 2; i++) {
                        data.push(5 + Math.random() * 2 - 1);  // Normal(5, 1)
                    }
                    
                    return data;
                };

                const data = generateKDEData(1000);
                
                // Create histogram
                const histogram = [];
                const binWidth = 0.2;
                const bins = {};
                
                data.forEach(value => {
                    const bin = Math.floor(value / binWidth) * binWidth;
                    bins[bin] = (bins[bin] || 0) + 1;
                });
                
                for (const bin in bins) {
                    histogram.push({x: parseFloat(bin), y: bins[bin]});
                }
                
                // Create KDE approximation
                const kde = [];
                for (let x = -2; x <= 7; x += 0.1) {
                    let density = 0;
                    
                    data.forEach(value => {
                        // Gaussian kernel
                        const h = 0.3;  // Bandwidth
                        density += (1 / Math.sqrt(2 * Math.PI)) * Math.exp(-0.5 * Math.pow((x - value) / h, 2)) / (count * h);
                    });
                    
                    kde.push({x: x, y: density * 100});  // Scale for visualization
                }

                new Chart(kdePlotCtx, {
                    type: 'bar',
                    data: {
                        datasets: [
                            {
                                label: 'Histogram',
                                data: histogram,
                                backgroundColor: 'rgba(54, 162, 235, 0.7)',
                                borderColor: 'rgba(54, 162, 235, 1)',
                                borderWidth: 1,
                                barPercentage: 1.0,
                                categoryPercentage: 1.0
                            },
                            {
                                label: 'KDE',
                                data: kde,
                                type: 'line',
                                borderColor: 'rgba(255, 99, 132, 1)',
                                backgroundColor: 'rgba(255, 99, 132, 0.2)',
                                borderWidth: 2,
                                fill: false,
                                tension: 0.4,
                                pointRadius: 0
                            }
                        ]
                    },
                    options: {
                        responsive: true,
                        maintainAspectRatio: false,
                        scales: {
                            y: {
                                beginAtZero: true,
                                title: {
                                    display: true,
                                    text: 'Density'
                                }
                            },
                            x: {
                                title: {
                                    display: true,
                                    text: 'Value'
                                }
                            }
                        }
                    }
                });
            }

            // GMM Clustering visualization
            const gmmClusteringCtx = document.getElementById('gmm-clustering');
            if (gmmClusteringCtx) {
                // Generate data for GMM clustering
                const generateGMMData = (count, means, covariances) => {
                    const data = [];
                    const labels = [];
                    
                    for (let i = 0; i < count; i++) {
                        const component = Math.floor(Math.random() * means.length);
                        const mean = means[component];
                        const cov = covariances[component];
                        
                        // Generate a point from a multivariate normal distribution
                        const z1 = Math.sqrt(-2.0 * Math.log(Math.random())) * Math.cos(2.0 * Math.PI * Math.random());
                        const z2 = Math.sqrt(-2.0 * Math.log(Math.random())) * Math.cos(2.0 * Math.PI * Math.random());
                        
                        const x = mean[0] + Math.sqrt(cov[0][0]) * z1;
                        const y = mean[1] + Math.sqrt(cov[1][1]) * z2;
                        
                        data.push({x: x, y: y});
                        labels.push(component);
                    }
                    
                    return {data, labels};
                };

                const means = [
                    [0, 0],
                    [6, 3],
                    [-3, 5]
                ];
                
                const covariances = [
                    [[1, 0], [0, 1]],
                    [[1, 0], [0, 1]],
                    [[1, 0], [0, 1]]
                ];

                const {data, labels} = generateGMMData(300, means, covariances);
                
                // Group data by component
                const componentData = [[], [], []];
                data.forEach((point, index) => {
                    componentData[labels[index]].push(point);
                });

                new Chart(gmmClusteringCtx, {
                    type: 'scatter',
                    data: {
                        datasets: [
                            {
                                label: 'Component 1',
                                data: componentData[0],
                                backgroundColor: 'rgba(54, 162, 235, 0.7)',
                                borderColor: 'rgba(54, 162, 235, 1)',
                                borderWidth: 1
                            },
                            {
                                label: 'Component 2',
                                data: componentData[1],
                                backgroundColor: 'rgba(255, 99, 132, 0.7)',
                                borderColor: 'rgba(255, 99, 132, 1)',
                                borderWidth: 1
                            },
                            {
                                label: 'Component 3',
                                data: componentData[2],
                                backgroundColor: 'rgba(75, 192, 192, 0.7)',
                                borderColor: 'rgba(75, 192, 192, 1)',
                                borderWidth: 1
                            }
                        ]
                    },
                    options: {
                        responsive: true,
                        maintainAspectRatio: false,
                        scales: {
                            y: {
                                title: {
                                    display: true,
                                    text: 'Y'
                                }
                            },
                            x: {
                                title: {
                                    display: true,
                                    text: 'X'
                                }
                            }
                        }
                    }
                });
            }

            // K-Means Clustering visualization
            const kmeansClusteringCtx = document.getElementById('kmeans-clustering');
            if (kmeansClusteringCtx) {
                // Generate data for K-means clustering
                const generateKMeansData = (count, centers) => {
                    const data = [];
                    const labels = [];
                    
                    for (let i = 0; i < count; i++) {
                        const center = centers[Math.floor(Math.random() * centers.length)];
                        const x = center[0] + (Math.random() - 0.5) * 2;
                        const y = center[1] + (Math.random() - 0.5) * 2;
                        
                        data.push({x: x, y: y});
                    }
                    
                    return data;
                };

                const centers = [
                    [0, 0],
                    [5, 0],
                    [-5, 5]
                ];

                const data = generateKMeansData(300, centers);
                
                // Simulate K-means clustering
                const kmeansLabels = data.map(point => {
                    // Find the nearest center
                    let minDist = Infinity;
                    let nearestCenter = 0;
                    
                    centers.forEach((center, index) => {
                        const dist = Math.sqrt(Math.pow(point.x - center[0], 2) + Math.pow(point.y - center[1], 2));
                        if (dist < minDist) {
                            minDist = dist;
                            nearestCenter = index;
                        }
                    });
                    
                    return nearestCenter;
                });
                
                // Group data by cluster
                const clusterData = [[], [], []];
                data.forEach((point, index) => {
                    clusterData[kmeansLabels[index]].push(point);
                });

                new Chart(kmeansClusteringCtx, {
                    type: 'scatter',
                    data: {
                        datasets: [
                            {
                                label: 'Cluster 1',
                                data: clusterData[0],
                                backgroundColor: 'rgba(54, 162, 235, 0.7)',
                                borderColor: 'rgba(54, 162, 235, 1)',
                                borderWidth: 1
                            },
                            {
                                label: 'Cluster 2',
                                data: clusterData[1],
                                backgroundColor: 'rgba(255, 99, 132, 0.7)',
                                borderColor: 'rgba(255, 99, 132, 1)',
                                borderWidth: 1
                            },
                            {
                                label: 'Cluster 3',
                                data: clusterData[2],
                                backgroundColor: 'rgba(75, 192, 192, 0.7)',
                                borderColor: 'rgba(75, 192, 192, 1)',
                                borderWidth: 1
                            },
                            {
                                label: 'Centroids',
                                data: centers.map(center => ({x: center[0], y: center[1]})),
                                backgroundColor: 'rgba(255, 206, 86, 0.7)',
                                borderColor: 'rgba(255, 206, 86, 1)',
                                borderWidth: 2,
                                pointStyle: 'crossRot',
                                radius: 10
                            }
                        ]
                    },
                    options: {
                        responsive: true,
                        maintainAspectRatio: false,
                        scales: {
                            y: {
                                title: {
                                    display: true,
                                    text: 'Y'
                                }
                            },
                            x: {
                                title: {
                                    display: true,
                                    text: 'X'
                                }
                            }
                        }
                    }
                });
            }

            // Hierarchical Clustering visualization
            const hierarchicalClusteringCtx = document.getElementById('hierarchical-clustering');
            if (hierarchicalClusteringCtx) {
                // Generate data for hierarchical clustering
                const generateHierarchicalData = (count, centers) => {
                    const data = [];
                    const labels = [];
                    
                    for (let i = 0; i < count; i++) {
                        const center = centers[Math.floor(Math.random() * centers.length)];
                        const x = center[0] + (Math.random() - 0.5) * 2;
                        const y = center[1] + (Math.random() - 0.5) * 2;
                        
                        data.push({x: x, y: y});
                    }
                    
                    return data;
                };

                const centers = [
                    [0, 0],
                    [5, 0],
                    [-5, 5]
                ];

                const data = generateHierarchicalData(50, centers);
                
                // Simulate hierarchical clustering
                const hierarchicalLabels = data.map(point => {
                    // Find the nearest center
                    let minDist = Infinity;
                    let nearestCenter = 0;
                    
                    centers.forEach((center, index) => {
                        const dist = Math.sqrt(Math.pow(point.x - center[0], 2) + Math.pow(point.y - center[1], 2));
                        if (dist < minDist) {
                            minDist = dist;
                            nearestCenter = index;
                        }
                    });
                    
                    return nearestCenter;
                });
                
                // Group data by cluster
                const clusterData = [[], [], []];
                data.forEach((point, index) => {
                    clusterData[hierarchicalLabels[index]].push(point);
                });

                new Chart(hierarchicalClusteringCtx, {
                    type: 'scatter',
                    data: {
                        datasets: [
                            {
                                label: 'Cluster 1',
                                data: clusterData[0],
                                backgroundColor: 'rgba(54, 162, 235, 0.7)',
                                borderColor: 'rgba(54, 162, 235, 1)',
                                borderWidth: 1
                            },
                            {
                                label: 'Cluster 2',
                                data: clusterData[1],
                                backgroundColor: 'rgba(255, 99, 132, 0.7)',
                                borderColor: 'rgba(255, 99, 132, 1)',
                                borderWidth: 1
                            },
                            {
                                label: 'Cluster 3',
                                data: clusterData[2],
                                backgroundColor: 'rgba(75, 192, 192, 0.7)',
                                borderColor: 'rgba(75, 192, 192, 1)',
                                borderWidth: 1
                            }
                        ]
                    },
                    options: {
                        responsive: true,
                        maintainAspectRatio: false,
                        scales: {
                            y: {
                                title: {
                                    display: true,
                                    text: 'Y'
                                }
                            },
                            x: {
                                title: {
                                    display: true,
                                    text: 'X'
                                }
                            }
                        }
                    }
                });
            }

            // PCA visualization
            const pcaVisualizationCtx = document.getElementById('pca-visualization');
            if (pcaVisualizationCtx) {
                // Generate data for PCA
                const generatePCAData = (count) => {
                    const data = [];
                    
                    for (let i = 0; i < count; i++) {
                        // Generate correlated data
                        const x = Math.random() * 4 - 2;
                        const y = 0.6 * x + (Math.random() - 0.5) * 2;
                        
                        data.push({x: x, y: y});
                    }
                    
                    return data;
                };

                const data = generatePCAData(100);
                
                // Simulate PCA transformation
                const pcaData = data.map(point => {
                    // Rotate the data
                    const angle = Math.PI / 4;  // 45 degrees
                    const x = point.x * Math.cos(angle) - point.y * Math.sin(angle);
                    const y = point.x * Math.sin(angle) + point.y * Math.cos(angle);
                    
                    return {x: x, y: y};
                });

                new Chart(pcaVisualizationCtx, {
                    type: 'scatter',
                    data: {
                        datasets: [
                            {
                                label: 'Original Data',
                                data: data,
                                backgroundColor: 'rgba(54, 162, 235, 0.7)',
                                borderColor: 'rgba(54, 162, 235, 1)',
                                borderWidth: 1
                            },
                            {
                                label: 'PCA Transformed Data',
                                data: pcaData,
                                backgroundColor: 'rgba(255, 99, 132, 0.7)',
                                borderColor: 'rgba(255, 99, 132, 1)',
                                borderWidth: 1
                            }
                        ]
                    },
                    options: {
                        responsive: true,
                        maintainAspectRatio: false,
                        scales: {
                            y: {
                                title: {
                                    display: true,
                                    text: 'Y'
                                }
                            },
                            x: {
                                title: {
                                    display: true,
                                    text: 'X'
                                }
                            }
                        }
                    }
                });
            }

            // Nonlinear Regression visualization
            const nonlinearRegressionCtx = document.getElementById('nonlinear-regression');
            if (nonlinearRegressionCtx) {
                // Generate data for nonlinear regression
                const generateNonlinearData = (count) => {
                    const data = [];
                    
                    for (let i = 0; i < count; i++) {
                        const x = Math.random() * 4;
                        const y = 2.5 * Math.exp(-1.3 * x) + 0.5 + (Math.random() - 0.5) * 0.4;
                        
                        data.push({x: x, y: y});
                    }
                    
                    return data;
                };

                const data = generateNonlinearData(50);
                
                // Generate nonlinear regression line
                const regressionLine = [];
                for (let x = 0; x <= 4; x += 0.1) {
                    regressionLine.push({x: x, y: 2.5 * Math.exp(-1.3 * x) + 0.5});
                }

                new Chart(nonlinearRegressionCtx, {
                    type: 'scatter',
                    data: {
                        datasets: [
                            {
                                label: 'Data',
                                data: data,
                                backgroundColor: 'rgba(54, 162, 235, 0.7)',
                                borderColor: 'rgba(54, 162, 235, 1)',
                                borderWidth: 1
                            },
                            {
                                label: 'Nonlinear Fit',
                                data: regressionLine,
                                type: 'line',
                                borderColor: 'rgba(255, 99, 132, 1)',
                                backgroundColor: 'rgba(255, 99, 132, 0.2)',
                                borderWidth: 2,
                                fill: false,
                                pointRadius: 0
                            }
                        ]
                    },
                    options: {
                        responsive: true,
                        maintainAspectRatio: false,
                        scales: {
                            y: {
                                title: {
                                    display: true,
                                    text: 'Y'
                                }
                            },
                            x: {
                                title: {
                                    display: true,
                                    text: 'X'
                                }
                            }
                        }
                    }
                });
            }

            // Smoothing Splines visualization
            const smoothingSplinesCtx = document.getElementById('smoothing-splines');
            if (smoothingSplinesCtx) {
                // Generate data for smoothing splines
                const generateSplineData = (count) => {
                    const data = [];
                    
                    for (let i = 0; i < count; i++) {
                        const x = Math.random() * 10;
                        const y = Math.sin(x) + (Math.random() - 0.5) * 0.2;
                        
                        data.push({x: x, y: y});
                    }
                    
                    return data;
                };

                const data = generateSplineData(50);
                
                // Generate smoothing splines with different smoothing parameters
                const smoothingLines = [];
                const smoothingParams = [0.1, 1.0, 10.0];
                const colors = [
                    'rgba(255, 99, 132, 1)',
                    'rgba(54, 162, 235, 1)',
                    'rgba(75, 192, 192, 1)'
                ];
                
                smoothingParams.forEach((param, index) => {
                    const lineData = [];
                    
                    for (let x = 0; x <= 10; x += 0.1) {
                        // Simulate smoothing spline
                        let y = Math.sin(x);
                        
                        // Add smoothing effect
                        if (param > 0.1) {
                            y = 0.9 * y + 0.1 * Math.sin(x + 1);
                        }
                        
                        if (param > 1.0) {
                            y = 0.8 * y + 0.2 * Math.sin(x + 2);
                        }
                        
                        lineData.push({x: x, y: y});
                    }
                    
                    smoothingLines.push({
                        label: `λ = ${param}`,
                        data: lineData,
                        type: 'line',
                        borderColor: colors[index],
                        backgroundColor: colors[index].replace('1)', '0.2)'),
                        borderWidth: 2,
                        fill: false,
                        pointRadius: 0
                    });
                });

                new Chart(smoothingSplinesCtx, {
                    type: 'scatter',
                    data: {
                        datasets: [
                            {
                                label: 'Data',
                                data: data,
                                backgroundColor: 'rgba(54, 162, 235, 0.7)',
                                borderColor: 'rgba(54, 162, 235, 1)',
                                borderWidth: 1
                            },
                            ...smoothingLines
                        ]
                    },
                    options: {
                        responsive: true,
                        maintainAspectRatio: false,
                        scales: {
                            y: {
                                title: {
                                    display: true,
                                    text: 'Y'
                                }
                            },
                            x: {
                                title: {
                                    display: true,
                                    text: 'X'
                                }
                            }
                        }
                    }
                });
            }

            // Gaussian Process Regression visualization
            const gpRegressionCtx = document.getElementById('gp-regression');
            if (gpRegressionCtx) {
                // Generate data for GP regression
                const generateGPData = (count) => {
                    const data = [];
                    
                    for (let i = 0; i < count; i++) {
                        const x = Math.random() * 10;
                        const y = Math.sin(x) + (Math.random() - 0.5) * 0.1;
                        
                        data.push({x: x, y: y});
                    }
                    
                    return data;
                };

                const data = generateGPData(20);
                
                // Generate GP prediction with confidence interval
                const predictionLine = [];
                const upperBound = [];
                const lowerBound = [];
                
                for (let x = 0; x <= 10; x += 0.1) {
                    // Simulate GP prediction
                    const y = Math.sin(x);
                    const uncertainty = 0.1 + 0.05 * Math.sin(x * 2);
                    
                    predictionLine.push({x: x, y: y});
                    upperBound.push({x: x, y: y + 1.96 * uncertainty});
                    lowerBound.push({x: x, y: y - 1.96 * uncertainty});
                }

                new Chart(gpRegressionCtx, {
                    type: 'scatter',
                    data: {
                        datasets: [
                            {
                                label: 'Data',
                                data: data,
                                backgroundColor: 'rgba(255, 99, 132, 0.7)',
                                borderColor: 'rgba(255, 99, 132, 1)',
                                borderWidth: 1
                            },
                            {
                                label: 'Prediction',
                                data: predictionLine,
                                type: 'line',
                                borderColor: 'rgba(54, 162, 235, 1)',
                                backgroundColor: 'rgba(54, 162, 235, 0.2)',
                                borderWidth: 2,
                                fill
